{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_parse_BCJ(path = None, doc = None):\n",
    "    '''Given file path (text file) of negligence cases, finds static \n",
    "    information within the case (information that can be pattern matched)\n",
    "    Expects a B.C.J. case format (British Columbia Judgments)\n",
    "    \n",
    "    The following fields are currently implemented:\n",
    "    - Case Title\n",
    "    - Judge Name\n",
    "    - Registry\n",
    "    - Year\n",
    "    - Decision Length (in paragraphs)\n",
    "    - Damages\n",
    "    - Multiple Defendants\n",
    "    - Plaintiff Wins\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: case_parsed_data (list) of case_dict (Dictionary): List of Dictionaries with rule based parsable fields filled in\n",
    "    '''\n",
    "    if path:\n",
    "        with open(path, encoding='utf-8') as document:\n",
    "            document_data = document.read()\n",
    "        document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "    \n",
    "\n",
    "    case_parsed_data = []\n",
    "    for i in range(len(document_data)):\n",
    "        case_dict = dict() \n",
    "        case = document_data[i]\n",
    "        case = case.strip() # Make sure to strip!\n",
    "        if len(case) == 0: # Skip empty lines\n",
    "            continue\n",
    "        \n",
    "        lines = case.split('\\n')\n",
    "        if len(lines) < 2:\n",
    "            print(case)\n",
    "        case_title = lines[0]\n",
    "        case_type = lines[1]\n",
    "\n",
    "        if 'R. v.' in case_title or '(Re)' in case_title: # Skip crown cases, Skip (Re) cases\n",
    "            continue\n",
    "            \n",
    "        # Skip client/solicitor cases (not same as plaintiff/defendant)\n",
    "        regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?|[L|l]awyer[s]?))', case)\n",
    "        if regex_client_solicitor:\n",
    "            continue\n",
    "        \n",
    "        regex_solicitor_client = re.search(r'(Between.*([L|l]awyer[s]?|[S|s]olicitor[s]?).*([C|c]lient[s]?))', case)\n",
    "        if regex_solicitor_client:\n",
    "            continue\n",
    "            \n",
    "        # In some rare cases we have 'IN THE MATTER OF ..' (rather than 'Between ...') .. but it is following by the normal\n",
    "        # plaintiff/defendant dynamic. Only skip cases if there is no mention of the following terms\n",
    "        # (Can be cleaned up in future)\n",
    "        key_words = ['appellant', 'respondent', 'claimant', 'petitioner', 'plaintiff', 'defendant',\n",
    "        'appellants', 'respondents', 'claimants', 'petitioners', 'plaintiffs', 'defendants']\n",
    "        regex_in_matter_of = re.search(r'IN THE MATTER OF .*\\n\\([0-9]+ paras.\\)', case)\n",
    "        if regex_in_matter_of:\n",
    "            remove = True\n",
    "            for key in key_words:\n",
    "                if key in regex_in_matter_of.group(0).lower().strip():\n",
    "                    remove = False\n",
    "                    \n",
    "            if remove:\n",
    "                continue\n",
    "\n",
    "        if 'British Columbia Judgments' in case_type: # Make sure we're dealing with a B.C.J. case\n",
    "        \n",
    "            # Fields that can be found via pattern matching\n",
    "            if re.search('contributory negligence', case, re.IGNORECASE):\n",
    "                contributory_negligence_raised = True\n",
    "            else:\n",
    "                contributory_negligence_raised = False\n",
    "            case_number = re.search(r'\\/P([0-9]+)\\.txt', path).group(1)\n",
    "            decision_len = re.search(r'\\(([0-9]+) paras\\.?\\)', case) # e.g.) (100 paras.)\n",
    "            registry = re.search(r'(Registry|Registries): ?([A-Za-z0-9 ]+)', case) # e.g.) Registry: Vancouver\n",
    "            written_decision = True if int(decision_len.group(1)) > 1 else False\n",
    "            if registry:\n",
    "                registry = registry.group(2).strip()\n",
    "            else:\n",
    "                registry = re.search(r'([A-Za-z ]+) Registry No.', case) # Alt form e.g.) Vancouver Registory No. XXX\n",
    "                if registry:\n",
    "                    registry = registry.group(1).strip()\n",
    "                else:\n",
    "                    registry = re.search(r'([A-Za-z ]+) No. S[0-9]*', case)\n",
    "                    if registry:\n",
    "                        registry = registry.group(1).strip()\n",
    "                    else:\n",
    "                        print('WARNING: Registry could not be found (This shouldn\\'t occur!)')\n",
    "            # Fields that are always in the same place\n",
    "            judge_name = lines[4].strip()\n",
    "            case_title = lines[0].strip()\n",
    "            # Extract year from case_title (in case we want to make visualizations, etc.)\n",
    "            year = re.search(r'20[0-2][0-9]', case_title) # Limit regex to be from 2000 to 2029\n",
    "            if year:\n",
    "                year = year.group(0)\n",
    "            else:\n",
    "                # Rare case: Sometimes the title is too long. Rely on Heard date.\n",
    "                year = re.search(r'Heard:.* ([2][0][0-2][0-9])', case)\n",
    "                if year:\n",
    "                    year = year.group(1)\n",
    "                else:\n",
    "                    print('WARNING: Year not found')\n",
    "            case_dict['case_number'] = '%s of %s'%(i+1+((int(case_number)-1)*50), case_number)\n",
    "            case_dict['case_title'] = case_title\n",
    "            case_dict['year'] = year\n",
    "            case_dict['registry'] = registry\n",
    "            case_dict['judge'] = judge_name\n",
    "            case_dict['decision_length'] = decision_len.group(1)\n",
    "            case_dict['multiple_defendants'] = rule_based_multiple_defendants_parse(case)\n",
    "            case_dict['contributory_negligence_raised'] = contributory_negligence_raised\n",
    "            case_dict['written_decision'] = written_decision\n",
    "            \n",
    "            # TODO: Improve plaintiff_wins to take one case at a time.\n",
    "            plaintiff_list = plaintiff_wins(path)\n",
    "            if case_title in plaintiff_list:\n",
    "                case_dict['plaintiff_wins'] = plaintiff_list[case_title]\n",
    "            else:\n",
    "                case_dict['plaintiff_wins'] = \"NA\"\n",
    "                \n",
    "            case_dict['damages'] = rule_based_damage_extraction(case)\n",
    "                \n",
    "        # don't add empty dictionaries (non BCJ cases) to list\n",
    "        if case_dict != dict(): \n",
    "            case_parsed_data.append(case_dict)\n",
    "    return case_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_multiple_defendants_parse(doc):\n",
    "    ''' Work in progress. Subject to minor changes to Regex patterns.\n",
    "    \n",
    "    TODO:\n",
    "        - Clarify solicitor/client cases with Lachlan\n",
    "        - Clarify cases that say \"IN MATTER OF ...\", currently returning 'UNK' for these\n",
    "    \n",
    "    -----\n",
    "    \n",
    "    Given a case. Uses regex/pattern-matching to determine whether we have multiple defendants.\n",
    "    For the most part the logic relies on whether the langauge used implies plurality or not.\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: response (String, 'Y', 'N', or 'UNK')\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Cases with (Re) in the title always have one person involved\n",
    "    # May drop these cases depending on advice from Lachlan.\n",
    "    if '(Re)' in doc.split('\\n')[0]:\n",
    "        return 'N'\n",
    "    \n",
    "    # Case 1)\n",
    "    # Traditional/most common. Of form \"Between A, B, C, Plaintiff(s), X, Y, Z Defendant(s)\"\n",
    "    # Can successfully cover ~98% of data\n",
    "    regex_between_plaintiff_claimant = re.search(r'(Between.*([P|p]laintiff[s]?|[C|c]laimant[s]?|[A|a]ppellant[s]?|[P|p]etitioner[s]?).*([D|d]efendant[s]?|[R|r]espondent[s]?).*\\n)', doc)\n",
    "    \n",
    "    # Match found\n",
    "    if regex_between_plaintiff_claimant:\n",
    "        text = regex_between_plaintiff_claimant.group(0).lower()\n",
    "        if 'defendants' in text or 'respondents' in text:\n",
    "            return 'Y'\n",
    "        elif 'defendant' in text or 'respondent' in text:\n",
    "            return 'N'\n",
    "    \n",
    "    # If not found, try other less common cases\n",
    "    else:\n",
    "        # Case 2)\n",
    "        # Sometimes it does not mention the name of the second item. (Defendent/Respondent)\n",
    "        # We can estimate if there are multiple based on the number of \",\" in the line (Covers all cases in initial data)\n",
    "        regex_missing_defendent = re.search(r'(Between.*([P|p]laintiff[s]?|[C|c]laimant[s]?|[A|a]ppellant[s]?|[P|p]etitioner[s]?).*\\n)', doc)\n",
    "        if regex_missing_defendent:\n",
    "            text = regex_missing_defendent.group(0).lower()\n",
    "            if len(text.split(',')) > 5:\n",
    "                return 'Y'\n",
    "            else:\n",
    "                return 'N'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Case 3A) solicitor-client\n",
    "            # Some cases have a solicitor (lawyer) and a client\n",
    "            # Currently assuming the second item is the defendant\n",
    "            regex_solicitor_client = re.search(r'(Between.*([S|s]olicitor[s]?).*([C|c]lient[s]?))', doc)\n",
    "            if regex_solicitor_client:\n",
    "                text = regex_solicitor_client.group(0).lower()\n",
    "                if 'clients' in text:\n",
    "                    return 'Y'\n",
    "                else:\n",
    "                    return 'N'\n",
    "            else:\n",
    "                # Case 3B) client - solicitor\n",
    "                regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?))', doc)\n",
    "                if regex_client_solicitor:\n",
    "                    text = regex_client_solicitor.group(0).lower()\n",
    "                    if 'solicitors' in text:\n",
    "                        return 'Y'\n",
    "                    else:\n",
    "                        return 'N'\n",
    "                else:\n",
    "                    return 'UNK'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plaintiff_wins(path):\n",
    "    '''This function will search the cases and returns a dictionary\n",
    "    with case names as keys and boolean for value, True if the plaintiff\n",
    "    wins the case and False if plaintiff looses'''\n",
    "#     list_of_files = os.listdir(path)\n",
    "    plaintiff_dict = {}\n",
    "\n",
    "    with open(path,'r') as f:\n",
    "        \n",
    "        contents = f.read() \n",
    "        cases = contents.split(\"End of Document\\n\")\n",
    "        for line in cases:\n",
    "            lines = line.strip().split(\"\\n\")\n",
    "            name = lines[0]\n",
    "            # regex search for keyword HELD in cases, which determines if case was allowed or dismissed\n",
    "            HELD = re.search(r'HELD.+', line)\n",
    "            if HELD:\n",
    "                matched = HELD.group(0)\n",
    "                if \"allowed\" in matched or \"favour\" in matched or \"awarded\" in matched or \"granted\" in matched:\n",
    "                    plaintiff_dict[name] = True\n",
    "                if \"dismissed\" in matched:\n",
    "                    plaintiff_dict[name] = False\n",
    "            else:\n",
    "                if line:\n",
    "                    awarded =  re.search(r'award(.+)?.+?(plaintiff(.+)?)?', lines[-2])\n",
    "                    #regex searches for pattern of plaintiff/defendant/applicant....entitled/have...costs\n",
    "                    entiteled = re.search(r'(plaintiff|defendant.?|applicant)(.+)?(entitle(.)?(.+)?|have).+?cost(.+)?', lines[-2])\n",
    "                    #regex searches for pattern of successful...(case)\n",
    "                    successful = re.search(r'successful(.+)?.+?', lines[-2])\n",
    "                    #regex searches for dismiss....\n",
    "                    dismiss = re.search(r'(dismiss(.+)?.+)|(adjourned.+?)', lines[-2])\n",
    "                    costs = re.search(r'costs.+?(award(.+)?|cause).+?', lines[-2])\n",
    "\n",
    "                    if dismiss and \"not dismissed\" not in lines[-2]:\n",
    "                        plaintiff_dict[name] = False\n",
    "                    elif awarded:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    elif entiteled:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    elif successful:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    elif costs:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    else:\n",
    "                        plaintiff_dict[name] = \"OpenCase\"\n",
    "\n",
    "        \n",
    "    return plaintiff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_tokenize(doc):\n",
    "    ''' String of Entire Document and returns list of lists of paragraphs in document\n",
    "    ---------\n",
    "    Input: doc (str) - string of single legal case\n",
    "    Return: docs_split(list) - list of lists of numbrered paragraphs per document'''\n",
    "    \n",
    "    doc_data = []\n",
    "    lines = doc.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    doc_data.append(lines[0])\n",
    "    decision_length = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', doc).group(1)\n",
    "\n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    pattern = r'[\\W|\\w]?(?=\\n[0-9]{1,%s}[\\xa0]{2})'%len(decision_length)\n",
    "    paras_split = re.split(pattern, doc)\n",
    "\n",
    "    paras = []\n",
    "    for para in paras_split:   \n",
    "        # make sure the paragraph starts with the correct characters\n",
    "        para_start = re.match(r'^\\n([0-9]{1,%s})[\\xa0]{2}'%len(decision_length), para)\n",
    "        if para_start:\n",
    "            paras.append(para)\n",
    "    doc_data.extend(paras)\n",
    "    return doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex_damages = r'[\\w|-]* ?(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "#regex_damages = r'(?:[\\w|-]* ?){0,3}(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "#regex_in_trust = r'(?:in-?trust|award).*?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "\n",
    "# Rule based dmg extraction REGEX patterns\n",
    "regex_damages = r'(?![and])(?:[\\w|-]* ?){0,2} ?(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "regex_damages_2 = r'[^:] \\$? ?[0-9][0-9|,|.]+[0-9] (?:for|representing)?[ \\w\\-+]+damages?'\n",
    "regex_damages_3 = r'[^:] \\$? ?[0-9][0-9|,|.]+[0-9] (?:for|representing)?[ \\w\\-+]+damages?(?:(?:for|representing)?.*?[;.\\n])'\n",
    "regex_future_care_loss = r'(?:future|past|in[-| ]?trust|award).*?(?:loss|costs?|income|care)?.*?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "regex_for_cost_of = r'\\$? ?[0-9][0-9|,|.]+[0-9][\\w ]*? cost .*?\\.'\n",
    "\n",
    "# Keywords to look in match for categorization\n",
    "general_damage_keywords = [('general',), ('future', 'income', 'loss'), ('future', 'income'), ('future', 'wage', 'loss'), ('future', 'earning'), ('!past', 'earning', 'capacity'), ('future', 'capacity'), ('future', 'earning'), ('!past', 'loss', 'opportunity'), ('!past', 'loss', 'housekeep'), ('ei', 'benefit')]\n",
    "special_damage_keywords = [('special',), ('trust',), ('past', 'income', 'loss'), ('past', 'wage'), ('past', 'earning'), ('past', 'income'), ('earning', 'capacity')]\n",
    "aggravated_damage_keywords = [('aggravated',)]\n",
    "non_pecuniary_damage_keywords = [('non', 'pecuniary')]\n",
    "punitive_damage_keywords = [('punitive',)]\n",
    "future_care_damage_keywords = [('future', 'care'), ('future', 'cost')]\n",
    "\n",
    "def rule_based_damage_extraction(doc, min_score = 0.9, max_match_len_split = 10):\n",
    "    '''Helper functino for rule_based_parse_BCJ\n",
    "    \n",
    "    Given a case, attempts to extract damages using regex patterns\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    min_score (float): The minimum paragraph score to consider having a valid $ number\n",
    "                       Paragraph has score 1 if its the last paragraph\n",
    "                       Paragraph has score 0 if its the first paragraph\n",
    "    max_match_len_split (int): The max amount of items that can appear in a regex match after splitting (no. words)\n",
    "    \n",
    "    Returns: damages (Dict): Contains any found damages\n",
    "    \n",
    "    '''\n",
    "    damages = defaultdict(float)\n",
    "    repetition_detection = defaultdict(set) # try to stem the repeated values\n",
    "    no_paras = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', doc).group(1) # Get number of paragraphs\n",
    "    pattern = r'([.]?)(?=\\n[0-9]{1,%s}[\\xa0|\\s| ]{2})'%len(no_paras) # Used to split into paras\n",
    "    paras_split = re.split(pattern, doc)\n",
    "    money_patt = r'\\$[0-9|,]+' # Used to get all paragraphs with a money amount\n",
    "    scored_paras = [] # Score paragraphs based on where they appear in the document\n",
    "                      # Score of 0.0 would be the first paragraph. Score of 1.0 would be the last paragraph\n",
    "        \n",
    "    for i, paragraph in enumerate(paras_split):\n",
    "        if re.search(money_patt, paragraph):\n",
    "            scored_paras.append((i / len(paras_split), paragraph)) # (score, paragraph). Score formula: i/no_paras\n",
    "            \n",
    "    scored_paras = sorted(scored_paras, key=lambda x:x[0])[::-1] # Store from last paragraph to first\n",
    "    if len(scored_paras) == 0:\n",
    "        return None\n",
    "    if scored_paras[0][0] < min_score: #If highest scored paragraph is less than minimum score.\n",
    "        return None\n",
    "    \n",
    "    patterns = [regex_damages, regex_damages_2, regex_damages_3, regex_future_care_loss, regex_for_cost_of]\n",
    "    banned_words = ['seek', 'claim', 'propose', 'range', ' v. '] # Skip paragraphs containing these\n",
    "    counter_words = ['summary', 'dismissed'] # Unless these are mentioned. \n",
    "                                             # example) \"Special damage is $5k. But claims for aggravated are 'dismissed'\" \n",
    "    \n",
    "    # Get money mounts from the text\n",
    "    total = None\n",
    "    matches = []\n",
    "    summary_matches = []\n",
    "    for i, scored_para in enumerate(scored_paras):\n",
    "        text = scored_para[1]\n",
    "        score = scored_para[0]\n",
    "        \n",
    "        if score > min_score:\n",
    "            if any(item.startswith('summary') for item in text.lower().split()[:4]) or any(item.startswith('conclusion') for item in text.lower().split()[:4]):\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    summary_matches.append((score, t_m))\n",
    "            elif i+1 < len(scored_paras) and (any(item.startswith('summary') for item in scored_paras[i+1][1].lower().split()[-4:]) or any(item.startswith('conclusion') for item in scored_paras[i+1][1].lower().split()[-4:])):\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    summary_matches.append((score, t_m))\n",
    "            else:\n",
    "                skip = False # Skip paras with banned words\n",
    "                for banned_word in banned_words: \n",
    "                    if banned_word in text:\n",
    "                        skip = True       \n",
    "                for counter_word in counter_words:\n",
    "                    if counter_word in text:\n",
    "                        skip = False\n",
    "                if skip:\n",
    "                    continue\n",
    "\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    matches.append((score, t_m))\n",
    "        \n",
    "    # Only keep matches from the summary if a summary was found. If not keep all matches.\n",
    "    if len(summary_matches) > 0: \n",
    "        matches = summary_matches\n",
    "\n",
    "    # Extract $ value. Determine correct column\n",
    "    regex_number_extraction = r' ?[0-9][0-9|,|.]+[0-9]'\n",
    "    for score, match in matches:\n",
    "        skip = False # Banned words should not appear in final matches\n",
    "        for banned_word in banned_words: \n",
    "            if banned_word in match:    \n",
    "                skip = True\n",
    "                break\n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        amount = re.findall(regex_number_extraction, match, re.IGNORECASE)\n",
    "        extracted_value = clean_money_amount(amount)\n",
    "        if extracted_value is None: # Make sure we are able to extract a value\n",
    "            continue\n",
    "            \n",
    "        value_mapped = False # If we mapped the value into a damage category - stop trying to map into other categories\n",
    "        value_mapped = assign_damage_to_category(extracted_value, general_damage_keywords, match, score, matches, 'General', damages, repetition_detection, repetition_key = ('general',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, special_damage_keywords, match, score, matches, 'Special', damages, repetition_detection, repetition_key = ('special',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, non_pecuniary_damage_keywords, match, score, matches, 'Non-pecuniary', damages, repetition_detection, repetition_key = ('non','pecuniary'))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, aggravated_damage_keywords, match, score, matches, 'Aggravated', damages, repetition_detection, repetition_key = ('aggravated',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, aggravated_damage_keywords, match, score, matches, 'Punitive', damages, repetition_detection, repetition_key = ('punitive',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, future_care_damage_keywords, match, score, matches, 'Future Care', damages, repetition_detection) \n",
    "        if not value_mapped: # Last attempt: Only use \"total amounts\" if nothing else was found\n",
    "            total_keywords = [('total',), ('sum',), ('award',)]\n",
    "            for keywords in total_keywords:\n",
    "                if match_contains_words(match.lower(), keywords):\n",
    "                    if is_best_score(score, matches, keywords):\n",
    "                        if extracted_value not in repetition_detection[('total',)]:\n",
    "                            damages['Pecuniary Total'] = damages['Special'] + damages['General'] + damages['Punitive'] + damages['Aggravated'] + damages['Future Care']\n",
    "                            damages['Total'] = damages['Pecuniary Total'] + damages['Non-pecuniary']\n",
    "                            if damages['Total'] == 0:\n",
    "                                total = extracted_value\n",
    "                                repetition_detection[('total',)].add(extracted_value)\n",
    "                        \n",
    "    damages['Pecuniary Total'] = damages['Special'] + damages['General'] + damages['Punitive'] + damages['Aggravated'] + damages['Future Care']\n",
    "    damages['Total'] = damages['Pecuniary Total'] + damages['Non-pecuniary']\n",
    "    \n",
    "    if damages['Total'] == 0 and total is not None: # Only use the \"total\" if we couldnt find anything else!\n",
    "        damages['Total'] = total\n",
    "        damages['General'] = total\n",
    "        \n",
    "    columns = ['Total', 'Pecuniary Total', 'Non-pecuniary', 'Special', 'General', 'Punitive', 'Aggravated', 'Future Care']\n",
    "    for c in columns:\n",
    "        damages[c] = None if damages[c] == 0 else damages[c]\n",
    "    \n",
    "    return damages\n",
    "\n",
    "def assign_damage_to_category(damage, damage_keywords, match, match_score, matches, damage_type, damage_dict, repetition_dict, repetition_key = None):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Adds damage to dictionary based on given parameters so long as it is the\n",
    "    highest scoring match & doesn't appear in the repetition dictionary\n",
    "    \n",
    "    Argumets:\n",
    "    damage (float) - The damage amount in the match\n",
    "    damage_keywords (list) - Keywords that may appear in match\n",
    "    match (string) - The match string itself\n",
    "    matches (list) - All matches. Used to determine if we found the best match\n",
    "    damage_dict (dict) - Dictionary storing all damages\n",
    "                       - Will be modified in place\n",
    "    repetition_dict (dict) - Dictionary storing repeated values\n",
    "                           - Will be modified in place\n",
    "    (Optional) repetition_key (Tuple) - If not none, will use this key to store repetitions. Else will use matching keyword\n",
    "    \n",
    "    Returns:\n",
    "    value_belongs (Boolean) - True if the value belongs in the given keyword category. False otherwise\n",
    "    '''\n",
    "    match = match.lower()\n",
    "    value_belongs = False\n",
    "    \n",
    "    for keywords in damage_keywords:\n",
    "        if match_contains_words(match, keywords):\n",
    "            value_belongs = True\n",
    "            if is_best_score(match_score, matches, keywords):\n",
    "                if damage not in repetition_dict[repetition_key if repetition_key else keywords]:\n",
    "                    damage_dict[damage_type] += damage\n",
    "                    repetition_dict[repetition_key if repetition_key else keywords].add(damage)\n",
    "            break\n",
    "    \n",
    "    return value_belongs\n",
    "\n",
    "def clean_money_amount(money_regex_match):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Arguments:\n",
    "    money_regex_match (Regex.findall object) - Match of $ amount\n",
    "    \n",
    "    Returns:\n",
    "    None if a bad match\n",
    "    extracted_value (float) - The money amount in float form\n",
    "    '''\n",
    "    # If our regex contains more than 1 or 0 money values. We cannot use the match.\n",
    "    if len(money_regex_match) > 1:\n",
    "        return None\n",
    "    if len(money_regex_match) == 0:\n",
    "        print('Error: No Money in match!', match)\n",
    "        return None\n",
    "    extracted_value = None\n",
    "    amount = money_regex_match[0].replace(',', '')\n",
    "    # Deals with money at end of sentence. example) ... for '5,000.00.' -> '5000.00'\n",
    "    if amount[-1] == '.': \n",
    "        amount = amount[:-1]\n",
    "    if 'million' in amount:\n",
    "        amount = str(float(re.findall('[0-9|\\.]+', amount)[0])*10e6)\n",
    "    # Deals with a rare typo in some cases. example) 50.000.00 -> 50000.00\n",
    "    if amount.count('.') > 1: \n",
    "        dot_count = amount.count('.')\n",
    "        changes_made = 0\n",
    "        new_amount = ''\n",
    "        for letter in amount:\n",
    "            if letter == '.' and changes_made != dot_count-1:\n",
    "                changes_made += 1\n",
    "            else:\n",
    "                new_amount += letter\n",
    "        amount = new_amount\n",
    "    extracted_value = float(amount)\n",
    "    return extracted_value\n",
    "\n",
    "def get_matching_text(patterns, text, max_match_len_split):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given a set of regex; pulls out all matching text\n",
    "    \n",
    "    Arguments:\n",
    "    patterns (list) - List of regex patterns in string format\n",
    "    text (string) - Text to search for matches in\n",
    "    \n",
    "    Returns:\n",
    "    matches (list) - List containing all matches in text format\n",
    "    '''\n",
    "\n",
    "    matches = []\n",
    "    for pattern in patterns:\n",
    "        for match in re.findall(pattern, text, re.IGNORECASE):\n",
    "            if 'and' not in match:\n",
    "                if len(match.split()) <= max_match_len_split:\n",
    "                    matches.append(match)\n",
    "                    \n",
    "    return matches\n",
    "\n",
    "def is_best_score(score, matches, keywords):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given a set of regex matches, determine if the score is the highest score out of all matches for the given keywords\n",
    "    \n",
    "    Arguments:\n",
    "    score (float) - The score of the item you're inspecting\n",
    "    matches (list) - List of matches where each element is of form (score, match text)\n",
    "    keywords (tuple) - All words that should appear in the match\n",
    "    \n",
    "    Returns: True or False\n",
    "    \n",
    "    '''\n",
    "    best_score = score\n",
    "    \n",
    "    for score, match in matches:\n",
    "        if all(word in match.lower() for word in keywords):\n",
    "            if score > best_score:\n",
    "                return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def match_contains_words(match, words):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given some text. Find if the words are all present in the text.\n",
    "    If word begins with '!' the word cannot appear in the text. Can handle mix/matching of both types.\n",
    "    \n",
    "    Arguments:\n",
    "    match (String) - The text to look for words in\n",
    "    words (list) - List of words to check for. If word begins with ! (i.e. '!past'), then the word cannot appear in it\n",
    "    \n",
    "    Returns:\n",
    "    True if all words are present (or not present if using !)\n",
    "    False otherwise\n",
    "    \n",
    "    '''\n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    "    for word in words:\n",
    "        if word.startswith('!'):\n",
    "            neg_words.append(word[1:])\n",
    "        else:\n",
    "            pos_words.append(word)\n",
    "            \n",
    "    if all(word in match for word in pos_words):\n",
    "        if all(word not in match for word in neg_words):\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = rule_based_parse_BCJ('../data/Lexis Cases txt/P1.txt')\n",
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../data/Lexis Cases txt/'\n",
    "list_of_files = os.listdir(path)\n",
    "print(list_of_files)all_cases_parsed =[]\n",
    "\n",
    "for file in list_of_files:\n",
    "    if file != \".DS_Store\" and file != '.ipynb_checkpoints':\n",
    "        all_cases_parsed.extend(rule_based_parse_BCJ(path + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_parsed =[]\n",
    "\n",
    "for file in list_of_files:\n",
    "    if file != \".DS_Store\" and file != '.ipynb_checkpoints':\n",
    "        all_cases_parsed.extend(rule_based_parse_BCJ(path + file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_parsed[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_convert_cases_to_DF(cases):\n",
    "    '''Given a list of parsed cases returns a dataframe'''\n",
    "    lists = defaultdict(list)    \n",
    "    for case in cases:\n",
    "        lists['Case Number'].append(case['case_number'])\n",
    "        lists['Case Name'].append(case['case_title'])\n",
    "        lists['Year'].append(case['year'])\n",
    "        lists['Total Damage'].append(case['damages']['Total'] if case['damages'] != None else None)\n",
    "        lists['Total Pecuniary'].append(case['damages']['Pecuniary Total'] if case['damages'] != None else None)\n",
    "        lists['Non Pecuniary'].append(case['damages']['Non-pecuniary'] if case['damages'] != None else None)\n",
    "        lists['General'].append(case['damages']['General'] if case['damages'] != None else None)\n",
    "        lists['Special'].append(case['damages']['Special'] if case['damages'] != None else None)\n",
    "        lists['Punitive'].append(case['damages']['Punitive'] if case['damages'] != None else None)\n",
    "        lists['Aggravated'].append(case['damages']['Aggravated'] if case['damages'] != None else None)\n",
    "        lists['Future Care'].append(case['damages']['Future Care'] if case['damages'] != None else None)\n",
    "        lists['Judge Name'].append(case['judge'])\n",
    "        lists['Decision Length'].append(case['decision_length'])\n",
    "        lists['Multiple defendants?'].append(case['multiple_defendants'])\n",
    "#         lists['File'].append(case['filename'])\n",
    "        lists['Plaintiff Wins?'].append(case['plaintiff_wins'])\n",
    "        lists['Contributory Negligence Raised'].append(case['contributory_negligence_raised'])\n",
    "        lists['Contributory Negligence Successful'].append(case['contributory_negligence_successful'])\n",
    "        lists['Percent Reduction'].append(case['percent_reduction'])\n",
    "        lists['Written Decision?'].append(case['written_decision'])\n",
    "        lists['Registry'].append(case['registry'])\n",
    "    df = pd.DataFrame()\n",
    "    for key in lists.keys():\n",
    "        df[key] = lists[key]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dev_data, gold_data, subset=None):\n",
    "    # keep track of wrong % reductions\n",
    "    case_titles_incorrect = set()\n",
    "    print('#### Evaluation ####')\n",
    "    # Use case name as 'primary key'\n",
    "    dev_case_names = list(dev_data['Case Name'])\n",
    "    gold_case_names = list(gold_data['Case Name'])\n",
    "    # Filter data to only use overlapping items\n",
    "    gold_data = gold_data[gold_data['Case Name'].isin(dev_case_names)]\n",
    "    dev_data = dev_data[dev_data['Case Name'].isin(gold_case_names)]\n",
    "    # Mapping from our variable names to Lachlan's column names\n",
    "    column_mapping = {'Decision Length': 'Decision Length: paragraphs)',\n",
    "                      'Total Damage': '$ Damages total before contributory negligence',\n",
    "                      'Non Pecuniary': '$ Non-Pecuniary Damages', \n",
    "                      'Total Pecuniary': '$ Pecuniary Damages Total',\n",
    "                      'Special': '$ Special damages Pecuniary (ie. any expenses already incurred)',\n",
    "                      'Future Care': 'Future Care Costs (General Damages)',\n",
    "                      'General': '$ General Damages',\n",
    "                      'Punitive': '$ Punitive Damages',\n",
    "                      'Aggravated': '$Aggravated Damages',\n",
    "                      'Contributory Negligence Raised': 'Contributory Negligence Raised?',\n",
    "                     'Contributory Negligence Successful':'Contributory Negligence Successful?',\n",
    "                     'Percent Reduction':'% Reduction as a result of contributory negligence'\n",
    "                     }\n",
    "    dev_data.rename(columns = column_mapping, inplace = True)\n",
    "    if subset is None: # Use all columns if no subset specified\n",
    "        subset = dev_data.columns\n",
    "    for column in dev_data.columns:\n",
    "        if column in gold_data.columns:\n",
    "            if column in subset:\n",
    "                empty_correct = 0\n",
    "                non_empty_correct = 0\n",
    "                total_empty = 0\n",
    "                total_non_empty = 0\n",
    "                for case_name in list(dev_data['Case Name']):\n",
    "                    dev_value = list(dev_data[dev_data['Case Name'] == case_name][column])[0]\n",
    "                    gold_value = list(gold_data[gold_data['Case Name'] == case_name][column])[0]\n",
    "                    # Convert string to float if possible\n",
    "                    try:\n",
    "                        gold_value = float(gold_value)\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        dev_value = float(dev_value)\n",
    "                    except:\n",
    "                        pass\n",
    "                    # Set values to 'None' if they're a NaN float value\n",
    "                    dev_value = None if isinstance(dev_value, float) and math.isnan(dev_value) else dev_value\n",
    "                    gold_value = None if isinstance(gold_value, float) and math.isnan(gold_value) else gold_value\n",
    "                    # Lowercase values if they're a string\n",
    "                    dev_value = dev_value.lower().strip() if isinstance(dev_value, str) else dev_value\n",
    "                    gold_value = gold_value.lower().strip() if isinstance(gold_value, str) else gold_value\n",
    "                    if gold_value is None:\n",
    "                        total_empty += 1\n",
    "                        if dev_value is None:\n",
    "                            empty_correct += 1\n",
    "                    else:\n",
    "                        total_non_empty += 1\n",
    "                        if isinstance(dev_value, float) and isinstance(gold_value, float):\n",
    "                            if math.isclose(dev_value, gold_value, abs_tol=1): # Tolerance within 1\n",
    "                                non_empty_correct += 1\n",
    "                        elif dev_value == gold_value:\n",
    "                            non_empty_correct += 1\n",
    "                     # trying to trouble shoot % reduction issues       \n",
    "                    if column == '% Reduction as a result of contributory negligence' and gold_value != dev_value:\n",
    "                        case_titles_incorrect.add(case_name)\n",
    "                        print(case_name)\n",
    "                        print(dev_data[dev_data['Case Name'] == case_name]['Case Number'])\n",
    "                        print('gold:', gold_value)\n",
    "                        print('dev:', dev_value)\n",
    "                        print('======')\n",
    "                print('-------')\n",
    "                print('COLUMN:', column)\n",
    "                if total_empty != 0:\n",
    "                    print('Empty field accuracy:', empty_correct / total_empty * 100, '%', empty_correct, '/', total_empty)\n",
    "                if total_non_empty != 0:\n",
    "                    print('Filled field accuracy:', non_empty_correct / total_non_empty * 100, '%', non_empty_correct, '/', total_non_empty)\n",
    "                print('Overall accuracy:', (empty_correct+non_empty_correct) / (total_non_empty+total_empty) * 100, '%', (empty_correct+non_empty_correct), '/', (total_non_empty+total_empty))\n",
    "    return case_titles_incorrect\n",
    "    # for testing:\n",
    "    #return dev_data, gold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = rule_based_convert_cases_to_DF(all_cases_parsed)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gold = pd.read_excel('../data/Case Annotation.xlsx', header=2)\n",
    "gold['Contributory Negligence Successful?'].replace(to_replace = ['Y', 'y'], value = True, inplace = True)\n",
    "gold['Contributory Negligence Successful?'].replace(to_replace = ['N', 'n'], value = False, inplace = True)\n",
    "gold['Contributory Negligence Successful?'].replace(to_replace = [np.nan], value = False, inplace = True)\n",
    "gold['Contributory Negligence Raised?'].replace(to_replace = ['Y', 'y'], value = True, inplace = True)\n",
    "gold['Contributory Negligence Raised?'].replace(to_replace = ['N', 'n'], value = False, inplace = True)\n",
    "gold['Contributory Negligence Raised?'].replace(to_replace = [np.nan], value = False, inplace = True)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold[gold['Case Name'] == 'Gill v. A&P Fruit Growers Ltd., [2009] B.C.J. No. 593']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['Case Name'] == 'Paskall v. Scheithauer, [2012] B.C.J. No. 2601']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "case_titles_incorrect = evaluate(test_df, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_tokenize(case):\n",
    "    ''' String of Entire Document and returns list of lists of paragraphs in document\n",
    "    ---------\n",
    "    Input: case (str) - string of single legal case\n",
    "    Return: docs_split(list) - list of of numbrered paragraphs in the document where the first item is the case_title'''\n",
    "    \n",
    "    case_data = []\n",
    "    lines = case.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    case_data.append(lines[0])\n",
    "    decision_length = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', case).group(1)\n",
    "\n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    pattern = r'.?(?=\\n[0-9]{1,%s}[\\xa0]{2})'%len(decision_length)\n",
    "    paras_split = re.split(pattern, case)\n",
    "\n",
    "    paras = []\n",
    "    for para in paras_split:   \n",
    "        # make sure the paragraph starts with the correct characters\n",
    "        para_start = re.match(r'^\\n([0-9]{1,%s})[\\xa0]{2}'%len(decision_length), para)\n",
    "        if para_start:\n",
    "            paras.append(para)\n",
    "    case_data.extend(paras)\n",
    "    return case_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_tokenize(case):\n",
    "    ''' String of Entire Document and returns the document summary and HELD section.\n",
    "    ---------\n",
    "    Input: doc (str) - string of single legal case\n",
    "    Return: summary - summary and HELD section of case (str)'''\n",
    "    \n",
    "    lines = case.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    \n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    summary = re.search(r'\\([0-9]{1,3} paras\\.\\)\\ncase summary\\n((.*\\n+?)+)(?=HELD|(Statutes, Regulations and Rules Cited:)|(Counsel\\n))', case, re.IGNORECASE)\n",
    "    if summary:\n",
    "        summary = summary.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_and_float(value, text, context_length = 6, plaintiff_name = 'Plaintiff', defendant_name = 'Defendant'):\n",
    "    '''Given a string value found in a body of text, \n",
    "    return a its context of length context_length, and its float equivalent.\n",
    "    -----------------\n",
    "    Arguments:\n",
    "    value - percent match found in text\n",
    "    text - string value where matches were extracted from, eg paragraph or summary (str)\n",
    "    context_length - the length of context around each quantity to return\n",
    "    Rerturn:\n",
    "    value_context - string of context around value (str)\n",
    "    extracted_value - string quantity value extracted to its float equivalent\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # get context for monetary/percent values \n",
    "    \n",
    "    context = ''\n",
    "    amount = re.findall(r'[0-9]+[0-9|,]*(?:\\.[0-9]+)?', value)\n",
    "    extracted_value = clean_money_amount(amount) #use helper function to get float of dollar/percent value\n",
    "    if not extracted_value:\n",
    "        print('cant convert string, %s'%value)\n",
    "        return context, None\n",
    "    # get indices of last instance of value in text - tokenize like this for values of type 'per cent and percent'\n",
    "    start_idx = text.rfind(value)\n",
    "    if start_idx == -1:\n",
    "        print('ERROR: value not in text')\n",
    "    end_idx = start_idx + len(value)\n",
    "    tokens = text[:start_idx].split() + [value] + text[end_idx:].split()\n",
    "#     if 'percent' in value:\n",
    "#         value = amount[0]+'%'\n",
    "#         text = text.replace(value, amount[0]+'%')\n",
    "#     elif 'per cent' in value:\n",
    "#         value = amount[0]+'%'\n",
    "#         text = text.replace(value, amount[0]+'%')\n",
    "#     tokens = text.split()\n",
    "    loc = [i for i, token in enumerate(tokens) if value in token] \n",
    "    # if the quantity is in the text, choose context of last mention of value\n",
    "    if len(loc) > 0:\n",
    "        loc = loc[-1] \n",
    "        if loc - context_length >= 0 and loc + context_length < len(tokens):\n",
    "            context = \" \".join(tokens[loc - context_length:loc + context_length + 1])\n",
    "        elif loc - context_length < 0 and loc + context_length < len(tokens):\n",
    "            beg = abs(loc -context_length)\n",
    "            context = \" \".join(tokens[loc-context_length + beg:loc + context_length + 1])\n",
    "        elif loc - context_length > 0 and loc + context_length > len(tokens): \n",
    "            context = \" \".join(tokens[loc - context_length:len(tokens)])\n",
    "\n",
    "\n",
    "    return context.lower(), extracted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_parse_BCJ(path):\n",
    "    '''Given file path (text file) of negligence cases, finds static \n",
    "    information within the case (information that can be pattern matched)\n",
    "    Expects a B.C.J. case format (British Columbia Judgments)\n",
    "    \n",
    "    The following fields are currently implemented:\n",
    "    - Case Title\n",
    "    - Judge Name\n",
    "    - Registry\n",
    "    - Year\n",
    "    - Decision Length (in paragraphs)\n",
    "    - Damages\n",
    "    - Multiple Defendants\n",
    "    - Plaintiff Wins\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: case_parsed_data (list) of case_dict (Dictionary): List of Dictionaries with rule based parsable fields filled in\n",
    "    '''\n",
    "    if path:\n",
    "        with open(path, encoding='utf-8') as document:\n",
    "            document_data = document.read()\n",
    "        document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "    \n",
    "\n",
    "    case_parsed_data = []\n",
    "    for i in range(len(document_data)):\n",
    "        case_dict = dict() \n",
    "        case = document_data[i]\n",
    "        case = case.strip() # Make sure to strip!\n",
    "        if len(case) == 0: # Skip empty lines\n",
    "            continue\n",
    "        \n",
    "        lines = case.split('\\n')\n",
    "        if len(lines) < 2:\n",
    "            print(case)\n",
    "        case_title = lines[0]\n",
    "        case_type = lines[1]\n",
    "\n",
    "        if 'R. v.' in case_title or '(Re)' in case_title: # Skip crown cases, Skip (Re) cases\n",
    "            continue\n",
    "            \n",
    "        # Skip client/solicitor cases (not same as plaintiff/defendant)\n",
    "        regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?|[L|l]awyer[s]?))', case)\n",
    "        if regex_client_solicitor:\n",
    "            continue\n",
    "        \n",
    "        regex_solicitor_client = re.search(r'(Between.*([L|l]awyer[s]?|[S|s]olicitor[s]?).*([C|c]lient[s]?))', case)\n",
    "        if regex_solicitor_client:\n",
    "            continue\n",
    "            \n",
    "        # In some rare cases we have 'IN THE MATTER OF ..' (rather than 'Between ...') .. but it is following by the normal\n",
    "        # plaintiff/defendant dynamic. Only skip cases if there is no mention of the following terms\n",
    "        # (Can be cleaned up in future)\n",
    "        key_words = ['appellant', 'respondent', 'claimant', 'petitioner', 'plaintiff', 'defendant',\n",
    "        'appellants', 'respondents', 'claimants', 'petitioners', 'plaintiffs', 'defendants']\n",
    "        regex_in_matter_of = re.search(r'IN THE MATTER OF .*\\n\\([0-9]+ paras.\\)', case)\n",
    "        if regex_in_matter_of:\n",
    "            remove = True\n",
    "            for key in key_words:\n",
    "                if key in regex_in_matter_of.group(0).lower().strip():\n",
    "                    remove = False\n",
    "                    \n",
    "            if remove:\n",
    "                continue\n",
    "\n",
    "        if 'British Columbia Judgments' in case_type: # Make sure we're dealing with a B.C.J. case\n",
    "        \n",
    "            # Fields that can be found via pattern matching\n",
    "            if re.search('contributory negligence', case, re.IGNORECASE):\n",
    "                contributory_negligence_raised = True\n",
    "            else:\n",
    "                contributory_negligence_raised = False\n",
    "            case_number = re.search(r'\\/P([0-9]+)\\.txt', path).group(1)\n",
    "            decision_len = re.search(r'\\(([0-9]+) paras\\.?\\)', case) # e.g.) (100 paras.)\n",
    "            registry = re.search(r'(Registry|Registries): ?([A-Za-z0-9 ]+)', case) # e.g.) Registry: Vancouver\n",
    "            written_decision = True if int(decision_len.group(1)) > 1 else False\n",
    "            if registry:\n",
    "                registry = registry.group(2).strip()\n",
    "            else:\n",
    "                registry = re.search(r'([A-Za-z ]+) Registry No.', case) # Alt form e.g.) Vancouver Registory No. XXX\n",
    "                if registry:\n",
    "                    registry = registry.group(1).strip()\n",
    "                else:\n",
    "                    registry = re.search(r'([A-Za-z ]+) No. S[0-9]*', case)\n",
    "                    if registry:\n",
    "                        registry = registry.group(1).strip()\n",
    "                    else:\n",
    "                        print('WARNING: Registry could not be found (This shouldn\\'t occur!)')\n",
    "            # Fields that are always in the same place\n",
    "            judge_name = lines[4].strip()\n",
    "            case_title = lines[0].strip()\n",
    "            # Extract year from case_title (in case we want to make visualizations, etc.)\n",
    "            year = re.search(r'20[0-2][0-9]', case_title) # Limit regex to be from 2000 to 2029\n",
    "            if year:\n",
    "                year = year.group(0)\n",
    "            else:\n",
    "                # Rare case: Sometimes the title is too long. Rely on Heard date.\n",
    "                year = re.search(r'Heard:.* ([2][0][0-2][0-9])', case)\n",
    "                if year:\n",
    "                    year = year.group(1)\n",
    "                else:\n",
    "                    print('WARNING: Year not found')\n",
    "            case_dict['case_number'] = '%s of %s'%(i+1+((int(case_number)-1)*50), case_number)\n",
    "            case_dict['case_title'] = case_title\n",
    "            case_dict['year'] = year\n",
    "            case_dict['registry'] = registry\n",
    "            case_dict['judge'] = judge_name\n",
    "            case_dict['decision_length'] = decision_len.group(1)\n",
    "            case_dict['multiple_defendants'] = rule_based_multiple_defendants_parse(case)\n",
    "            case_dict['contributory_negligence_raised'] = contributory_negligence_raised\n",
    "            case_dict['written_decision'] = written_decision\n",
    "            \n",
    "            # TODO: Improve plaintiff_wins to take one case at a time.\n",
    "            plaintiff_list = plaintiff_wins(path)\n",
    "            if case_title in plaintiff_list:\n",
    "                case_dict['plaintiff_wins'] = plaintiff_list[case_title]\n",
    "            else:\n",
    "                case_dict['plaintiff_wins'] = \"NA\"\n",
    "                \n",
    "            case_dict['damages'] = rule_based_damage_extraction(case)\n",
    "            percent_reduction, contributory_negligence_successful = get_percent_reduction_and_contributory_negligence_success(case_dict, case)\n",
    "            case_dict['percent_reduction'] = percent_reduction\n",
    "            case_dict['contributory_negligence_successful'] = contributory_negligence_successful\n",
    "        # don't add empty dictionaries (non BCJ cases) to list\n",
    "        if case_dict != dict(): \n",
    "            case_parsed_data.append(case_dict)\n",
    "    return case_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities):\n",
    "    ''' Given the context surrounding an extracted value, keywords relevant to contributory negligence, \n",
    "    a list of the Plaintiffs names, a list of the defendants names, and a combined list of entities related to either the Plaintiff or Defendant:\n",
    "    Return the modifed extracted value\n",
    "    ------------\n",
    "    Arugments:\n",
    "    context: (str)\n",
    "    extracted_value: (float) found in context\n",
    "    keywords, plaintiff_split, defendant_split, entities: (list) of strings\n",
    "    '''\n",
    "    # conditions for keeping extracted_value and updating extracted_value\n",
    "    # skip extracted_values with contexts lacking keywords/entities\n",
    "    if extracted_value == 100 or extracted_value == 0 or extracted_value < 10:\n",
    "        extracted_value = None\n",
    "        return extracted_value\n",
    "    if not any(token in context for token in keywords + entities) or context == '' or any('costs' == token for token in context.split()) or ('interest' in context and 'rate' in context.split()):\n",
    "        extracted_value = None\n",
    "        return extracted_value\n",
    "    if 'recover' in context and any(word in context for word in plaintiff_split + ['plaintiff']):\n",
    "#         print('plaintiff recovers percent subtract %s from 100'%extracted_value)\n",
    "        extracted_value = 100 - extracted_value\n",
    "    if any(word1 in context and word2 in context for word1 in defendant_split + ['defendant'] for word2 in ['liable', 'responsible', 'fault', 'against']):\n",
    "#         print('defendat is %s liable, subtract from 100'%extracted_value)\n",
    "        extracted_value = 100 - extracted_value\n",
    "    return extracted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['against', 'reduce', 'liability', 'liable', 'contributor', 'fault', 'apportion', 'recover', 'responsible']\n",
    "entities = ['defendant', 'plaintiff', 'she', 'he', 'John', 'Jane']\n",
    "context1 = 'was held 90%'\n",
    "test =keywords + entities\n",
    "\n",
    "if not any(word in context for word in test):\n",
    "    print('fdsfm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contributory_negligence_successful_fun(context, keywords):\n",
    "    '''Given text containing percent reduction and a list of keywords to check for,\n",
    "    confirm presence of keywords and return whether or not contributory negligence was successful\n",
    "    --------------\n",
    "    Arguments:\n",
    "    context (str)\n",
    "    keywords(list)\n",
    "    Returns: True or None (bool)'''\n",
    "    if any(word in context for word in keywords):\n",
    "        if 'plaintiff' or 'damages' or 'defendant' in context:\n",
    "            contributory_negligence_successful = True\n",
    "            return contributory_negligence_successful\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_reduction_and_contributory_negligence_success(case_dict, case, min_score = 0.9):\n",
    "    paragraphs = paragraph_tokenize(case)\n",
    "    case_title = case_dict['case_title']\n",
    "    assert paragraphs[0] == case_title\n",
    "    # default value for contributory negligence success is FALSE\n",
    "    contributory_negligence_successful = False\n",
    "    percent_pattern = r'([0-9][0-9|\\.]*(?:%|\\sper\\s?cent))'\n",
    "    # entities and keywords used to filter percent values\n",
    "    keywords = ['against', 'reduce', 'liability', 'liable', 'contributor', 'fault', 'apportion', 'recover', 'responsible']\n",
    "    # extract plaintiff and defendant name for use in %reduction conditions\n",
    "    plaintiff_defendant_pattern = r'([A-Za-z|-|\\.]+(:? \\(.*\\))?)+ v\\. ([A-Za-z|-]+)+' # group 1 is plaintiff group 2 is defendant\n",
    "    if re.search(plaintiff_defendant_pattern, case_title):\n",
    "        plaitiff_defendant = re.search(plaintiff_defendant_pattern, case_title).groups() # tuple (plaintiff, defendant)\n",
    "    else:\n",
    "        plaitiff_defendant = ('Plaintiff', 'Defendant')\n",
    "    plaintiff_split = [word.lower() for word in plaitiff_defendant[0].split()]\n",
    "    defendant_split = [word.lower() for word in plaitiff_defendant[-1].split()]\n",
    "    entities = ['defendant', 'plaintiff'] + plaintiff_split + defendant_split \n",
    "\n",
    "    if case_dict['contributory_negligence_raised'] and case_dict['plaintiff_wins']:\n",
    "        #### troubleshooting~~\n",
    "        if case_title in case_titles_incorrect:\n",
    "            print(case_title)\n",
    "        percent_reduction = None\n",
    "        best_percent = None\n",
    "        best_score = 0\n",
    "        for j, paragraph in enumerate(paragraphs[1:]):\n",
    "            score = float((j+1)/int(case_dict['decision_length']))\n",
    "            paragraph = paragraph.lower()\n",
    "            if not score >= min_score: ## min score not existant in bcj parser\n",
    "                continue\n",
    "\n",
    "            percent_mentioned = re.findall(percent_pattern, paragraph, re.IGNORECASE)\n",
    "            extracted_value_tie_breaker = Counter()\n",
    "            if len(percent_mentioned) > 0:\n",
    "                for percent in percent_mentioned:\n",
    "                    context, extracted_value = get_context_and_float(percent, paragraph)\n",
    "                    # conditions for keeping extracted_value and updating extracted_value\n",
    "                    # skip extracted_values with contexts lacking keywords/entities\n",
    "                    if context == '':\n",
    "                        continue\n",
    "                    extracted_value = conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities)\n",
    "                    if not extracted_value:\n",
    "                        \n",
    "                        continue\n",
    "                        \n",
    "                    extracted_value_tie_breaker.update([extracted_value])\n",
    "                \n",
    "                    # conditions for contributory negligence successful\n",
    "                    if not contributory_negligence_successful and extracted_value:\n",
    "                        contributory_negligence_successful = contributory_negligence_successful_fun(context, keywords)\n",
    "                    #### troubleshooting~~\n",
    "                    if case_title in case_titles_incorrect:\n",
    "                        print(extracted_value_tie_breaker, context)\n",
    "                        \n",
    "                    # matches patter \"PERCENT against plaintiff\"\n",
    "                    if ('against' in context or 'fault' in context) and any(plaintiff_word in context for plaintiff_word in plaintiff_split+['plaintiff']):\n",
    "                        best_percent = extracted_value\n",
    "                        best_score = score\n",
    "                        break                    \n",
    "                    \n",
    "                    # choose most common percent mentioned in highest scoring paragraph\n",
    "                    if extracted_value_tie_breaker != Counter():\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_percent = extracted_value_tie_breaker.most_common(1)[0][0]\n",
    "\n",
    "                #### troubleshooting~~\n",
    "                if case_title in case_titles_incorrect:\n",
    "                    print(\"paragraph:\", best_score, best_percent)\n",
    "                    print('======')\n",
    "             # if no percent found, check for equal apportionment\n",
    "            else:\n",
    "                equal_apportionment = re.findall(r'.{20} (?:liability|fault) [a-zA-Z]{1,3} apportione?d? equally .{20}', paragraph)\n",
    "                if len(equal_apportionment) > 0:\n",
    "                    if contributory_negligence_successful_fun(equal_apportionment[0], keywords):\n",
    "                        best_percent = 50.0\n",
    "                        contributory_negligence_successful = True\n",
    "        \n",
    "        if best_score == 0 or not best_percent or not contributory_negligence_successful:\n",
    "            # no percents found in paragraphs - time to check summary - same process\n",
    "            summary = summary_tokenize(case)\n",
    "            if summary:\n",
    "                summary = summary.lower()\n",
    "                percent_mentioned = re.findall(percent_pattern, summary, re.IGNORECASE)\n",
    "                #### troubleshooting~~\n",
    "                if case_title in case_titles_incorrect:\n",
    "                    print('checking summary...')\n",
    "                    print(percent_mentioned)\n",
    "                extracted_value_tie_breaker = Counter()\n",
    "                if len(percent_mentioned) > 0:\n",
    "                    for percent in percent_mentioned:\n",
    "                        context, extracted_value = get_context_and_float(percent, summary)\n",
    "                        #### troubleshooting~~\n",
    "                        if case_title in case_titles_incorrect:\n",
    "                            print(extracted_value, context)\n",
    "                        # conditions for keeping extracted_value and updating extracted_value\n",
    "                        # skip extracted_values with contexts lacking keywords/entities\n",
    "                        extracted_value = conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities)\n",
    "                        if not extracted_value:\n",
    "                            continue\n",
    "                        extracted_value_tie_breaker.update([extracted_value])\n",
    "                                                   \n",
    "                        # conditions for contributory negligence successful\n",
    "                        if not contributory_negligence_successful and extracted_value:\n",
    "                            contributory_negligence_successful = contributory_negligence_successful_fun(context, keywords) \n",
    "                            \n",
    "                        # matches patter \"PERCENT against plaintiff\"\n",
    "                        if ('against' in context or 'fault' in context) and any(plaintiff_word in context for plaintiff_word in plaintiff_split+['plaintiff']):\n",
    "                            best_percent = extracted_value\n",
    "                            best_score = score\n",
    "                            break \n",
    "                        # choose most common percent mentioned in summary\n",
    "                        if extracted_value_tie_breaker != Counter():\n",
    "                            best_percent = extracted_value_tie_breaker.most_common(1)[0][0]\n",
    "                        \n",
    "                        #### troubleshooting~~\n",
    "                        if case_title in case_titles_incorrect:\n",
    "                            print(\"summary:\", best_score, best_percent)\n",
    "                            print('======')\n",
    "               # if no percent found, check for equal apportionment\n",
    "                else:\n",
    "                    #### troubleshooting~~\n",
    "                    if case_title in case_titles_incorrect:\n",
    "                        print('checking equal apportionment...')\n",
    "                    equal_apportionment = re.findall(r'.{20} (?:liability|fault) [a-zA-Z]{1,3} apportione?d? equally .{20}', summary)\n",
    "                    if len(equal_apportionment) > 0:\n",
    "                        if contributory_negligence_successful_fun(equal_apportionment[0], keywords):\n",
    "                            best_percent = 50.0\n",
    "                            contributory_negligence_successful = True\n",
    "        if contributory_negligence_successful:\n",
    "            percent_reduction = best_percent\n",
    "    else:\n",
    "        percent_reduction = None\n",
    "    #### troubleshooting~~\n",
    "    if case_title in case_titles_incorrect:\n",
    "        print(percent_reduction)    \n",
    "    return percent_reduction, contributory_negligence_successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Case Summary\n",
    "Damages — Mitigation — In tort — Personal injuries, treatment for — Torts — Negligence — Standard of care, particular persons and relationships — Police officers — Motor vehicle, standard of care of driver — Keeping a proper lookout — Emergencies — Circumstances requiring caution or extreme caution — Emergency or police vehicles — Defences — Contributory negligence — Apportionment of fault.\n",
    "Action by Blackburn for damages for injuries suffered when her vehicle collided with a police car at an intersection. Constable Leyh was on his way to another accident and was proceeding through the intersection with his lights and siren activated. He slowed down as he approached the intersection and began speeding up half-way through it. Blackburn was a hearing impaired 17-year-old driver who was completely deaf without her hearing aids. Although she was wearing her hearing aids, witnesses stated that her radio was playing loudly. Blackburn did not hear the siren at all. Several vehicles in the lane next to Blackburn's lane had stopped at the green light. Blackburn proceeded through the intersection on the green light and was struck by the police car proceeding through the intersection on the red light. Constable Leyh had stopped at his home for his rain coat prior to leaving for the scene of the accident. As a result of the collision, Blackburn suffered a moderately severe cervical sprain and post-traumatic stress. However, she failed to follow the recommended course of treatment. \n",
    "\n",
    "HELD: Action allowed in part.\n",
    " Constable Leyh took a calculated risk that was not proportionate to the urgency of the situation. The amount of time consumed by retrieving his raincoat was much greater than the few seconds that would have been consumed by momentarily delaying his acceleration. Speeding up where there was still a risk of a vehicle approaching was not justified by the urgency of the situation. Blackburn failed to observe that traffic in the lane next to her had stopped at the green light and did not reduce her speed or exercise appropriate caution as she approached the intersection. Visual attentiveness took on an added importance in respect of a deaf person driving a car. Blackburn should have paid special attention to visual clues and should not have had the volume of her radio turned up loudly. Fault for the accident was apportioned 80 per cent to Blackburn and 20 percent to Constable Leyh. Taking into account Blackburn's failure to mitigate her damages, her non-pecuniary losses were assessed at $30,000 subject to the liability apportionment.'''\n",
    "get_context_and_float('80', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations of Numbers/Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_annotations = set(gold.iloc[:44, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../data/Lexis Cases txt/'\n",
    "list_of_files = os.listdir(path)\n",
    "to_annotate = dict()\n",
    "\n",
    "# f = open('../data/ilanas_annotations_.txt', 'w')\n",
    "for file in list_of_files:\n",
    "    if file != \".DS_Store\" and file != '.ipynb_checkpoints':\n",
    "        with open(path+file, encoding='utf-8') as document:\n",
    "            document_data = document.read()\n",
    "        document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "        for i in range(len(document_data)):\n",
    "            case = document_data[i]\n",
    "            if len(case) == 0: # Skip empty lines\n",
    "                continue\n",
    "            lines = case.split('\\n')\n",
    "            case_title = lines[0]\n",
    "            if case_title in my_annotations:\n",
    "                to_annotate[case_title] = re.findall(r'\\$ ?[0-9][0-9|,|.]+[0-9]', case)\n",
    "#                 f.write(case + 'End of Document\\n')\n",
    "            \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case2tags(case):\n",
    "    '''Input case is a string from the legal negligence case, with xml tags indicating damage types.\n",
    "    Return a list of quantities tagged (str) and a list of the associated damage type tags corresponding to those values\n",
    "    -------------------\n",
    "    Example: \n",
    "    case = \"I asses non-pecuniary damages of <damage type=non pecuniary>$1,000,000</damage>\"\n",
    "    case2tags(case) = ['$1,000,000'], ['non pecuniary']\n",
    "    '''\n",
    "    # your code here\n",
    "    soup = BeautifulSoup('<xml>'+case+'</xml>', \"xml\")\n",
    "    tags = []\n",
    "    values = []\n",
    "    full_match = []\n",
    "    for damage in soup.find_all('damage'):\n",
    "        if 'non-pecuniary' in damage['type']:\n",
    "            tags.append(damage['type'].replace('non-pecuniary', 'non pecuniary'))\n",
    "        else:\n",
    "            tags.append(damage['type'])\n",
    "        values.append(damage.get_text())\n",
    "    return values, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'total': 105,\n",
       "         'other': 1670,\n",
       "         'total after': 16,\n",
       "         'non pecuniary': 126,\n",
       "         'past wage loss': 108,\n",
       "         'sub-future wage loss': 22,\n",
       "         'future care': 97,\n",
       "         'special': 117,\n",
       "         'future wage loss': 91,\n",
       "         'reduction': 2,\n",
       "         'sub-past wage loss': 18,\n",
       "         'sub-special': 58,\n",
       "         'sub-future care': 41,\n",
       "         'general': 31,\n",
       "         'punitive': 7,\n",
       "         'in trust': 4,\n",
       "         'reduction by': 5,\n",
       "         'aggravated': 8,\n",
       "         'sub-non pecuniary': 8,\n",
       "         'sub-total': 12,\n",
       "         'sub-general': 15,\n",
       "         'reduction to': 8,\n",
       "         'past income loss': 1})"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mennonite Church British Columbia v. Sur-Del Roofing Ltd., [2010] B.C.J. No. 297\n",
      "12\n",
      "Paskall v. Scheithauer, [2012] B.C.J. No. 2601\n",
      "23\n",
      "Salgado v. Toth, [2009] B.C.J. No. 2230\n",
      "47\n",
      "Gray v. Fraser Health Authority (c.o.b. Ridge Meadows Hospital), [2009] B.C.J. No. 372\n",
      "40\n",
      "Kirkham v. Richardson, [2014] B.C.J. No. 1194\n",
      "76\n",
      "Neff v. Patry, [2008] B.C.J. No. 209\n",
      "14\n",
      "Hardychuk v. Johnstone, [2012] B.C.J. No. 1909\n",
      "63\n",
      "Najdychor v. Swartz, [2009] B.C.J. No. 1202\n",
      "64\n",
      "Delgiglio v. Becker, [2012] B.C.J. No. 650\n",
      "45\n",
      "Rycroft v. Rego, [2017] B.C.J. No. 447\n",
      "30\n",
      "Roger Garside Construction Ltd. v. Stirling, [2013] B.C.J. No. 1777\n",
      "2\n",
      "Zhang v. Law, [2009] B.C.J. No. 1468\n",
      "27\n",
      "Chamberlain v. Pro Star Mechanical Technologies Ltd., [2014] B.C.J. No. 2669\n",
      "33\n",
      "Andrusko v. Alexander, [2013] B.C.J. No. 1161\n",
      "51\n",
      "Anderson v. Minhas, [2011] B.C.J. No. 259\n",
      "22\n",
      "Berenjian v. Primus, [2013] B.C.J. No. 194\n",
      "26\n",
      "Kumar v. Picco, [2007] B.C.J. No. 2463\n",
      "29\n",
      "Gillespie v. Yellow Cab Co., [2014] B.C.J. No. 2332\n",
      "49\n",
      "Gregory v. Penner, [2010] B.C.J. No. 32\n",
      "43\n",
      "Kuras v. Repo, [2014] B.C.J. No. 2204\n",
      "35\n",
      "Okanagan-Similkameen (Regional District) v. Associated Engineering (B.C.) Ltd., [2015] B.C.J. No. 1105\n",
      "92\n",
      "Kerr (Litigation Guardian of) v. Creighton, [2007] B.C.J. No. 309\n",
      "0\n",
      "Rosso v. Balubal, [2014] B.C.J. No. 2385\n",
      "29\n",
      "Akbari v. Insurance Corp. of British Columbia, [2012] B.C.J. No. 2451\n",
      "17\n",
      "Lumanlan v. Sadler, [2008] B.C.J. No. 2184\n",
      "46\n",
      "Gregorowicz (Litigation guardian of) v. Lee, [2010] B.C.J. No. 636\n",
      "4\n",
      "Dzumhur v. Davoody, [2015] B.C.J. No. 2730\n",
      "87\n",
      "Evans v. Keill, [2018] B.C.J. No. 3274\n",
      "72\n",
      "Eaton v. Regan, [2005] B.C.J. No. 240\n",
      "40\n",
      "Hawkenson v. Rogers, [2005] B.C.J. No. 456\n",
      "77\n",
      "Leweke v. Saanich School District No. 63, [2004] B.C.J. No. 1985\n",
      "52\n",
      "Kilian v. Valentin, [2012] B.C.J. No. 2009\n",
      "64\n",
      "Cox v. Bounthavilay, [2007] B.C.J. No. 1776\n",
      "12\n",
      "Blackburn v. British Columbia, [2001] B.C.J. No. 1647\n",
      "14\n",
      "First Majestic Silver Corp. v. Santos, [2013] B.C.J. No. 834\n",
      "65\n",
      "Koshman v. Brodis, [2013] B.C.J. No. 755\n",
      "50\n",
      "Gill v. A&P Fruit Growers Ltd., [2009] B.C.J. No. 593\n",
      "1\n",
      "Kahl v. Jakobsson, [2006] B.C.J. No. 1722\n",
      "52\n",
      "Mainardi v. Shannon, [2005] B.C.J. No. 1033\n",
      "2\n",
      "0813054 B.C. Ltd. v. Overland West Freight Lines Ltd., [2013] B.C.J. No. 2829\n",
      "12\n",
      "Rhodes v. Biggar, [2010] B.C.J. No. 1022\n",
      "37\n",
      "Mawani v. Pitcairn, [2012] B.C.J. No. 1819\n",
      "0\n",
      "Ediger (Guardian ad litem of) v. Johnston, [2009] B.C.J. No. 564\n",
      "90\n",
      "Furness v. Guest, [2010] B.C.J. No. 1388\n",
      "23\n",
      "Fabian v. Song, [2018] B.C.J. No. 896\n",
      "60\n",
      "Aiken (Guardian ad litem of) v. Van Dyk, [2001] B.C.J. No. 1751\n",
      "0\n",
      "Millard v. Singleton, [2015] B.C.J. No. 1234\n",
      "0\n",
      "Dr. Andrew Hokhold Inc. v. Wells (c.o.b. Spall Machine & Welding), [2005] B.C.J. No. 255\n",
      "14\n",
      "Cowie v. Draper, [2010] B.C.J. No. 910\n",
      "0\n",
      "Jackson v. Fisheries and Oceans Canada, [2006] B.C.J. No. 2654\n",
      "0\n",
      "Kappell v. Brown, [2012] B.C.J. No. 139\n",
      "27\n",
      "Bjornson v. Field, [2007] B.C.J. No. 2734\n",
      "38\n",
      "Austin v. Joaquin, [2007] B.C.J. No. 1894\n",
      "4\n",
      "Los Angeles Salad Co. v. Canadian Food Inspection Agency, [2009] B.C.J. No. 161\n",
      "0\n",
      "Fichtner v. Johnston Meier Insurance Services Ltd., [2001] B.C.J. No. 1666\n",
      "2\n",
      "Strata Plan NW 3341 v. Canlan Ice Sports Corp., [2001] B.C.J. No. 1723\n",
      "17\n",
      "Brooks-Martin v. Martin, [2011] B.C.J. No. 243\n",
      "0\n",
      "Mclaren v. Rice, [2009] B.C.J. No. 2108\n",
      "8\n",
      "Neidermayer v. Gillies, [2012] B.C.J. No. 183\n",
      "38\n",
      "Gibson v. Matthies, [2017] B.C.J. No. 965\n",
      "0\n",
      "Brito (Guardian ad litem of) v. Woolley, [2001] B.C.J. No. 1692\n",
      "153\n",
      "Ruchelski v. Moore, [2013] B.C.J. No. 561\n",
      "63\n",
      "Abbott v. Gerges, [2014] B.C.J. No. 1848\n",
      "54\n",
      "N&C Transportation Ltd. v. Navistar International Corp., [2016] B.C.J. No. 2369\n",
      "2\n",
      "Harder v. Poettcker, [2015] B.C.J. No. 2579\n",
      "7\n",
      "Paniccia v. Eckert, [2012] B.C.J. No. 1997\n",
      "21\n",
      "Intrawest Corp. v. Hart, [2002] B.C.J. No. 301\n",
      "31\n",
      "[84, 87]\n",
      "1,340.17\n",
      "['profit:', '1,340.17', 'gst', '(on', '$16,082.00)'] ['(on', '$16,082.00)', '1,125.74', 'total:', '$17,207.74']\n",
      "Jacobs v. Basil, [2017] B.C.J. No. 1517\n",
      "7\n",
      "Johal v. Conron, [2013] B.C.J. No. 2318\n",
      "41\n",
      "Van den Hemel v. Kugathasan, [2010] B.C.J. No. 1767\n",
      "45\n",
      "Ho v. Lau, [2012] B.C.J. No. 2561\n",
      "7\n",
      "Thiessen v. Mutual Life Assurance Co. of Canada, [2001] B.C.J. No. 1849\n",
      "18\n",
      "Rackstraw (Litigation guardian of) v. Robertson, [2011] B.C.J. No. 1354\n",
      "0\n",
      "Ahlwat v. Green, [2014] B.C.J. No. 2452\n",
      "0\n",
      "Mardones v. Toyota Credit Canada Inc., [2008] B.C.J. No. 1217\n",
      "7\n",
      "Ellis (Litigation guardian of) v. Duong, [2017] B.C.J. No. 546\n",
      "54\n",
      "McGavin v. Talbot, [2017] B.C.J. No. 2439\n",
      "55\n",
      "Aberdeen v. Langley (Township), [2007] B.C.J. No. 1515\n",
      "84\n",
      "C.H. v. British Columbia, [2003] B.C.J. No. 1706\n",
      "38\n",
      "Logeman v. Rossa, [2006] B.C.J. No. 963\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "case_info = defaultdict(dict)\n",
    "case_tags = []\n",
    "tag_distribution = Counter()\n",
    "\n",
    "# define context length for use in features\n",
    "context_length = 5\n",
    "features_per_case = []\n",
    "path = \"../data/\"\n",
    "list_of_files = os.listdir(path)\n",
    "\n",
    "#iterate over new annotations\n",
    "for file in list_of_files:\n",
    "    if file == \"ilanas_annotations.txt\" or file == \"ravi_annotations.txt\":\n",
    "        with open(path+file) as f:\n",
    "            cases = f.read()\n",
    "        cases = cases.split('End of Document\\n')\n",
    "        for i in range(len(cases)):\n",
    "            case = cases[i]\n",
    "            if len(case) == 0: # Skip empty lines\n",
    "                continue\n",
    "            lines = case.split('\\n')\n",
    "            case_title = lines[0]\n",
    "            case_type = lines[1]\n",
    "\n",
    "            # skip irrelevant cases\n",
    "            if 'R. v.' in case_title or '(Re)' in case_title: # Skip crown cases, Skip (Re) cases\n",
    "                    continue\n",
    "\n",
    "            # Skip client/solicitor cases (not same as plaintiff/defendant)\n",
    "            regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?|[L|l]awyer[s]?))', case)\n",
    "            if regex_client_solicitor:\n",
    "                continue\n",
    "            regex_solicitor_client = re.search(r'(Between.*([L|l]awyer[s]?|[S|s]olicitor[s]?).*([C|c]lient[s]?))', case)\n",
    "            if regex_solicitor_client:\n",
    "                continue\n",
    "\n",
    "            key_words = ['appellant', 'respondent', 'claimant', 'petitioner', 'plaintiff', 'defendant',\n",
    "                'appellants', 'respondents', 'claimants', 'petitioners', 'plaintiffs', 'defendants']\n",
    "            regex_in_matter_of = re.search(r'IN THE MATTER OF .*\\n\\([0-9]+ paras.\\)', case)\n",
    "            if regex_in_matter_of:\n",
    "                remove = True\n",
    "                for key in key_words:\n",
    "                    if key in regex_in_matter_of.group(0).lower().strip():\n",
    "                        remove = False\n",
    "                if remove:\n",
    "                    continue\n",
    "                    \n",
    "            # Make sure we're dealing with a B.C.J. case   \n",
    "            if 'British Columbia Judgments' in case_type: \n",
    "                \n",
    "                #remove stopwords and lower case\n",
    "                case = ' '.join([word for word in case.lower().split() if word not in stop_words])\n",
    "                #get tagged values from annotations\n",
    "                values, tags = case2tags(case)\n",
    "                case_tags.append(tags)\n",
    "                \n",
    "                # save tags, values per case in dictionary for reference\n",
    "                case_info[case_title]['values'] = values\n",
    "                case_info[case_title]['tags'] = tags\n",
    "                tag_distribution.update(tags)\n",
    "\n",
    "                # get context of tagged values in case\n",
    "                print(case_title)\n",
    "                print(len(values))\n",
    "                visited_indices = set()\n",
    "                case_feats = []\n",
    "                for value, tag in zip(values, tags):\n",
    "                    # save features in dictionary\n",
    "                    features = dict()\n",
    "                    \n",
    "                    # temp value is equal to value without $ symbol - useful for regex escape issues\n",
    "                    temp = value\n",
    "                    if '$' in value:\n",
    "                        temp = value.replace('$', '')\n",
    "                    pattern = re.compile('''<damage type=['|\"]%s['|\"]>\\$?%s<\\/damage>'''%(tag, temp))\n",
    "                    matches = pattern.finditer(case)\n",
    "                    for match in matches:\n",
    "                        if match.start() in visited_indices: # dont want to add the same context twice\n",
    "                            continue\n",
    "                        start_idx = match.start()\n",
    "                        end_idx = match.end()\n",
    "                        \n",
    "                        # first use nltk sent_tokenize to get sentence on either side of value only\n",
    "                        tokens = sent_tokenize(case[:start_idx])[-1] + \" \"+ value + \" \" + sent_tokenize(case[end_idx:])[0] #sentence before and after value\n",
    "                        tokens = tokens.split()\n",
    "                        # get indices of quantity value in text\n",
    "                        loc = [i for i, token in enumerate(tokens) if value == tokens[i] or i+1<len(tokens) and value == tokens[i]+\" \"+tokens[i+1]] #case $3 million split on whitespace\n",
    "\n",
    "                        # remove tags from text for context\n",
    "                        tokens = \" \".join(tokens)\n",
    "                        tokens = re.sub(\"<damage\", \"\", tokens)\n",
    "                        tokens = re.sub(\"<\\/damage>\", \"\", tokens)\n",
    "                        tokens = re.sub(\"type='[a-z| |-]+'>\", '', tokens).split()\n",
    "                        \n",
    "                        # there should always be (at least) one match - ideally one...\n",
    "                        if len(loc) < 1:\n",
    "                            print(\"WHYY\")\n",
    "                            print(match.start())\n",
    "                            print(value)\n",
    "                            continue\n",
    "                        if len(loc) > 1:\n",
    "                            print(loc)\n",
    "                            print(value)\n",
    "                            print(tokens[loc[0]-5:loc[0]], tokens[loc[-1]-5:loc[-1]])\n",
    "                            loc = loc[-1] #if more than one match, choose last one\n",
    "                        else:\n",
    "                            loc = loc[0] \n",
    "\n",
    "\n",
    "                        # context before and after of length: context-length    \n",
    "                        if loc - context_length >= 0 and loc + context_length < len(tokens):\n",
    "                            context_before = \" \".join(tokens[loc - context_length:loc+1])\n",
    "                            context_after = \" \".join(tokens[loc+1:loc + context_length + 1])\n",
    "                            context = \" \".join(tokens[loc - context_length:loc + context_length + 1])\n",
    "                        elif loc - context_length < 0 and loc + context_length < len(tokens):\n",
    "                            beg = abs(loc -context_length)\n",
    "                            context_before = \" \".join(tokens[loc-context_length + beg:loc+1])\n",
    "                            context_after =  \" \".join(tokens[loc+1:loc + context_length + 1])\n",
    "                            context = \" \".join(tokens[loc-context_length + beg:loc + context_length + 1])\n",
    "                        elif loc - context_length > 0 and loc + context_length > len(tokens): \n",
    "                            context_before = \" \".join(tokens[loc - context_length:loc+1])\n",
    "                            context_after = \" \".join(tokens[loc +1 : len(tokens)])\n",
    "                            context = \" \".join(tokens[loc - context_length : len(tokens)])\n",
    "\n",
    "        #                 assert any(val in tokens[loc] for val in value)\n",
    "                        count+= 1\n",
    "\n",
    "                        visited_indices.add(match.start())\n",
    "                        # features to engineer...\n",
    "                        features['value'] = value\n",
    "                        features['context_before'] = context_before\n",
    "                        features['context_after'] = context_after\n",
    "                        features['context'] = context\n",
    "                        features['float'] = clean_money_amount([temp])\n",
    "                        features['start_idx_ratio'] = match.start()/len(case)\n",
    "                        features['greater_than_1000'] = features['float'] > 1000\n",
    "                        break #only add one match at a time - features need to have same order as values list\n",
    "                    case_feats.append(features)\n",
    "                features_per_case.append(case_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n",
      "===\n",
      "23\n",
      "23\n",
      "===\n",
      "60\n",
      "60\n",
      "===\n",
      "14\n",
      "14\n",
      "===\n",
      "27\n",
      "27\n",
      "===\n",
      "38\n",
      "38\n",
      "===\n",
      "4\n",
      "4\n",
      "===\n",
      "2\n",
      "2\n",
      "===\n",
      "17\n",
      "17\n",
      "===\n",
      "8\n",
      "8\n",
      "===\n",
      "38\n",
      "38\n",
      "===\n",
      "153\n",
      "153\n",
      "===\n",
      "63\n",
      "63\n",
      "===\n",
      "54\n",
      "54\n",
      "===\n",
      "2\n",
      "2\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "21\n",
      "21\n",
      "===\n",
      "31\n",
      "31\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "41\n",
      "41\n",
      "===\n",
      "45\n",
      "45\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "18\n",
      "18\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "54\n",
      "54\n",
      "===\n",
      "55\n",
      "55\n",
      "===\n",
      "84\n",
      "84\n",
      "===\n",
      "38\n",
      "38\n",
      "===\n",
      "6\n",
      "6\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(features_per_case)):\n",
    "    d = features_per_case[i]\n",
    "    d2 = case_info[i]\n",
    "    if d ==[]:\n",
    "        continue\n",
    "    print(len(d))\n",
    "    print(len(d2))\n",
    "    print('===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': '$5,647,773', 'context_before': 'pain suffering — plaintiff awarded $5,647,773', 'context_after': 'total damages injuries suffered bicycle', 'context': 'pain suffering — plaintiff awarded $5,647,773 total damages injuries suffered bicycle', 'float': 5647773.0, 'start_idx_ratio': 0.006906599039551071, 'greater_than_1000': True} total\n",
      "===\n",
      "{'value': '$5,647,773', 'context_before': '— paralysis — plaintiff awarded $5,647,773', 'context_after': 'total damages injuries suffered bicycle', 'context': '— paralysis — plaintiff awarded $5,647,773 total damages injuries suffered bicycle', 'float': 5647773.0, 'start_idx_ratio': 0.009910250184355833, 'greater_than_1000': True} total\n",
      "===\n",
      "{'value': '$5,647,773', 'context_before': 'highway repair — plaintiff awarded $5,647,773', 'context_after': 'total damages injuries suffered bicycle', 'context': 'highway repair — plaintiff awarded $5,647,773 total damages injuries suffered bicycle', 'float': 5647773.0, 'start_idx_ratio': 0.013129732549146567, 'greater_than_1000': True} total\n",
      "===\n",
      "{'value': '$5,647,773', 'context_before': 'liability driver — plaintiff awarded $5,647,773', 'context_after': 'total damages injuries suffered bicycle', 'context': 'liability driver — plaintiff awarded $5,647,773 total damages injuries suffered bicycle', 'float': 5647773.0, 'start_idx_ratio': 0.016556053166423855, 'greater_than_1000': True} total\n",
      "===\n",
      "{'value': '$5,647,773', 'context_before': 'per cent liable; plaintiff awarded $5,647,773', 'context_after': 'total damages -- court found', 'context': 'per cent liable; plaintiff awarded $5,647,773 total damages -- court found', 'float': 5647773.0, 'start_idx_ratio': 0.024595766110901276, 'greater_than_1000': True} total\n",
      "===\n",
      "{'value': '$311,000', 'context_before': '-- damages awarded consisted $311,000 non-pecuniary', 'context_after': 'loss, $153,249 past wage loss,', 'context': '-- damages awarded consisted $311,000 non-pecuniary loss, $153,249 past wage loss,', 'float': 311000.0, 'start_idx_ratio': 0.03453299519775536, 'greater_than_1000': True} non pecuniary\n",
      "===\n",
      "{'value': '$153,249', 'context_before': 'non-pecuniary loss, $153,249 past wage loss,', 'context_after': '$502,381 future wage loss, $4,151,504', 'context': 'non-pecuniary loss, $153,249 past wage loss, $502,381 future wage loss, $4,151,504', 'float': 153249.0, 'start_idx_ratio': 0.03513552402021619, 'greater_than_1000': True} past wage loss\n",
      "===\n",
      "{'value': '$502,381', 'context_before': 'future wage loss, $4,151,504 cost future', 'context_after': 'care, $388,639 cost care/ home', 'context': 'future wage loss, $4,151,504 cost future care, $388,639 cost care/ home', 'float': 502381.0, 'start_idx_ratio': 0.03571107394017878, 'greater_than_1000': True} future wage loss\n",
      "===\n",
      "{'value': '$4,151,504', 'context_before': '$388,639 cost care/ home replacement, $96,000', 'context_after': 'in-trust claim, $45,000 special damages', 'context': '$388,639 cost care/ home replacement, $96,000 in-trust claim, $45,000 special damages', 'float': 4151504.0, 'start_idx_ratio': 0.03632259573013903, 'greater_than_1000': True} future care\n",
      "===\n",
      "{'value': '$388,639', 'context_before': 'in-trust claim, $45,000 special damages --', 'context_after': \"plaintiff's life expectancy set 25.2\", 'context': \"in-trust claim, $45,000 special damages -- plaintiff's life expectancy set 25.2\", 'float': 388639.0, 'start_idx_ratio': 0.03690713861760104, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$96,000', 'context_before': \"plaintiff's life expectancy set 25.2 years\", 'context_after': '(low due increased risk due', 'context': \"plaintiff's life expectancy set 25.2 years (low due increased risk due\", 'float': 96000.0, 'start_idx_ratio': 0.03757261821255778, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$45,000', 'context_before': '25.2 years (low due increased risk', 'context_after': 'due injuries), found need care', 'context': '25.2 years (low due increased risk due injuries), found need care', 'float': 45000.0, 'start_idx_ratio': 0.038112196262522706, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$1500', 'context_before': 'dispute suggestion approximately $1500', 'context_after': 'cost 1999 veer confirmed modest', 'context': 'dispute suggestion approximately $1500 cost 1999 veer confirmed modest', 'float': 1500.0, 'start_idx_ratio': 0.12871634381913344, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$1500', 'context_before': 'something relatively modest cost, approximately $1500', 'context_after': 'expended july 1999, could avoided.', 'context': 'something relatively modest cost, approximately $1500 expended july 1999, could avoided.', 'float': 1500.0, 'start_idx_ratio': 0.19541718376229789, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$100,000', 'context_before': 'future care, limited called \" $100,000', 'context_after': 'limit\".', 'context': 'future care, limited called \" $100,000 limit\".', 'float': 100000.0, 'start_idx_ratio': 0.5518894224716272, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$280,000', 'context_before': 'attract upper limit non-pecuniary damages\" $280,000', 'context_after': '.', 'context': 'attract upper limit non-pecuniary damages\" $280,000 .', 'float': 280000.0, 'start_idx_ratio': 0.8007428191154518, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$100,000', 'context_before': 'policy limit non-pecuniary damages fixed $100,000', 'context_after': '.', 'context': 'policy limit non-pecuniary damages fixed $100,000 .', 'float': 100000.0, 'start_idx_ratio': 0.80538319034515, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$100,000', 'context_before': 'avoid extravagant claims, upper limit $100,000', 'context_after': 'imposed.', 'context': 'avoid extravagant claims, upper limit $100,000 imposed.', 'float': 100000.0, 'start_idx_ratio': 0.806723142502563, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$100,000', 'context_before': '\"devastating\", and, therefore, fell within $100,000', 'context_after': 'category fixed lindal trilogy cases.', 'context': '\"devastating\", and, therefore, fell within $100,000 category fixed lindal trilogy cases.', 'float': 100000.0, 'start_idx_ratio': 0.8087195812874333, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$311,000', 'context_before': 'adjusted upper limit catastrophic claims $311,000', 'context_after': '.', 'context': 'adjusted upper limit catastrophic claims $311,000 .', 'float': 311000.0, 'start_idx_ratio': 0.8128653393046638, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$100,000', 'context_before': 'opines \"most\" cases catastrophic loss $100,000', 'context_after': 'rough upper limit.', 'context': 'opines \"most\" cases catastrophic loss $100,000 rough upper limit.', 'float': 100000.0, 'start_idx_ratio': 0.8143581719095667, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$311,000', 'context_before': 'opines \"most\" cases catastrophic loss $100,000', 'context_after': 'rough upper limit.', 'context': 'opines \"most\" cases catastrophic loss $100,000 rough upper limit.', 'float': 311000.0, 'start_idx_ratio': 0.8206802280616557, 'greater_than_1000': True} non pecuniary\n",
      "===\n",
      "{'value': '$255,415.00', 'context_before': \"report, aberdeen's past wage loss $255,415.00\", 'context_after': 'reduced $153,249.00 40% reduction pursuant', 'context': \"report, aberdeen's past wage loss $255,415.00 reduced $153,249.00 40% reduction pursuant\", 'float': 255415.0, 'start_idx_ratio': 0.8243583517689167, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$153,249.00', 'context_before': 'wage loss $255,415.00 reduced $153,249.00 40%', 'context_after': 'reduction pursuant hudniuk v. warkentin.', 'context': 'wage loss $255,415.00 reduced $153,249.00 40% reduction pursuant hudniuk v. warkentin.', 'float': 153249.0, 'start_idx_ratio': 0.8248080001438874, 'greater_than_1000': True} reduction to\n",
      "===\n",
      "{'value': '$153,249', 'context_before': \"would assess aberdeen's wage loss $153,249\", 'context_after': 'taking account hudniuk v. warkentin', 'context': \"would assess aberdeen's wage loss $153,249 taking account hudniuk v. warkentin\", 'float': 153249.0, 'start_idx_ratio': 0.8303116962535297, 'greater_than_1000': True} past wage loss\n",
      "===\n",
      "{'value': '$502,381.00', 'context_before': 'future without injury loss income $502,381.00', 'context_after': 'inclusive future loss pension benefits', 'context': 'future without injury loss income $502,381.00 inclusive future loss pension benefits', 'float': 502381.0, 'start_idx_ratio': 0.8394935160704329, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$502,381', 'context_before': 'future wages including pension benefits $502,381', 'context_after': '.', 'context': 'future wages including pension benefits $502,381 .', 'float': 502381.0, 'start_idx_ratio': 0.8431896257126926, 'greater_than_1000': True} future wage loss\n",
      "===\n",
      "{'value': '$500', 'context_before': 'expenses cost future care report $500', 'context_after': '.', 'context': 'expenses cost future care report $500 .', 'float': 500.0, 'start_idx_ratio': 0.8633608518138816, 'greater_than_1000': False} other\n",
      "===\n",
      "{'value': '$500', 'context_before': 'could pack, move, unpack household $500', 'context_after': '.', 'context': 'could pack, move, unpack household $500 .', 'float': 500.0, 'start_idx_ratio': 0.8644040360438138, 'greater_than_1000': False} other\n",
      "===\n",
      "{'value': '$5,021,538', 'context_before': '218 total report (cdn) $5,021,538', 'context_after': ', $259,966 gst pst, (us)', 'context': '218 total report (cdn) $5,021,538 , $259,966 gst pst, (us)', 'float': 5021538.0, 'start_idx_ratio': 0.8862659400348927, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$259,966', 'context_before': 'total report (cdn) $5,021,538, $259,966 gst', 'context_after': 'pst, (us) $57,085.', 'context': 'total report (cdn) $5,021,538, $259,966 gst pst, (us) $57,085.', 'float': 259966.0, 'start_idx_ratio': 0.8866436446698681, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$57,085', 'context_before': 'gst pst, (us) $57,085 .', 'context_after': '', 'context': 'gst pst, (us) $57,085 .', 'float': 57085.0, 'start_idx_ratio': 0.8871112789798378, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '720', 'context_before': 'needs: 1. urologist (3 years) 720', 'context_after': '(onward) 1,125 2. psychiatry 5,970', 'context': 'needs: 1. urologist (3 years) 720 (onward) 1,125 2. psychiatry 5,970', 'float': 720.0, 'start_idx_ratio': 0.8895213942696811, 'greater_than_1000': False} other\n",
      "===\n",
      "{'value': '1,125', 'context_before': '(3 years) 720 (onward) 1,125 2.', 'context_after': 'psychiatry 5,970 3. psychological counselling', 'context': '(3 years) 720 (onward) 1,125 2. psychiatry 5,970 3. psychological counselling', 'float': 1125.0, 'start_idx_ratio': 0.889908091872156, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '5,970', 'context_before': '1,125 2. psychiatry 5,970 3. psychological', 'context_after': 'counselling - family (2 years)', 'context': '1,125 2. psychiatry 5,970 3. psychological counselling - family (2 years)', 'float': 5970.0, 'start_idx_ratio': 0.8903577402471268, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '3,169', 'context_before': '(2 years) 3,169 (onward) 16,959 4.', 'context_after': 'registered nurse services 1 19,866', 'context': '(2 years) 3,169 (onward) 16,959 4. registered nurse services 1 19,866', 'float': 3169.0, 'start_idx_ratio': 0.8911131495170777, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '16,959', 'context_before': '(onward) 16,959 4. registered nurse services', 'context_after': '1 19,866 5. acupuncture (3', 'context': '(onward) 16,959 4. registered nurse services 1 19,866 5. acupuncture (3', 'float': 16959.0, 'start_idx_ratio': 0.8915178330545513, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '19,866', 'context_before': '19,866 5. acupuncture (3 years) 3,163', 'context_after': '190 (onward) 7,409 445 rehabilitation', 'context': '19,866 5. acupuncture (3 years) 3,163 190 (onward) 7,409 445 rehabilitation', 'float': 19866.0, 'start_idx_ratio': 0.8921293548445116, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '3,163', 'context_before': '190 (onward) 7,409 445 rehabilitation support', 'context_after': 'services: 6. cost care assessed', 'context': '190 (onward) 7,409 445 rehabilitation support services: 6. cost care assessed', 'float': 3163.0, 'start_idx_ratio': 0.8926869188294754, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '3,770', 'context_before': '7. kinesiologist supervision 59,258 3555 personnel', 'context_after': 'care support: 8. personal support', 'context': '7. kinesiologist supervision 59,258 3555 personnel care support: 8. personal support', 'float': 3770.0, 'start_idx_ratio': 0.8937750678969046, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '725,443', 'context_before': 'childcare grandchildren 3,200 192 equipment external', 'context_after': 'aids/devices: 10. seating assessment (manual', 'context': 'childcare grandchildren 3,200 192 equipment external aids/devices: 10. seating assessment (manual', 'float': 725443.0, 'start_idx_ratio': 0.8949801255418263, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '3,200', 'context_before': 'aids/devices: 10. seating assessment (manual wheelchair)', 'context_after': '5,781 11. environmental control unit', 'context': 'aids/devices: 10. seating assessment (manual wheelchair) 5,781 11. environmental control unit', 'float': 3200.0, 'start_idx_ratio': 0.8956276192017842, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '5,865', 'context_before': '5,228 317 14. tens unit (combined', 'context_after': 'costs) 2,274 297 15. computerized', 'context': '5,228 317 14. tens unit (combined costs) 2,274 297 15. computerized', 'float': 5865.0, 'start_idx_ratio': 0.8973722548966708, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '27,885', 'context_before': '(usd) (57,084) educational vocational support: 17.', 'context_after': 'educational consultant 15,922 955 transportation:', 'context': '(usd) (57,084) educational vocational support: 17. educational consultant 15,922 955 transportation:', 'float': 27885.0, 'start_idx_ratio': 0.8987931437615785, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '6,468', 'context_before': '1,130,000 total reduction (usd$) 57,084 notes:', 'context_after': '1 nursing services allowed one-half', 'context': '1,130,000 total reduction (usd$) 57,084 notes: 1 nursing services allowed one-half', 'float': 6468.0, 'start_idx_ratio': 0.900969441896437, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '1,130,000', 'context_before': 'services allowed one-half claim, reduction cost', 'context_after': 'report $19,866.', 'context': 'services allowed one-half claim, reduction cost report $19,866.', 'float': 1130000.0, 'start_idx_ratio': 0.9016888792963902, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '57,084', 'context_before': 'cost report $19,866.', 'context_after': '', 'context': 'cost report $19,866.', 'float': 57084.0, 'start_idx_ratio': 0.9022554362488534, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$19,866', 'context_before': '', 'context_after': '', 'context': '', 'float': 19866.0, 'start_idx_ratio': 0.9032356697062897, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$725,443', 'context_before': 'hours day), reduction cost report $725,443', 'context_after': 'plus $43,527 taxes.', 'context': 'hours day), reduction cost report $725,443 plus $43,527 taxes.', 'float': 725443.0, 'start_idx_ratio': 0.9044227414162125, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$43,527', 'context_before': 'cost report $725,443 plus $43,527 taxes.', 'context_after': '', 'context': 'cost report $725,443 plus $43,527 taxes.', 'float': 43527.0, 'start_idx_ratio': 0.9048184319861868, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$3,949,949', 'context_before': 'cost future care total (cdn) $3,949,949', 'context_after': 'plus $201,605 gst pst, us', 'context': 'cost future care total (cdn) $3,949,949 plus $201,605 gst pst, us', 'float': 3949949.0, 'start_idx_ratio': 0.9062752927210921, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$201,605', 'context_before': 'total (cdn) $3,949,949 plus $201,605 gst', 'context_after': 'pst, us dollar component, total', 'context': 'total (cdn) $3,949,949 plus $201,605 gst pst, us dollar component, total', 'float': 201605.0, 'start_idx_ratio': 0.9066889692260652, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$4,151,504', 'context_before': 'dollar component, total $4,151,504 .', 'context_after': '', 'context': 'dollar component, total $4,151,504 .', 'float': 4151504.0, 'start_idx_ratio': 0.9073634417885214, 'greater_than_1000': True} future care\n",
      "===\n",
      "{'value': '$19,866', 'context_before': 'would allow $19,866', 'context_after': ', one-half claim costs savings', 'context': 'would allow $19,866 , one-half claim costs savings', 'float': 19866.0, 'start_idx_ratio': 0.9106818467958057, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$19,866', 'context_before': 'one-half claim costs savings $19,866 ,', 'context_after': 'registered nurse services provided public', 'context': 'one-half claim costs savings $19,866 , registered nurse services provided public', 'float': 19866.0, 'start_idx_ratio': 0.9112933685857659, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$725,443', 'context_before': 'support worker would result reduction $725,443', 'context_after': 'cost report.', 'context': 'support worker would result reduction $725,443 cost report.', 'float': 725443.0, 'start_idx_ratio': 0.918847461285275, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$67,500', 'context_before': 'design fee component twenty percent $67,500', 'context_after': ', based total construction cost', 'context': 'design fee component twenty percent $67,500 , based total construction cost', 'float': 67500.0, 'start_idx_ratio': 0.9566359107178186, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$405,000', 'context_before': 'construction equipment fixture costs would $405,000', 'context_after': '.', 'context': 'construction equipment fixture costs would $405,000 .', 'float': 405000.0, 'start_idx_ratio': 0.9581287433227216, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$82,231', 'context_before': 'table c expenses report would $82,231', 'context_after': 'plus $16,446 twenty percent architectural', 'context': 'table c expenses report would $82,231 plus $16,446 twenty percent architectural', 'float': 82231.0, 'start_idx_ratio': 0.9609795140200363, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$16,446', 'context_before': 'report would $82,231 plus $16,446 twenty', 'context_after': 'percent architectural fees which, added', 'context': 'report would $82,231 plus $16,446 twenty percent architectural fees which, added', 'float': 16446.0, 'start_idx_ratio': 0.9613662116225112, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$405,000', 'context_before': 'fees which, added $405,000 equals total', 'context_after': 'construction costs new residence $503,667.', 'context': 'fees which, added $405,000 equals total construction costs new residence $503,667.', 'float': 405000.0, 'start_idx_ratio': 0.9621306138599615, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$503,667', 'context_before': 'new residence $503,667 .', 'context_after': '', 'context': 'new residence $503,667 .', 'float': 503667.0, 'start_idx_ratio': 0.9628950160974118, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$325,000', 'context_before': 'one estimate cost lot would $325,000', 'context_after': '.', 'context': 'one estimate cost lot would $325,000 .', 'float': 325000.0, 'start_idx_ratio': 0.9642079893523264, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$828,647', 'context_before': 'one estimate cost lot would $325,000', 'context_after': '.', 'context': 'one estimate cost lot would $325,000 .', 'float': 828647.0, 'start_idx_ratio': 0.9647835392722891, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$465,000', 'context_before': 'current home appraised $465,000', 'context_after': 'which, reduction real estate commission', 'context': 'current home appraised $465,000 which, reduction real estate commission', 'float': 465000.0, 'start_idx_ratio': 0.9660065828522095, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$100,000', 'context_before': 'estate commission (7% first $100,000 ,', 'context_after': '31/2% everything thereafter, plus 6%', 'context': 'estate commission (7% first $100,000 , 31/2% everything thereafter, plus 6%', 'float': 100000.0, 'start_idx_ratio': 0.9668069569596576, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$20,962', 'context_before': '6% gst equals $20,962 ) would', 'context_after': 'leave net receipt aberdeen sale', 'context': '6% gst equals $20,962 ) would leave net receipt aberdeen sale', 'float': 20962.0, 'start_idx_ratio': 0.9675983380996062, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$440,038', 'context_before': 'sale home $440,038 .', 'context_after': '', 'context': 'sale home $440,038 .', 'float': 440038.0, 'start_idx_ratio': 0.9683357614345581, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$388,639', 'context_before': 'damages aberdeen home replacement amount $388,639', 'context_after': '.', 'context': 'damages aberdeen home replacement amount $388,639 .', 'float': 388639.0, 'start_idx_ratio': 0.9703951509919243, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$15', 'context_before': 'home care worker, earning approximately $15', 'context_after': 'hour.', 'context': 'home care worker, earning approximately $15 hour.', 'float': 15.0, 'start_idx_ratio': 0.9841004334610335, 'greater_than_1000': False} other\n",
      "===\n",
      "{'value': '$1,000', 'context_before': 'claim follows: jenny aberdeen - $1,000', 'context_after': 'per month; ryan aberdeen -', 'context': 'claim follows: jenny aberdeen - $1,000 per month; ryan aberdeen -', 'float': 1000.0, 'start_idx_ratio': 0.987769564200795, 'greater_than_1000': False} other\n",
      "===\n",
      "{'value': '$500', 'context_before': 'month; ryan aberdeen - $500 per', 'context_after': 'month; gail aberdeen - $500', 'context': 'month; ryan aberdeen - $500 per month; gail aberdeen - $500', 'float': 500.0, 'start_idx_ratio': 0.9883451141207575, 'greater_than_1000': False} other\n",
      "===\n",
      "{'value': '$500', 'context_before': 'gail aberdeen - $500 per month.', 'context_after': '', 'context': 'gail aberdeen - $500 per month.', 'float': 500.0, 'start_idx_ratio': 0.9889026781057213, 'greater_than_1000': False} other\n",
      "===\n",
      "{'value': '$2,000', 'context_before': 'gail aberdeen - $500 per month.', 'context_after': '', 'context': 'gail aberdeen - $500 per month.', 'float': 2000.0, 'start_idx_ratio': 0.989370312415691, 'greater_than_1000': True} other\n",
      "===\n",
      "{'value': '$96,000', 'context_before': 'gail aberdeen - $500 per month.', 'context_after': '', 'context': 'gail aberdeen - $500 per month.', 'float': 96000.0, 'start_idx_ratio': 0.9900537779456465, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$45,000.00', 'context_before': '240 parties agreed special damages $45,000.00', 'context_after': '.', 'context': '240 parties agreed special damages $45,000.00 .', 'float': 45000.0, 'start_idx_ratio': 0.9909171028255904, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$311,000', 'context_before': 'damages assessed follows: non-pecuniary loss: $311,000', 'context_after': 'past wage loss (after applying', 'context': 'damages assessed follows: non-pecuniary loss: $311,000 past wage loss (after applying', 'float': 311000.0, 'start_idx_ratio': 0.9939297469378946, 'greater_than_1000': True} non pecuniary\n",
      "===\n",
      "{'value': '$153,249', 'context_before': '(after applying hudniuk): $153,249 future wage', 'context_after': 'loss: $502,381 cost future care:', 'context': '(after applying hudniuk): $153,249 future wage loss: $502,381 cost future care:', 'float': 153249.0, 'start_idx_ratio': 0.9947211280778431, 'greater_than_1000': True} past wage loss\n",
      "===\n",
      "{'value': '$502,381', 'context_before': '$502,381 cost future care: $4,151,504 cost', 'context_after': 'care - home replacement: $388,639', 'context': '$502,381 cost future care: $4,151,504 cost care - home replacement: $388,639', 'float': 502381.0, 'start_idx_ratio': 0.9953146639328045, 'greater_than_1000': True} future wage loss\n",
      "===\n",
      "{'value': '$4,151,504', 'context_before': '- home replacement: $388,639 trust claim:', 'context_after': '$96,000 special damages: $45,000 243', 'context': '- home replacement: $388,639 trust claim: $96,000 special damages: $45,000 243', 'float': 4151504.0, 'start_idx_ratio': 0.9959261857227648, 'greater_than_1000': True} future care\n",
      "===\n",
      "{'value': '$388,639', 'context_before': 'damages: $45,000 243 thus, total damages', 'context_after': 'assessed $5,647,773.', 'context': 'damages: $45,000 243 thus, total damages assessed $5,647,773.', 'float': 388639.0, 'start_idx_ratio': 0.9966186442202198, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$96,000', 'context_before': 'total damages assessed $5,647,773.', 'context_after': '', 'context': 'total damages assessed $5,647,773.', 'float': 96000.0, 'start_idx_ratio': 0.9971402363351859, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$45,000', 'context_before': '', 'context_after': '', 'context': '', 'float': 45000.0, 'start_idx_ratio': 0.9976888073526502, 'greater_than_1000': True} sub-special\n",
      "===\n",
      "{'value': '$5,647,773', 'context_before': '', 'context_after': '', 'context': '', 'float': 5647773.0, 'start_idx_ratio': 0.9983812658501052, 'greater_than_1000': True} total\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(features_per_case[-3])):\n",
    "    d = features_per_case[-3][i]\n",
    "    v = case_tags[-3][i]\n",
    "    print(d, v)\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# turn list of lists into one-dimenion\n",
    "tags = list(chain.from_iterable(case_info))\n",
    "feats = list(chain.from_iterable(features_per_case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vectorizer = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #cross validation instead\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     feats, tags, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.09955655785733969, micro: 0.47761194029850745 f-scores\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('macro: %s, micro: %s f-scores' %(f1_score(y_test, y_pred, average = 'macro'), f1_score(y_test, y_pred, average= 'micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47761194029850745"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilana/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                   precision    recall  f1-score   support\\n\\n       aggravated       0.00      0.00      0.00         2\\n      future care       1.00      0.25      0.40        16\\n future wage loss       0.00      0.00      0.00         7\\n          general       0.00      0.00      0.00         9\\n    non pecuniary       0.00      0.00      0.00        11\\n            other       0.69      0.69      0.69       210\\n   past wage loss       0.10      0.08      0.09        13\\n         punitive       0.00      0.00      0.00         2\\n     reduction to       0.00      0.00      0.00         2\\n          special       0.00      0.00      0.00        22\\n  sub-future care       0.00      0.00      0.00         5\\n      sub-general       0.00      0.00      0.00         4\\nsub-non pecuniary       0.00      0.00      0.00         3\\n      sub-special       0.17      0.20      0.18        10\\n        sub-total       0.00      0.00      0.00         3\\n            total       0.15      0.50      0.24        16\\n\\n         accuracy                           0.48       335\\n        macro avg       0.13      0.11      0.10       335\\n     weighted avg       0.49      0.48      0.47       335\\n'"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilana/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                   precision    recall  f1-score   support\\n\\n       aggravated       0.00      0.00      0.00         2\\n      future care       0.00      0.00      0.00        16\\n future wage loss       0.00      0.00      0.00         7\\n          general       0.00      0.00      0.00         9\\n    non pecuniary       0.00      0.00      0.00        11\\n            other       0.63      0.99      0.77       210\\n   past wage loss       0.00      0.00      0.00        13\\n         punitive       0.00      0.00      0.00         2\\n     reduction to       0.00      0.00      0.00         2\\n          special       0.00      0.00      0.00        22\\n  sub-future care       0.00      0.00      0.00         5\\n      sub-general       0.00      0.00      0.00         4\\nsub-non pecuniary       0.00      0.00      0.00         3\\n      sub-special       0.00      0.00      0.00        10\\n        sub-total       0.00      0.00      0.00         3\\n            total       0.71      0.31      0.43        16\\n\\n         accuracy                           0.64       335\\n        macro avg       0.08      0.08      0.08       335\\n     weighted avg       0.43      0.64      0.51       335\\n'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "classification_report(y_test, y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<damage type=\"non pecuniary\">$1,000,000</damage>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['$1,000,000'], ['non pecuniary'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case2tags(\"I asses non-pecuniary damages of <damage type='non pecuniary'>$1,000,000</damage>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<damage type=\"non pecuniary\">$1,000,000</damage>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup('<xml>'+\"I asses non-pecuniary damages of <damage type='non pecuniary'>$1,000,000</damage>\"+'</xml>', \"xml\")\n",
    "soup.find_all('damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
