{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_parse_BCJ(path = None, doc = None):\n",
    "    '''Given file path (text file) of negligence cases, finds static \n",
    "    information within the case (information that can be pattern matched)\n",
    "    Expects a B.C.J. case format (British Columbia Judgments)\n",
    "    \n",
    "    The following fields are currently implemented:\n",
    "    - Case Title\n",
    "    - Judge Name\n",
    "    - Registry\n",
    "    - Year\n",
    "    - Decision Length (in paragraphs)\n",
    "    - Damages\n",
    "    - Multiple Defendants\n",
    "    - Plaintiff Wins\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: case_parsed_data (list) of case_dict (Dictionary): List of Dictionaries with rule based parsable fields filled in\n",
    "    '''\n",
    "    if path:\n",
    "        with open(path, encoding='utf-8') as document:\n",
    "            document_data = document.read()\n",
    "        document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "    \n",
    "\n",
    "    case_parsed_data = []\n",
    "    for i in range(len(document_data)):\n",
    "        case_dict = dict() \n",
    "        case = document_data[i]\n",
    "        case = case.strip() # Make sure to strip!\n",
    "        if len(case) == 0: # Skip empty lines\n",
    "            continue\n",
    "        \n",
    "        lines = case.split('\\n')\n",
    "        if len(lines) < 2:\n",
    "            print(case)\n",
    "        case_title = lines[0]\n",
    "        case_type = lines[1]\n",
    "\n",
    "        if 'R. v.' in case_title or '(Re)' in case_title: # Skip crown cases, Skip (Re) cases\n",
    "            continue\n",
    "            \n",
    "        # Skip client/solicitor cases (not same as plaintiff/defendant)\n",
    "        regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?|[L|l]awyer[s]?))', case)\n",
    "        if regex_client_solicitor:\n",
    "            continue\n",
    "        \n",
    "        regex_solicitor_client = re.search(r'(Between.*([L|l]awyer[s]?|[S|s]olicitor[s]?).*([C|c]lient[s]?))', case)\n",
    "        if regex_solicitor_client:\n",
    "            continue\n",
    "            \n",
    "        # In some rare cases we have 'IN THE MATTER OF ..' (rather than 'Between ...') .. but it is following by the normal\n",
    "        # plaintiff/defendant dynamic. Only skip cases if there is no mention of the following terms\n",
    "        # (Can be cleaned up in future)\n",
    "        key_words = ['appellant', 'respondent', 'claimant', 'petitioner', 'plaintiff', 'defendant',\n",
    "        'appellants', 'respondents', 'claimants', 'petitioners', 'plaintiffs', 'defendants']\n",
    "        regex_in_matter_of = re.search(r'IN THE MATTER OF .*\\n\\([0-9]+ paras.\\)', case)\n",
    "        if regex_in_matter_of:\n",
    "            remove = True\n",
    "            for key in key_words:\n",
    "                if key in regex_in_matter_of.group(0).lower().strip():\n",
    "                    remove = False\n",
    "                    \n",
    "            if remove:\n",
    "                continue\n",
    "\n",
    "        if 'British Columbia Judgments' in case_type: # Make sure we're dealing with a B.C.J. case\n",
    "        \n",
    "            # Fields that can be found via pattern matching\n",
    "            if re.search('contributory negligence', case, re.IGNORECASE):\n",
    "                contributory_negligence_raised = True\n",
    "            else:\n",
    "                contributory_negligence_raised = False\n",
    "            case_number = re.search(r'\\/P([0-9]+)\\.txt', path).group(1)\n",
    "            decision_len = re.search(r'\\(([0-9]+) paras\\.?\\)', case) # e.g.) (100 paras.)\n",
    "            registry = re.search(r'(Registry|Registries): ?([A-Za-z0-9 ]+)', case) # e.g.) Registry: Vancouver\n",
    "            written_decision = True if int(decision_len.group(1)) > 1 else False\n",
    "            if registry:\n",
    "                registry = registry.group(2).strip()\n",
    "            else:\n",
    "                registry = re.search(r'([A-Za-z ]+) Registry No.', case) # Alt form e.g.) Vancouver Registory No. XXX\n",
    "                if registry:\n",
    "                    registry = registry.group(1).strip()\n",
    "                else:\n",
    "                    registry = re.search(r'([A-Za-z ]+) No. S[0-9]*', case)\n",
    "                    if registry:\n",
    "                        registry = registry.group(1).strip()\n",
    "                    else:\n",
    "                        print('WARNING: Registry could not be found (This shouldn\\'t occur!)')\n",
    "            # Fields that are always in the same place\n",
    "            judge_name = lines[4].strip()\n",
    "            case_title = lines[0].strip()\n",
    "            # Extract year from case_title (in case we want to make visualizations, etc.)\n",
    "            year = re.search(r'20[0-2][0-9]', case_title) # Limit regex to be from 2000 to 2029\n",
    "            if year:\n",
    "                year = year.group(0)\n",
    "            else:\n",
    "                # Rare case: Sometimes the title is too long. Rely on Heard date.\n",
    "                year = re.search(r'Heard:.* ([2][0][0-2][0-9])', case)\n",
    "                if year:\n",
    "                    year = year.group(1)\n",
    "                else:\n",
    "                    print('WARNING: Year not found')\n",
    "            case_dict['case_number'] = '%s of %s'%(i+1+((int(case_number)-1)*50), case_number)\n",
    "            case_dict['case_title'] = case_title\n",
    "            case_dict['year'] = year\n",
    "            case_dict['registry'] = registry\n",
    "            case_dict['judge'] = judge_name\n",
    "            case_dict['decision_length'] = decision_len.group(1)\n",
    "            case_dict['multiple_defendants'] = rule_based_multiple_defendants_parse(case)\n",
    "            case_dict['contributory_negligence_raised'] = contributory_negligence_raised\n",
    "            case_dict['written_decision'] = written_decision\n",
    "            \n",
    "            # TODO: Improve plaintiff_wins to take one case at a time.\n",
    "            plaintiff_list = plaintiff_wins(path)\n",
    "            if case_title in plaintiff_list:\n",
    "                case_dict['plaintiff_wins'] = plaintiff_list[case_title]\n",
    "            else:\n",
    "                case_dict['plaintiff_wins'] = \"NA\"\n",
    "                \n",
    "            case_dict['damages'] = rule_based_damage_extraction(case)\n",
    "                \n",
    "        # don't add empty dictionaries (non BCJ cases) to list\n",
    "        if case_dict != dict(): \n",
    "            case_parsed_data.append(case_dict)\n",
    "    return case_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_multiple_defendants_parse(doc):\n",
    "    ''' Work in progress. Subject to minor changes to Regex patterns.\n",
    "    \n",
    "    TODO:\n",
    "        - Clarify solicitor/client cases with Lachlan\n",
    "        - Clarify cases that say \"IN MATTER OF ...\", currently returning 'UNK' for these\n",
    "    \n",
    "    -----\n",
    "    \n",
    "    Given a case. Uses regex/pattern-matching to determine whether we have multiple defendants.\n",
    "    For the most part the logic relies on whether the langauge used implies plurality or not.\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: response (String, 'Y', 'N', or 'UNK')\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Cases with (Re) in the title always have one person involved\n",
    "    # May drop these cases depending on advice from Lachlan.\n",
    "    if '(Re)' in doc.split('\\n')[0]:\n",
    "        return 'N'\n",
    "    \n",
    "    # Case 1)\n",
    "    # Traditional/most common. Of form \"Between A, B, C, Plaintiff(s), X, Y, Z Defendant(s)\"\n",
    "    # Can successfully cover ~98% of data\n",
    "    regex_between_plaintiff_claimant = re.search(r'(Between.*([P|p]laintiff[s]?|[C|c]laimant[s]?|[A|a]ppellant[s]?|[P|p]etitioner[s]?).*([D|d]efendant[s]?|[R|r]espondent[s]?).*\\n)', doc)\n",
    "    \n",
    "    # Match found\n",
    "    if regex_between_plaintiff_claimant:\n",
    "        text = regex_between_plaintiff_claimant.group(0).lower()\n",
    "        if 'defendants' in text or 'respondents' in text:\n",
    "            return 'Y'\n",
    "        elif 'defendant' in text or 'respondent' in text:\n",
    "            return 'N'\n",
    "    \n",
    "    # If not found, try other less common cases\n",
    "    else:\n",
    "        # Case 2)\n",
    "        # Sometimes it does not mention the name of the second item. (Defendent/Respondent)\n",
    "        # We can estimate if there are multiple based on the number of \",\" in the line (Covers all cases in initial data)\n",
    "        regex_missing_defendent = re.search(r'(Between.*([P|p]laintiff[s]?|[C|c]laimant[s]?|[A|a]ppellant[s]?|[P|p]etitioner[s]?).*\\n)', doc)\n",
    "        if regex_missing_defendent:\n",
    "            text = regex_missing_defendent.group(0).lower()\n",
    "            if len(text.split(',')) > 5:\n",
    "                return 'Y'\n",
    "            else:\n",
    "                return 'N'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Case 3A) solicitor-client\n",
    "            # Some cases have a solicitor (lawyer) and a client\n",
    "            # Currently assuming the second item is the defendant\n",
    "            regex_solicitor_client = re.search(r'(Between.*([S|s]olicitor[s]?).*([C|c]lient[s]?))', doc)\n",
    "            if regex_solicitor_client:\n",
    "                text = regex_solicitor_client.group(0).lower()\n",
    "                if 'clients' in text:\n",
    "                    return 'Y'\n",
    "                else:\n",
    "                    return 'N'\n",
    "            else:\n",
    "                # Case 3B) client - solicitor\n",
    "                regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?))', doc)\n",
    "                if regex_client_solicitor:\n",
    "                    text = regex_client_solicitor.group(0).lower()\n",
    "                    if 'solicitors' in text:\n",
    "                        return 'Y'\n",
    "                    else:\n",
    "                        return 'N'\n",
    "                else:\n",
    "                    return 'UNK'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plaintiff_wins(path):\n",
    "    '''This function will search the cases and returns a dictionary\n",
    "    with case names as keys and boolean for value, True if the plaintiff\n",
    "    wins the case and False if plaintiff looses'''\n",
    "#     list_of_files = os.listdir(path)\n",
    "    plaintiff_dict = {}\n",
    "\n",
    "    with open(path,'r') as f:\n",
    "        \n",
    "        contents = f.read() \n",
    "        cases = contents.split(\"End of Document\\n\")\n",
    "        for line in cases:\n",
    "            lines = line.strip().split(\"\\n\")\n",
    "            name = lines[0]\n",
    "            # regex search for keyword HELD in cases, which determines if case was allowed or dismissed\n",
    "            HELD = re.search(r'HELD.+', line)\n",
    "            if HELD:\n",
    "                matched = HELD.group(0)\n",
    "                if \"allowed\" in matched or \"favour\" in matched or \"awarded\" in matched or \"granted\" in matched:\n",
    "                    plaintiff_dict[name] = True\n",
    "                if \"dismissed\" in matched:\n",
    "                    plaintiff_dict[name] = False\n",
    "            else:\n",
    "                if line:\n",
    "                    awarded =  re.search(r'award(.+)?.+?(plaintiff(.+)?)?', lines[-2])\n",
    "                    #regex searches for pattern of plaintiff/defendant/applicant....entitled/have...costs\n",
    "                    entiteled = re.search(r'(plaintiff|defendant.?|applicant)(.+)?(entitle(.)?(.+)?|have).+?cost(.+)?', lines[-2])\n",
    "                    #regex searches for pattern of successful...(case)\n",
    "                    successful = re.search(r'successful(.+)?.+?', lines[-2])\n",
    "                    #regex searches for dismiss....\n",
    "                    dismiss = re.search(r'(dismiss(.+)?.+)|(adjourned.+?)', lines[-2])\n",
    "                    costs = re.search(r'costs.+?(award(.+)?|cause).+?', lines[-2])\n",
    "\n",
    "                    if dismiss and \"not dismissed\" not in lines[-2]:\n",
    "                        plaintiff_dict[name] = False\n",
    "                    elif awarded:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    elif entiteled:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    elif successful:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    elif costs:\n",
    "                        plaintiff_dict[name] = True\n",
    "                    else:\n",
    "                        plaintiff_dict[name] = \"OpenCase\"\n",
    "\n",
    "        \n",
    "    return plaintiff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_tokenize(doc):\n",
    "    ''' String of Entire Document and returns list of lists of paragraphs in document\n",
    "    ---------\n",
    "    Input: doc (str) - string of single legal case\n",
    "    Return: docs_split(list) - list of lists of numbrered paragraphs per document'''\n",
    "    \n",
    "    doc_data = []\n",
    "    lines = doc.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    doc_data.append(lines[0])\n",
    "    decision_length = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', doc).group(1)\n",
    "\n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    pattern = r'[\\W|\\w]?(?=\\n[0-9]{1,%s}[\\xa0]{2})'%len(decision_length)\n",
    "    paras_split = re.split(pattern, doc)\n",
    "\n",
    "    paras = []\n",
    "    for para in paras_split:   \n",
    "        # make sure the paragraph starts with the correct characters\n",
    "        para_start = re.match(r'^\\n([0-9]{1,%s})[\\xa0]{2}'%len(decision_length), para)\n",
    "        if para_start:\n",
    "            paras.append(para)\n",
    "    doc_data.extend(paras)\n",
    "    return doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex_damages = r'[\\w|-]* ?(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "#regex_damages = r'(?:[\\w|-]* ?){0,3}(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "#regex_in_trust = r'(?:in-?trust|award).*?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "\n",
    "# Rule based dmg extraction REGEX patterns\n",
    "regex_damages = r'(?![and])(?:[\\w|-]* ?){0,2} ?(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "regex_damages_2 = r'[^:] \\$? ?[0-9][0-9|,|.]+[0-9] (?:for|representing)?[ \\w\\-+]+damages?'\n",
    "regex_damages_3 = r'[^:] \\$? ?[0-9][0-9|,|.]+[0-9] (?:for|representing)?[ \\w\\-+]+damages?(?:(?:for|representing)?.*?[;.\\n])'\n",
    "regex_future_care_loss = r'(?:future|past|in[-| ]?trust|award).*?(?:loss|costs?|income|care)?.*?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "regex_for_cost_of = r'\\$? ?[0-9][0-9|,|.]+[0-9][\\w ]*? cost .*?\\.'\n",
    "\n",
    "# Keywords to look in match for categorization\n",
    "general_damage_keywords = [('general',), ('future', 'income', 'loss'), ('future', 'income'), ('future', 'wage', 'loss'), ('future', 'earning'), ('!past', 'earning', 'capacity'), ('future', 'capacity'), ('future', 'earning'), ('!past', 'loss', 'opportunity'), ('!past', 'loss', 'housekeep'), ('ei', 'benefit')]\n",
    "special_damage_keywords = [('special',), ('trust',), ('past', 'income', 'loss'), ('past', 'wage'), ('past', 'earning'), ('past', 'income'), ('earning', 'capacity')]\n",
    "aggravated_damage_keywords = [('aggravated',)]\n",
    "non_pecuniary_damage_keywords = [('non', 'pecuniary')]\n",
    "punitive_damage_keywords = [('punitive',)]\n",
    "future_care_damage_keywords = [('future', 'care'), ('future', 'cost')]\n",
    "\n",
    "def rule_based_damage_extraction(doc, min_score = 0.9, max_match_len_split = 10):\n",
    "    '''Helper functino for rule_based_parse_BCJ\n",
    "    \n",
    "    Given a case, attempts to extract damages using regex patterns\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    min_score (float): The minimum paragraph score to consider having a valid $ number\n",
    "                       Paragraph has score 1 if its the last paragraph\n",
    "                       Paragraph has score 0 if its the first paragraph\n",
    "    max_match_len_split (int): The max amount of items that can appear in a regex match after splitting (no. words)\n",
    "    \n",
    "    Returns: damages (Dict): Contains any found damages\n",
    "    \n",
    "    '''\n",
    "    damages = defaultdict(float)\n",
    "    repetition_detection = defaultdict(set) # try to stem the repeated values\n",
    "    no_paras = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', doc).group(1) # Get number of paragraphs\n",
    "    pattern = r'([.]?)(?=\\n[0-9]{1,%s}[\\xa0|\\s| ]{2})'%len(no_paras) # Used to split into paras\n",
    "    paras_split = re.split(pattern, doc)\n",
    "    money_patt = r'\\$[0-9|,]+' # Used to get all paragraphs with a money amount\n",
    "    scored_paras = [] # Score paragraphs based on where they appear in the document\n",
    "                      # Score of 0.0 would be the first paragraph. Score of 1.0 would be the last paragraph\n",
    "        \n",
    "    for i, paragraph in enumerate(paras_split):\n",
    "        if re.search(money_patt, paragraph):\n",
    "            scored_paras.append((i / len(paras_split), paragraph)) # (score, paragraph). Score formula: i/no_paras\n",
    "            \n",
    "    scored_paras = sorted(scored_paras, key=lambda x:x[0])[::-1] # Store from last paragraph to first\n",
    "    if len(scored_paras) == 0:\n",
    "        return None\n",
    "    if scored_paras[0][0] < min_score: #If highest scored paragraph is less than minimum score.\n",
    "        return None\n",
    "    \n",
    "    patterns = [regex_damages, regex_damages_2, regex_damages_3, regex_future_care_loss, regex_for_cost_of]\n",
    "    banned_words = ['seek', 'claim', 'propose', 'range', ' v. '] # Skip paragraphs containing these\n",
    "    counter_words = ['summary', 'dismissed'] # Unless these are mentioned. \n",
    "                                             # example) \"Special damage is $5k. But claims for aggravated are 'dismissed'\" \n",
    "    \n",
    "    # Get money mounts from the text\n",
    "    total = None\n",
    "    matches = []\n",
    "    summary_matches = []\n",
    "    for i, scored_para in enumerate(scored_paras):\n",
    "        text = scored_para[1]\n",
    "        score = scored_para[0]\n",
    "        \n",
    "        if score > min_score:\n",
    "            if any(item.startswith('summary') for item in text.lower().split()[:4]) or any(item.startswith('conclusion') for item in text.lower().split()[:4]):\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    summary_matches.append((score, t_m))\n",
    "            elif i+1 < len(scored_paras) and (any(item.startswith('summary') for item in scored_paras[i+1][1].lower().split()[-4:]) or any(item.startswith('conclusion') for item in scored_paras[i+1][1].lower().split()[-4:])):\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    summary_matches.append((score, t_m))\n",
    "            else:\n",
    "                skip = False # Skip paras with banned words\n",
    "                for banned_word in banned_words: \n",
    "                    if banned_word in text:\n",
    "                        skip = True       \n",
    "                for counter_word in counter_words:\n",
    "                    if counter_word in text:\n",
    "                        skip = False\n",
    "                if skip:\n",
    "                    continue\n",
    "\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    matches.append((score, t_m))\n",
    "        \n",
    "    # Only keep matches from the summary if a summary was found. If not keep all matches.\n",
    "    if len(summary_matches) > 0: \n",
    "        matches = summary_matches\n",
    "\n",
    "    # Extract $ value. Determine correct column\n",
    "    regex_number_extraction = r' ?[0-9][0-9|,|.]+[0-9]'\n",
    "    for score, match in matches:\n",
    "        skip = False # Banned words should not appear in final matches\n",
    "        for banned_word in banned_words: \n",
    "            if banned_word in match:    \n",
    "                skip = True\n",
    "                break\n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        amount = re.findall(regex_number_extraction, match, re.IGNORECASE)\n",
    "        extracted_value = clean_money_amount(amount)\n",
    "        if extracted_value is None: # Make sure we are able to extract a value\n",
    "            continue\n",
    "            \n",
    "        value_mapped = False # If we mapped the value into a damage category - stop trying to map into other categories\n",
    "        value_mapped = assign_damage_to_category(extracted_value, general_damage_keywords, match, score, matches, 'General', damages, repetition_detection, repetition_key = ('general',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, special_damage_keywords, match, score, matches, 'Special', damages, repetition_detection, repetition_key = ('special',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, non_pecuniary_damage_keywords, match, score, matches, 'Non-pecuniary', damages, repetition_detection, repetition_key = ('non','pecuniary'))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, aggravated_damage_keywords, match, score, matches, 'Aggravated', damages, repetition_detection, repetition_key = ('aggravated',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, aggravated_damage_keywords, match, score, matches, 'Punitive', damages, repetition_detection, repetition_key = ('punitive',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, future_care_damage_keywords, match, score, matches, 'Future Care', damages, repetition_detection) \n",
    "        if not value_mapped: # Last attempt: Only use \"total amounts\" if nothing else was found\n",
    "            total_keywords = [('total',), ('sum',), ('award',)]\n",
    "            for keywords in total_keywords:\n",
    "                if match_contains_words(match.lower(), keywords):\n",
    "                    if is_best_score(score, matches, keywords):\n",
    "                        if extracted_value not in repetition_detection[('total',)]:\n",
    "                            damages['Pecuniary Total'] = damages['Special'] + damages['General'] + damages['Punitive'] + damages['Aggravated'] + damages['Future Care']\n",
    "                            damages['Total'] = damages['Pecuniary Total'] + damages['Non-pecuniary']\n",
    "                            if damages['Total'] == 0:\n",
    "                                total = extracted_value\n",
    "                                repetition_detection[('total',)].add(extracted_value)\n",
    "                        \n",
    "    damages['Pecuniary Total'] = damages['Special'] + damages['General'] + damages['Punitive'] + damages['Aggravated'] + damages['Future Care']\n",
    "    damages['Total'] = damages['Pecuniary Total'] + damages['Non-pecuniary']\n",
    "    \n",
    "    if damages['Total'] == 0 and total is not None: # Only use the \"total\" if we couldnt find anything else!\n",
    "        damages['Total'] = total\n",
    "        damages['General'] = total\n",
    "        \n",
    "    columns = ['Total', 'Pecuniary Total', 'Non-pecuniary', 'Special', 'General', 'Punitive', 'Aggravated', 'Future Care']\n",
    "    for c in columns:\n",
    "        damages[c] = None if damages[c] == 0 else damages[c]\n",
    "    \n",
    "    return damages\n",
    "\n",
    "def assign_damage_to_category(damage, damage_keywords, match, match_score, matches, damage_type, damage_dict, repetition_dict, repetition_key = None):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Adds damage to dictionary based on given parameters so long as it is the\n",
    "    highest scoring match & doesn't appear in the repetition dictionary\n",
    "    \n",
    "    Argumets:\n",
    "    damage (float) - The damage amount in the match\n",
    "    damage_keywords (list) - Keywords that may appear in match\n",
    "    match (string) - The match string itself\n",
    "    matches (list) - All matches. Used to determine if we found the best match\n",
    "    damage_dict (dict) - Dictionary storing all damages\n",
    "                       - Will be modified in place\n",
    "    repetition_dict (dict) - Dictionary storing repeated values\n",
    "                           - Will be modified in place\n",
    "    (Optional) repetition_key (Tuple) - If not none, will use this key to store repetitions. Else will use matching keyword\n",
    "    \n",
    "    Returns:\n",
    "    value_belongs (Boolean) - True if the value belongs in the given keyword category. False otherwise\n",
    "    '''\n",
    "    match = match.lower()\n",
    "    value_belongs = False\n",
    "    \n",
    "    for keywords in damage_keywords:\n",
    "        if match_contains_words(match, keywords):\n",
    "            value_belongs = True\n",
    "            if is_best_score(match_score, matches, keywords):\n",
    "                if damage not in repetition_dict[repetition_key if repetition_key else keywords]:\n",
    "                    damage_dict[damage_type] += damage\n",
    "                    repetition_dict[repetition_key if repetition_key else keywords].add(damage)\n",
    "            break\n",
    "    \n",
    "    return value_belongs\n",
    "\n",
    "def clean_money_amount(money_regex_match):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Arguments:\n",
    "    money_regex_match (Regex.findall object) - Match of $ amount\n",
    "    \n",
    "    Returns:\n",
    "    None if a bad match\n",
    "    extracted_value (float) - The money amount in float form\n",
    "    '''\n",
    "    # If our regex contains more than 1 or 0 money values. We cannot use the match.\n",
    "    if len(money_regex_match) > 1:\n",
    "        return None\n",
    "    if len(money_regex_match) == 0:\n",
    "        print('Error: No Money in match!', match)\n",
    "        return None\n",
    "    extracted_value = None\n",
    "    amount = money_regex_match[0].replace(',', '')\n",
    "    amount.replace(' ', '')\n",
    "    # Deals with money at end of sentence. example) ... for '5,000.00.' -> '5000.00'\n",
    "    if amount[-1] == '.': \n",
    "        amount = amount[:-1]\n",
    "    if 'million' in amount or amount[-1] == 'm':\n",
    "        amount = str(float(re.findall('[0-9|\\.]+', amount)[0])*10e6)\n",
    "    # Deals with a rare typo in some cases. example) 50.000.00 -> 50000.00\n",
    "    if amount.count('.') > 1: \n",
    "        dot_count = amount.count('.')\n",
    "        changes_made = 0\n",
    "        new_amount = ''\n",
    "        for letter in amount:\n",
    "            if letter == '.' and changes_made != dot_count-1:\n",
    "                changes_made += 1\n",
    "            else:\n",
    "                new_amount += letter\n",
    "        amount = new_amount\n",
    "    extracted_value = float(amount)\n",
    "    return extracted_value\n",
    "\n",
    "def get_matching_text(patterns, text, max_match_len_split):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given a set of regex; pulls out all matching text\n",
    "    \n",
    "    Arguments:\n",
    "    patterns (list) - List of regex patterns in string format\n",
    "    text (string) - Text to search for matches in\n",
    "    \n",
    "    Returns:\n",
    "    matches (list) - List containing all matches in text format\n",
    "    '''\n",
    "\n",
    "    matches = []\n",
    "    for pattern in patterns:\n",
    "        for match in re.findall(pattern, text, re.IGNORECASE):\n",
    "            if 'and' not in match:\n",
    "                if len(match.split()) <= max_match_len_split:\n",
    "                    matches.append(match)\n",
    "                    \n",
    "    return matches\n",
    "\n",
    "def is_best_score(score, matches, keywords):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given a set of regex matches, determine if the score is the highest score out of all matches for the given keywords\n",
    "    \n",
    "    Arguments:\n",
    "    score (float) - The score of the item you're inspecting\n",
    "    matches (list) - List of matches where each element is of form (score, match text)\n",
    "    keywords (tuple) - All words that should appear in the match\n",
    "    \n",
    "    Returns: True or False\n",
    "    \n",
    "    '''\n",
    "    best_score = score\n",
    "    \n",
    "    for score, match in matches:\n",
    "        if all(word in match.lower() for word in keywords):\n",
    "            if score > best_score:\n",
    "                return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def match_contains_words(match, words):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given some text. Find if the words are all present in the text.\n",
    "    If word begins with '!' the word cannot appear in the text. Can handle mix/matching of both types.\n",
    "    \n",
    "    Arguments:\n",
    "    match (String) - The text to look for words in\n",
    "    words (list) - List of words to check for. If word begins with ! (i.e. '!past'), then the word cannot appear in it\n",
    "    \n",
    "    Returns:\n",
    "    True if all words are present (or not present if using !)\n",
    "    False otherwise\n",
    "    \n",
    "    '''\n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    "    for word in words:\n",
    "        if word.startswith('!'):\n",
    "            neg_words.append(word[1:])\n",
    "        else:\n",
    "            pos_words.append(word)\n",
    "            \n",
    "    if all(word in match for word in pos_words):\n",
    "        if all(word not in match for word in neg_words):\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = rule_based_parse_BCJ('../data/Lexis Cases txt/P1.txt')\n",
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../data/Lexis Cases txt/'\n",
    "list_of_files = os.listdir(path)\n",
    "print(list_of_files)all_cases_parsed =[]\n",
    "\n",
    "for file in list_of_files:\n",
    "    if file != \".DS_Store\" and file != '.ipynb_checkpoints':\n",
    "        all_cases_parsed.extend(rule_based_parse_BCJ(path + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_parsed =[]\n",
    "\n",
    "for file in list_of_files:\n",
    "    if file != \".DS_Store\" and file != '.ipynb_checkpoints':\n",
    "        all_cases_parsed.extend(rule_based_parse_BCJ(path + file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_parsed[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_convert_cases_to_DF(cases):\n",
    "    '''Given a list of parsed cases returns a dataframe'''\n",
    "    lists = defaultdict(list)    \n",
    "    for case in cases:\n",
    "        lists['Case Number'].append(case['case_number'])\n",
    "        lists['Case Name'].append(case['case_title'])\n",
    "        lists['Year'].append(case['year'])\n",
    "        lists['Total Damage'].append(case['damages']['Total'] if case['damages'] != None else None)\n",
    "        lists['Total Pecuniary'].append(case['damages']['Pecuniary Total'] if case['damages'] != None else None)\n",
    "        lists['Non Pecuniary'].append(case['damages']['Non-pecuniary'] if case['damages'] != None else None)\n",
    "        lists['General'].append(case['damages']['General'] if case['damages'] != None else None)\n",
    "        lists['Special'].append(case['damages']['Special'] if case['damages'] != None else None)\n",
    "        lists['Punitive'].append(case['damages']['Punitive'] if case['damages'] != None else None)\n",
    "        lists['Aggravated'].append(case['damages']['Aggravated'] if case['damages'] != None else None)\n",
    "        lists['Future Care'].append(case['damages']['Future Care'] if case['damages'] != None else None)\n",
    "        lists['Judge Name'].append(case['judge'])\n",
    "        lists['Decision Length'].append(case['decision_length'])\n",
    "        lists['Multiple defendants?'].append(case['multiple_defendants'])\n",
    "#         lists['File'].append(case['filename'])\n",
    "        lists['Plaintiff Wins?'].append(case['plaintiff_wins'])\n",
    "        lists['Contributory Negligence Raised'].append(case['contributory_negligence_raised'])\n",
    "        lists['Contributory Negligence Successful'].append(case['contributory_negligence_successful'])\n",
    "        lists['Percent Reduction'].append(case['percent_reduction'])\n",
    "        lists['Written Decision?'].append(case['written_decision'])\n",
    "        lists['Registry'].append(case['registry'])\n",
    "    df = pd.DataFrame()\n",
    "    for key in lists.keys():\n",
    "        df[key] = lists[key]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dev_data, gold_data, subset=None):\n",
    "    # keep track of wrong % reductions\n",
    "    case_titles_incorrect = set()\n",
    "    print('#### Evaluation ####')\n",
    "    # Use case name as 'primary key'\n",
    "    dev_case_names = list(dev_data['Case Name'])\n",
    "    gold_case_names = list(gold_data['Case Name'])\n",
    "    # Filter data to only use overlapping items\n",
    "    gold_data = gold_data[gold_data['Case Name'].isin(dev_case_names)]\n",
    "    dev_data = dev_data[dev_data['Case Name'].isin(gold_case_names)]\n",
    "    # Mapping from our variable names to Lachlan's column names\n",
    "    column_mapping = {'Decision Length': 'Decision Length: paragraphs)',\n",
    "                      'Total Damage': '$ Damages total before contributory negligence',\n",
    "                      'Non Pecuniary': '$ Non-Pecuniary Damages', \n",
    "                      'Total Pecuniary': '$ Pecuniary Damages Total',\n",
    "                      'Special': '$ Special damages Pecuniary (ie. any expenses already incurred)',\n",
    "                      'Future Care': 'Future Care Costs (General Damages)',\n",
    "                      'General': '$ General Damages',\n",
    "                      'Punitive': '$ Punitive Damages',\n",
    "                      'Aggravated': '$Aggravated Damages',\n",
    "                      'Contributory Negligence Raised': 'Contributory Negligence Raised?',\n",
    "                     'Contributory Negligence Successful':'Contributory Negligence Successful?',\n",
    "                     'Percent Reduction':'% Reduction as a result of contributory negligence'\n",
    "                     }\n",
    "    dev_data.rename(columns = column_mapping, inplace = True)\n",
    "    if subset is None: # Use all columns if no subset specified\n",
    "        subset = dev_data.columns\n",
    "    for column in dev_data.columns:\n",
    "        if column in gold_data.columns:\n",
    "            if column in subset:\n",
    "                empty_correct = 0\n",
    "                non_empty_correct = 0\n",
    "                total_empty = 0\n",
    "                total_non_empty = 0\n",
    "                for case_name in list(dev_data['Case Name']):\n",
    "                    dev_value = list(dev_data[dev_data['Case Name'] == case_name][column])[0]\n",
    "                    gold_value = list(gold_data[gold_data['Case Name'] == case_name][column])[0]\n",
    "                    # Convert string to float if possible\n",
    "                    try:\n",
    "                        gold_value = float(gold_value)\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        dev_value = float(dev_value)\n",
    "                    except:\n",
    "                        pass\n",
    "                    # Set values to 'None' if they're a NaN float value\n",
    "                    dev_value = None if isinstance(dev_value, float) and math.isnan(dev_value) else dev_value\n",
    "                    gold_value = None if isinstance(gold_value, float) and math.isnan(gold_value) else gold_value\n",
    "                    # Lowercase values if they're a string\n",
    "                    dev_value = dev_value.lower().strip() if isinstance(dev_value, str) else dev_value\n",
    "                    gold_value = gold_value.lower().strip() if isinstance(gold_value, str) else gold_value\n",
    "                    if gold_value is None:\n",
    "                        total_empty += 1\n",
    "                        if dev_value is None:\n",
    "                            empty_correct += 1\n",
    "                    else:\n",
    "                        total_non_empty += 1\n",
    "                        if isinstance(dev_value, float) and isinstance(gold_value, float):\n",
    "                            if math.isclose(dev_value, gold_value, abs_tol=1): # Tolerance within 1\n",
    "                                non_empty_correct += 1\n",
    "                        elif dev_value == gold_value:\n",
    "                            non_empty_correct += 1\n",
    "                     # trying to trouble shoot % reduction issues       \n",
    "                    if column == '% Reduction as a result of contributory negligence' and gold_value != dev_value:\n",
    "                        case_titles_incorrect.add(case_name)\n",
    "                        print(case_name)\n",
    "                        print(dev_data[dev_data['Case Name'] == case_name]['Case Number'])\n",
    "                        print('gold:', gold_value)\n",
    "                        print('dev:', dev_value)\n",
    "                        print('======')\n",
    "                print('-------')\n",
    "                print('COLUMN:', column)\n",
    "                if total_empty != 0:\n",
    "                    print('Empty field accuracy:', empty_correct / total_empty * 100, '%', empty_correct, '/', total_empty)\n",
    "                if total_non_empty != 0:\n",
    "                    print('Filled field accuracy:', non_empty_correct / total_non_empty * 100, '%', non_empty_correct, '/', total_non_empty)\n",
    "                print('Overall accuracy:', (empty_correct+non_empty_correct) / (total_non_empty+total_empty) * 100, '%', (empty_correct+non_empty_correct), '/', (total_non_empty+total_empty))\n",
    "    return case_titles_incorrect\n",
    "    # for testing:\n",
    "    #return dev_data, gold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = rule_based_convert_cases_to_DF(all_cases_parsed)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gold = pd.read_excel('../data/Case Annotation.xlsx', header=2)\n",
    "gold['Contributory Negligence Successful?'].replace(to_replace = ['Y', 'y'], value = True, inplace = True)\n",
    "gold['Contributory Negligence Successful?'].replace(to_replace = ['N', 'n'], value = False, inplace = True)\n",
    "gold['Contributory Negligence Successful?'].replace(to_replace = [np.nan], value = False, inplace = True)\n",
    "gold['Contributory Negligence Raised?'].replace(to_replace = ['Y', 'y'], value = True, inplace = True)\n",
    "gold['Contributory Negligence Raised?'].replace(to_replace = ['N', 'n'], value = False, inplace = True)\n",
    "gold['Contributory Negligence Raised?'].replace(to_replace = [np.nan], value = False, inplace = True)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold[gold['Case Name'] == 'Gill v. A&P Fruit Growers Ltd., [2009] B.C.J. No. 593']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['Case Name'] == 'Paskall v. Scheithauer, [2012] B.C.J. No. 2601']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "case_titles_incorrect = evaluate(test_df, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_tokenize(case):\n",
    "    ''' String of Entire Document and returns list of lists of paragraphs in document\n",
    "    ---------\n",
    "    Input: case (str) - string of single legal case\n",
    "    Return: docs_split(list) - list of of numbrered paragraphs in the document where the first item is the case_title'''\n",
    "    \n",
    "    case_data = []\n",
    "    lines = case.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    case_data.append(lines[0])\n",
    "    decision_length = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', case, re.IGNORECASE).group(1)\n",
    "\n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    pattern = r'.?(?=\\n[0-9]{1,%s}[\\xa0]{2})'%len(decision_length)\n",
    "    paras_split = re.split(pattern, case, re.IGNORECASE)\n",
    "\n",
    "    paras = []\n",
    "    for para in paras_split:   \n",
    "        # make sure the paragraph starts with the correct characters\n",
    "        para_start = re.match(r'^\\n([0-9]{1,%s})[\\xa0]{2}'%len(decision_length), para, re.IGNORECASE)\n",
    "        if para_start:\n",
    "            paras.append(para)\n",
    "    case_data.extend(paras)\n",
    "    return case_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_tokenize(case):\n",
    "    ''' String of Entire Document and returns the document summary and HELD section.\n",
    "    ---------\n",
    "    Input: doc (str) - string of single legal case\n",
    "    Return: summary - summary and HELD section of case (str)'''\n",
    "    \n",
    "    lines = case.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    \n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    summary = re.search(r'\\([0-9]{1,3} paras\\.\\)\\ncase summary\\n((.*\\n+?)+)(?=HELD|(Statutes, Regulations and Rules Cited:)|(Counsel\\n))', case, re.IGNORECASE)\n",
    "    if summary:\n",
    "        summary = summary.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_and_float(value, text, context_length = 6, plaintiff_name = 'Plaintiff', defendant_name = 'Defendant'):\n",
    "    '''Given a string value found in a body of text, \n",
    "    return a its context of length context_length, and its float equivalent.\n",
    "    -----------------\n",
    "    Arguments:\n",
    "    value - percent match found in text\n",
    "    text - string value where matches were extracted from, eg paragraph or summary (str)\n",
    "    context_length - the length of context around each quantity to return\n",
    "    Rerturn:\n",
    "    value_context - string of context around value (str)\n",
    "    extracted_value - string quantity value extracted to its float equivalent\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # get context for monetary/percent values \n",
    "    \n",
    "    context = ''\n",
    "    amount = re.findall(r'[0-9]+[0-9|,]*(?:\\.[0-9]+)?', value)\n",
    "    extracted_value = clean_money_amount(amount) #use helper function to get float of dollar/percent value\n",
    "    if not extracted_value:\n",
    "        print('cant convert string, %s'%value)\n",
    "        return context, None\n",
    "    # get indices of last instance of value in text - tokenize like this for values of type 'per cent and percent'\n",
    "    start_idx = text.rfind(value)\n",
    "    if start_idx == -1:\n",
    "        print('ERROR: value not in text')\n",
    "    end_idx = start_idx + len(value)\n",
    "    tokens = text[:start_idx].split() + [value] + text[end_idx:].split()\n",
    "#     if 'percent' in value:\n",
    "#         value = amount[0]+'%'\n",
    "#         text = text.replace(value, amount[0]+'%')\n",
    "#     elif 'per cent' in value:\n",
    "#         value = amount[0]+'%'\n",
    "#         text = text.replace(value, amount[0]+'%')\n",
    "#     tokens = text.split()\n",
    "    loc = [i for i, token in enumerate(tokens) if value in token] \n",
    "    # if the quantity is in the text, choose context of last mention of value\n",
    "    if len(loc) > 0:\n",
    "        loc = loc[-1] \n",
    "        if loc - context_length >= 0 and loc + context_length < len(tokens):\n",
    "            context = \" \".join(tokens[loc - context_length:loc + context_length + 1])\n",
    "        elif loc - context_length < 0 and loc + context_length < len(tokens):\n",
    "            beg = abs(loc -context_length)\n",
    "            context = \" \".join(tokens[loc-context_length + beg:loc + context_length + 1])\n",
    "        elif loc - context_length > 0 and loc + context_length > len(tokens): \n",
    "            context = \" \".join(tokens[loc - context_length:len(tokens)])\n",
    "\n",
    "\n",
    "    return context.lower(), extracted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_parse_BCJ(path):\n",
    "    '''Given file path (text file) of negligence cases, finds static \n",
    "    information within the case (information that can be pattern matched)\n",
    "    Expects a B.C.J. case format (British Columbia Judgments)\n",
    "    \n",
    "    The following fields are currently implemented:\n",
    "    - Case Title\n",
    "    - Judge Name\n",
    "    - Registry\n",
    "    - Year\n",
    "    - Decision Length (in paragraphs)\n",
    "    - Damages\n",
    "    - Multiple Defendants\n",
    "    - Plaintiff Wins\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: case_parsed_data (list) of case_dict (Dictionary): List of Dictionaries with rule based parsable fields filled in\n",
    "    '''\n",
    "    if path:\n",
    "        with open(path, encoding='utf-8') as document:\n",
    "            document_data = document.read()\n",
    "        document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "    \n",
    "\n",
    "    case_parsed_data = []\n",
    "    for i in range(len(document_data)):\n",
    "        case_dict = dict() \n",
    "        case = document_data[i]\n",
    "        case = case.strip() # Make sure to strip!\n",
    "        if len(case) == 0: # Skip empty lines\n",
    "            continue\n",
    "        \n",
    "        lines = case.split('\\n')\n",
    "        if len(lines) < 2:\n",
    "            print(case)\n",
    "        case_title = lines[0]\n",
    "        case_type = lines[1]\n",
    "\n",
    "        if 'R. v.' in case_title or '(Re)' in case_title: # Skip crown cases, Skip (Re) cases\n",
    "            continue\n",
    "            \n",
    "        # Skip client/solicitor cases (not same as plaintiff/defendant)\n",
    "        regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?|[L|l]awyer[s]?))', case)\n",
    "        if regex_client_solicitor:\n",
    "            continue\n",
    "        \n",
    "        regex_solicitor_client = re.search(r'(Between.*([L|l]awyer[s]?|[S|s]olicitor[s]?).*([C|c]lient[s]?))', case)\n",
    "        if regex_solicitor_client:\n",
    "            continue\n",
    "            \n",
    "        # In some rare cases we have 'IN THE MATTER OF ..' (rather than 'Between ...') .. but it is following by the normal\n",
    "        # plaintiff/defendant dynamic. Only skip cases if there is no mention of the following terms\n",
    "        # (Can be cleaned up in future)\n",
    "        key_words = ['appellant', 'respondent', 'claimant', 'petitioner', 'plaintiff', 'defendant',\n",
    "        'appellants', 'respondents', 'claimants', 'petitioners', 'plaintiffs', 'defendants']\n",
    "        regex_in_matter_of = re.search(r'IN THE MATTER OF .*\\n\\([0-9]+ paras.\\)', case)\n",
    "        if regex_in_matter_of:\n",
    "            remove = True\n",
    "            for key in key_words:\n",
    "                if key in regex_in_matter_of.group(0).lower().strip():\n",
    "                    remove = False\n",
    "                    \n",
    "            if remove:\n",
    "                continue\n",
    "\n",
    "        if 'British Columbia Judgments' in case_type: # Make sure we're dealing with a B.C.J. case\n",
    "        \n",
    "            # Fields that can be found via pattern matching\n",
    "            if re.search('contributory negligence', case, re.IGNORECASE):\n",
    "                contributory_negligence_raised = True\n",
    "            else:\n",
    "                contributory_negligence_raised = False\n",
    "            case_number = re.search(r'\\/P([0-9]+)\\.txt', path).group(1)\n",
    "            decision_len = re.search(r'\\(([0-9]+) paras\\.?\\)', case) # e.g.) (100 paras.)\n",
    "            registry = re.search(r'(Registry|Registries): ?([A-Za-z0-9 ]+)', case) # e.g.) Registry: Vancouver\n",
    "            written_decision = True if int(decision_len.group(1)) > 1 else False\n",
    "            if registry:\n",
    "                registry = registry.group(2).strip()\n",
    "            else:\n",
    "                registry = re.search(r'([A-Za-z ]+) Registry No.', case) # Alt form e.g.) Vancouver Registory No. XXX\n",
    "                if registry:\n",
    "                    registry = registry.group(1).strip()\n",
    "                else:\n",
    "                    registry = re.search(r'([A-Za-z ]+) No. S[0-9]*', case)\n",
    "                    if registry:\n",
    "                        registry = registry.group(1).strip()\n",
    "                    else:\n",
    "                        print('WARNING: Registry could not be found (This shouldn\\'t occur!)')\n",
    "            # Fields that are always in the same place\n",
    "            judge_name = lines[4].strip()\n",
    "            case_title = lines[0].strip()\n",
    "            # Extract year from case_title (in case we want to make visualizations, etc.)\n",
    "            year = re.search(r'20[0-2][0-9]', case_title) # Limit regex to be from 2000 to 2029\n",
    "            if year:\n",
    "                year = year.group(0)\n",
    "            else:\n",
    "                # Rare case: Sometimes the title is too long. Rely on Heard date.\n",
    "                year = re.search(r'Heard:.* ([2][0][0-2][0-9])', case)\n",
    "                if year:\n",
    "                    year = year.group(1)\n",
    "                else:\n",
    "                    print('WARNING: Year not found')\n",
    "            case_dict['case_number'] = '%s of %s'%(i+1+((int(case_number)-1)*50), case_number)\n",
    "            case_dict['case_title'] = case_title\n",
    "            case_dict['year'] = year\n",
    "            case_dict['registry'] = registry\n",
    "            case_dict['judge'] = judge_name\n",
    "            case_dict['decision_length'] = decision_len.group(1)\n",
    "            case_dict['multiple_defendants'] = rule_based_multiple_defendants_parse(case)\n",
    "            case_dict['contributory_negligence_raised'] = contributory_negligence_raised\n",
    "            case_dict['written_decision'] = written_decision\n",
    "            \n",
    "            # TODO: Improve plaintiff_wins to take one case at a time.\n",
    "            plaintiff_list = plaintiff_wins(path)\n",
    "            if case_title in plaintiff_list:\n",
    "                case_dict['plaintiff_wins'] = plaintiff_list[case_title]\n",
    "            else:\n",
    "                case_dict['plaintiff_wins'] = \"NA\"\n",
    "                \n",
    "            case_dict['damages'] = rule_based_damage_extraction(case)\n",
    "            percent_reduction, contributory_negligence_successful = get_percent_reduction_and_contributory_negligence_success(case_dict, case)\n",
    "            case_dict['percent_reduction'] = percent_reduction\n",
    "            case_dict['contributory_negligence_successful'] = contributory_negligence_successful\n",
    "        # don't add empty dictionaries (non BCJ cases) to list\n",
    "        if case_dict != dict(): \n",
    "            case_parsed_data.append(case_dict)\n",
    "    return case_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities):\n",
    "    ''' Given the context surrounding an extracted value, keywords relevant to contributory negligence, \n",
    "    a list of the Plaintiffs names, a list of the defendants names, and a combined list of entities related to either the Plaintiff or Defendant:\n",
    "    Return the modifed extracted value\n",
    "    ------------\n",
    "    Arugments:\n",
    "    context: (str)\n",
    "    extracted_value: (float) found in context\n",
    "    keywords, plaintiff_split, defendant_split, entities: (list) of strings\n",
    "    '''\n",
    "    # conditions for keeping extracted_value and updating extracted_value\n",
    "    # skip extracted_values with contexts lacking keywords/entities\n",
    "    if extracted_value == 100 or extracted_value == 0 or extracted_value < 10:\n",
    "        extracted_value = None\n",
    "        return extracted_value\n",
    "    if not any(token in context for token in keywords + entities) or context == '' or any('costs' == token for token in context.split()) or ('interest' in context and 'rate' in context.split()):\n",
    "        extracted_value = None\n",
    "        return extracted_value\n",
    "    if 'recover' in context and any(word in context for word in plaintiff_split + ['plaintiff']):\n",
    "#         print('plaintiff recovers percent subtract %s from 100'%extracted_value)\n",
    "        extracted_value = 100 - extracted_value\n",
    "    if any(word1 in context and word2 in context for word1 in defendant_split + ['defendant'] for word2 in ['liable', 'responsible', 'fault', 'against']):\n",
    "#         print('defendat is %s liable, subtract from 100'%extracted_value)\n",
    "        extracted_value = 100 - extracted_value\n",
    "    return extracted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['against', 'reduce', 'liability', 'liable', 'contributor', 'fault', 'apportion', 'recover', 'responsible']\n",
    "entities = ['defendant', 'plaintiff', 'she', 'he', 'John', 'Jane']\n",
    "context1 = 'was held 90%'\n",
    "test =keywords + entities\n",
    "\n",
    "if not any(word in context for word in test):\n",
    "    print('fdsfm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contributory_negligence_successful_fun(context, keywords):\n",
    "    '''Given text containing percent reduction and a list of keywords to check for,\n",
    "    confirm presence of keywords and return whether or not contributory negligence was successful\n",
    "    --------------\n",
    "    Arguments:\n",
    "    context (str)\n",
    "    keywords(list)\n",
    "    Returns: True or None (bool)'''\n",
    "    if any(word in context for word in keywords):\n",
    "        if 'plaintiff' or 'damages' or 'defendant' in context:\n",
    "            contributory_negligence_successful = True\n",
    "            return contributory_negligence_successful\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_reduction_and_contributory_negligence_success(case_dict, case, min_score = 0.9):\n",
    "    paragraphs = paragraph_tokenize(case)\n",
    "    case_title = case_dict['case_title']\n",
    "    assert paragraphs[0] == case_title\n",
    "    # default value for contributory negligence success is FALSE\n",
    "    contributory_negligence_successful = False\n",
    "    percent_pattern = r'([0-9][0-9|\\.]*(?:%|\\sper\\s?cent))'\n",
    "    # entities and keywords used to filter percent values\n",
    "    keywords = ['against', 'reduce', 'liability', 'liable', 'contributor', 'fault', 'apportion', 'recover', 'responsible']\n",
    "    # extract plaintiff and defendant name for use in %reduction conditions\n",
    "    plaintiff_defendant_pattern = r'([A-Za-z|-|\\.]+(:? \\(.*\\))?)+ v\\. ([A-Za-z|-]+)+' # group 1 is plaintiff group 2 is defendant\n",
    "    if re.search(plaintiff_defendant_pattern, case_title):\n",
    "        plaitiff_defendant = re.search(plaintiff_defendant_pattern, case_title).groups() # tuple (plaintiff, defendant)\n",
    "    else:\n",
    "        plaitiff_defendant = ('Plaintiff', 'Defendant')\n",
    "    plaintiff_split = [word.lower() for word in plaitiff_defendant[0].split()]\n",
    "    defendant_split = [word.lower() for word in plaitiff_defendant[-1].split()]\n",
    "    entities = ['defendant', 'plaintiff'] + plaintiff_split + defendant_split \n",
    "\n",
    "    if case_dict['contributory_negligence_raised'] and case_dict['plaintiff_wins']:\n",
    "        #### troubleshooting~~\n",
    "        if case_title in case_titles_incorrect:\n",
    "            print(case_title)\n",
    "        percent_reduction = None\n",
    "        best_percent = None\n",
    "        best_score = 0\n",
    "        for j, paragraph in enumerate(paragraphs[1:]):\n",
    "            score = float((j+1)/int(case_dict['decision_length']))\n",
    "            paragraph = paragraph.lower()\n",
    "            if not score >= min_score: ## min score not existant in bcj parser\n",
    "                continue\n",
    "\n",
    "            percent_mentioned = re.findall(percent_pattern, paragraph, re.IGNORECASE)\n",
    "            extracted_value_tie_breaker = Counter()\n",
    "            if len(percent_mentioned) > 0:\n",
    "                for percent in percent_mentioned:\n",
    "                    context, extracted_value = get_context_and_float(percent, paragraph)\n",
    "                    # conditions for keeping extracted_value and updating extracted_value\n",
    "                    # skip extracted_values with contexts lacking keywords/entities\n",
    "                    if context == '':\n",
    "                        continue\n",
    "                    extracted_value = conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities)\n",
    "                    if not extracted_value:\n",
    "                        \n",
    "                        continue\n",
    "                        \n",
    "                    extracted_value_tie_breaker.update([extracted_value])\n",
    "                \n",
    "                    # conditions for contributory negligence successful\n",
    "                    if not contributory_negligence_successful and extracted_value:\n",
    "                        contributory_negligence_successful = contributory_negligence_successful_fun(context, keywords)\n",
    "                    #### troubleshooting~~\n",
    "                    if case_title in case_titles_incorrect:\n",
    "                        print(extracted_value_tie_breaker, context)\n",
    "                        \n",
    "                    # matches patter \"PERCENT against plaintiff\"\n",
    "                    if ('against' in context or 'fault' in context) and any(plaintiff_word in context for plaintiff_word in plaintiff_split+['plaintiff']):\n",
    "                        best_percent = extracted_value\n",
    "                        best_score = score\n",
    "                        break                    \n",
    "                    \n",
    "                    # choose most common percent mentioned in highest scoring paragraph\n",
    "                    if extracted_value_tie_breaker != Counter():\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_percent = extracted_value_tie_breaker.most_common(1)[0][0]\n",
    "\n",
    "                #### troubleshooting~~\n",
    "                if case_title in case_titles_incorrect:\n",
    "                    print(\"paragraph:\", best_score, best_percent)\n",
    "                    print('======')\n",
    "             # if no percent found, check for equal apportionment\n",
    "            else:\n",
    "                equal_apportionment = re.findall(r'.{20} (?:liability|fault) [a-zA-Z]{1,3} apportione?d? equally .{20}', paragraph)\n",
    "                if len(equal_apportionment) > 0:\n",
    "                    if contributory_negligence_successful_fun(equal_apportionment[0], keywords):\n",
    "                        best_percent = 50.0\n",
    "                        contributory_negligence_successful = True\n",
    "        \n",
    "        if best_score == 0 or not best_percent or not contributory_negligence_successful:\n",
    "            # no percents found in paragraphs - time to check summary - same process\n",
    "            summary = summary_tokenize(case)\n",
    "            if summary:\n",
    "                summary = summary.lower()\n",
    "                percent_mentioned = re.findall(percent_pattern, summary, re.IGNORECASE)\n",
    "                #### troubleshooting~~\n",
    "                if case_title in case_titles_incorrect:\n",
    "                    print('checking summary...')\n",
    "                    print(percent_mentioned)\n",
    "                extracted_value_tie_breaker = Counter()\n",
    "                if len(percent_mentioned) > 0:\n",
    "                    for percent in percent_mentioned:\n",
    "                        context, extracted_value = get_context_and_float(percent, summary)\n",
    "                        #### troubleshooting~~\n",
    "                        if case_title in case_titles_incorrect:\n",
    "                            print(extracted_value, context)\n",
    "                        # conditions for keeping extracted_value and updating extracted_value\n",
    "                        # skip extracted_values with contexts lacking keywords/entities\n",
    "                        extracted_value = conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities)\n",
    "                        if not extracted_value:\n",
    "                            continue\n",
    "                        extracted_value_tie_breaker.update([extracted_value])\n",
    "                                                   \n",
    "                        # conditions for contributory negligence successful\n",
    "                        if not contributory_negligence_successful and extracted_value:\n",
    "                            contributory_negligence_successful = contributory_negligence_successful_fun(context, keywords) \n",
    "                            \n",
    "                        # matches patter \"PERCENT against plaintiff\"\n",
    "                        if ('against' in context or 'fault' in context) and any(plaintiff_word in context for plaintiff_word in plaintiff_split+['plaintiff']):\n",
    "                            best_percent = extracted_value\n",
    "                            best_score = score\n",
    "                            break \n",
    "                        # choose most common percent mentioned in summary\n",
    "                        if extracted_value_tie_breaker != Counter():\n",
    "                            best_percent = extracted_value_tie_breaker.most_common(1)[0][0]\n",
    "                        \n",
    "                        #### troubleshooting~~\n",
    "                        if case_title in case_titles_incorrect:\n",
    "                            print(\"summary:\", best_score, best_percent)\n",
    "                            print('======')\n",
    "               # if no percent found, check for equal apportionment\n",
    "                else:\n",
    "                    #### troubleshooting~~\n",
    "                    if case_title in case_titles_incorrect:\n",
    "                        print('checking equal apportionment...')\n",
    "                    equal_apportionment = re.findall(r'.{20} (?:liability|fault) [a-zA-Z]{1,3} apportione?d? equally .{20}', summary)\n",
    "                    if len(equal_apportionment) > 0:\n",
    "                        if contributory_negligence_successful_fun(equal_apportionment[0], keywords):\n",
    "                            best_percent = 50.0\n",
    "                            contributory_negligence_successful = True\n",
    "        if contributory_negligence_successful:\n",
    "            percent_reduction = best_percent\n",
    "    else:\n",
    "        percent_reduction = None\n",
    "    #### troubleshooting~~\n",
    "    if case_title in case_titles_incorrect:\n",
    "        print(percent_reduction)    \n",
    "    return percent_reduction, contributory_negligence_successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Case Summary\n",
    "Damages — Mitigation — In tort — Personal injuries, treatment for — Torts — Negligence — Standard of care, particular persons and relationships — Police officers — Motor vehicle, standard of care of driver — Keeping a proper lookout — Emergencies — Circumstances requiring caution or extreme caution — Emergency or police vehicles — Defences — Contributory negligence — Apportionment of fault.\n",
    "Action by Blackburn for damages for injuries suffered when her vehicle collided with a police car at an intersection. Constable Leyh was on his way to another accident and was proceeding through the intersection with his lights and siren activated. He slowed down as he approached the intersection and began speeding up half-way through it. Blackburn was a hearing impaired 17-year-old driver who was completely deaf without her hearing aids. Although she was wearing her hearing aids, witnesses stated that her radio was playing loudly. Blackburn did not hear the siren at all. Several vehicles in the lane next to Blackburn's lane had stopped at the green light. Blackburn proceeded through the intersection on the green light and was struck by the police car proceeding through the intersection on the red light. Constable Leyh had stopped at his home for his rain coat prior to leaving for the scene of the accident. As a result of the collision, Blackburn suffered a moderately severe cervical sprain and post-traumatic stress. However, she failed to follow the recommended course of treatment. \n",
    "\n",
    "HELD: Action allowed in part.\n",
    " Constable Leyh took a calculated risk that was not proportionate to the urgency of the situation. The amount of time consumed by retrieving his raincoat was much greater than the few seconds that would have been consumed by momentarily delaying his acceleration. Speeding up where there was still a risk of a vehicle approaching was not justified by the urgency of the situation. Blackburn failed to observe that traffic in the lane next to her had stopped at the green light and did not reduce her speed or exercise appropriate caution as she approached the intersection. Visual attentiveness took on an added importance in respect of a deaf person driving a car. Blackburn should have paid special attention to visual clues and should not have had the volume of her radio turned up loudly. Fault for the accident was apportioned 80 per cent to Blackburn and 20 percent to Constable Leyh. Taking into account Blackburn's failure to mitigate her damages, her non-pecuniary losses were assessed at $30,000 subject to the liability apportionment.'''\n",
    "get_context_and_float('80', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations of Numbers/Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_annotations = set(gold.iloc[:44, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../data/Lexis Cases txt/'\n",
    "list_of_files = os.listdir(path)\n",
    "to_annotate = dict()\n",
    "\n",
    "# f = open('../data/ilanas_annotations_.txt', 'w')\n",
    "for file in list_of_files:\n",
    "    if file != \".DS_Store\" and file != '.ipynb_checkpoints':\n",
    "        with open(path+file, encoding='utf-8') as document:\n",
    "            document_data = document.read()\n",
    "        document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "        for i in range(len(document_data)):\n",
    "            case = document_data[i]\n",
    "            if len(case) == 0: # Skip empty lines\n",
    "                continue\n",
    "            lines = case.split('\\n')\n",
    "            case_title = lines[0]\n",
    "            if case_title in my_annotations:\n",
    "                to_annotate[case_title] = re.findall(r'\\$ ?[0-9][0-9|,|.]+[0-9]', case)\n",
    "#                 f.write(case + 'End of Document\\n')\n",
    "            \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import sent_tokenize\n",
    "import os\n",
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case2tags(case):\n",
    "    '''Input case is a string from the legal negligence case, with xml tags indicating damage types.\n",
    "    Return a list of quantities tagged (str) and a list of the associated damage type tags corresponding to those values\n",
    "    -------------------\n",
    "    Example: \n",
    "    case = \"I asses non-pecuniary damages of <damage type=non pecuniary>$1,000,000</damage>\"\n",
    "    case2tags(case) = ['$1,000,000'], ['non pecuniary']\n",
    "    '''\n",
    "    # your code here\n",
    "    soup = BeautifulSoup('<xml>'+case+'</xml>', \"xml\")\n",
    "    tags = []\n",
    "    values = []\n",
    "    full_match = []\n",
    "    for damage in soup.find_all('damage'):\n",
    "        if 'non-pecuniary' in damage['type']:\n",
    "            tags.append(damage['type'].replace('non-pecuniary', 'non pecuniary'))\n",
    "        elif 'income' in damage['type']:\n",
    "            tags.append(damage['type'].replace('income', 'wage'))\n",
    "        elif 'in trust' in damage['type']:\n",
    "            tags.append(damage['type'].replace('in trust', 'special'))\n",
    "        elif 'reduc' in damage['type']:\n",
    "            tags.append('reduction')\n",
    "        else:\n",
    "            tags.append(damage['type'])\n",
    "        values.append(damage.get_text())\n",
    "    return values, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'non pecuniary': 225,\n",
       "         'special': 180,\n",
       "         'other': 2763,\n",
       "         'future care': 148,\n",
       "         'past wage loss': 160,\n",
       "         'future wage loss': 147,\n",
       "         'total': 136,\n",
       "         'general': 61,\n",
       "         'sub-special': 127,\n",
       "         'sub-future wage loss': 33,\n",
       "         'sub-future care': 56,\n",
       "         'reduction': 33,\n",
       "         'total after': 19,\n",
       "         'sub-total': 18,\n",
       "         'punitive': 14,\n",
       "         'sub-past wage loss': 23,\n",
       "         'sub-non pecuniary': 12,\n",
       "         'aggravated': 8,\n",
       "         'sub-general': 15})"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stovel v. Paul, [2013] B.C.J. No. 33\n",
      "Szymanski v. Morin, [2010] B.C.J. No. 5\n",
      "Durand v. Bolt, [2007] B.C.J. No. 701\n",
      "Fox (Guardian ad litem of) v. Edwards, [2001] B.C.J. No. 370\n",
      "Quade v. Schwartz, [2008] B.C.J. No. 1032\n",
      "Qiao v. Buckley, [2008] B.C.J. No. 2533\n",
      "McKee (Guardian ad litem of) v. McCoy, [2001] B.C.J. No. 2675\n",
      "Arbutus Bay Estates Ltd. v. Canada (Attorney General), [2016] B.C.J. No. 2351\n",
      "Bideci v. Neuhold, [2014] B.C.J. No. 1382\n",
      "Bradley v. Bath, [2008] B.C.J. No. 1485\n",
      "Minhas v. Sartor, [2014] B.C.J. No. 69\n",
      "Myatt v. Holicza, [2000] B.C.J. No. 1610\n",
      "Aulakh v. Nahal, [2017] B.C.J. No. 1159\n",
      "Stegemann v. Pasemko, [2007] B.C.J. No. 1585\n",
      "P.B. v. R.V.E., [2007] B.C.J. No. 2305\n",
      "Browne v. Insurance Corp. of British Columbia, [2003] B.C.J. No. 2012\n",
      "M.A. Concrete Ltd. v. Truter, [2015] B.C.J. No. 276\n",
      "Haag v. Serry, [2009] B.C.J. No. 276\n",
      "Uppal v. Rawlins, [2010] B.C.J. No. 16\n",
      "Combs v. Bergen, [2013] B.C.J. No. 350\n",
      "Ahmadi v. West, [2014] B.C.J. No. 2680\n",
      "Notenbomer v. Andjelic, [2008] B.C.J. No. 721\n",
      "Curran v. MacDougall, [2006] B.C.J. No. 1391\n",
      "Foster v. Kindlan, [2012] B.C.J. No. 936\n",
      "McGlone (Guardian ad litem of) v. Kelly, [2002] B.C.J. No. 1368\n",
      "Frankson v. Myre, [2008] B.C.J. No. 1156\n",
      "Fiessel v. Fiessel, [2006] B.C.J. No. 1686\n",
      "Gallagher v. Olsson, [2001] B.C.J. No. 1340\n",
      "Harris v. Gill, [2019] B.C.J. No. 1839\n",
      "emoto (Litigation guardian of) v. Phagura, [2014] B.C.J. No. 283\n",
      "MacLean v. Budget-Rent-A-Car of Edmonton Ltd., [2006] B.C.J. No. 2032\n",
      "Stevens v. Charlie Estate, [2019] B.C.J. No. 1958\n",
      "Swieczko v. Nehme, [2016] B.C.J. No. 467\n",
      "Weston v. Shaw, [2017] B.C.J. No. 37\n",
      "Bouvier v. Behrend, [2014] B.C.J. No. 1362\n",
      "Cam v. Hood, [2006] B.C.J. No. 2766\n",
      "FBI Foods Ltd. v. Glassner, [2001] B.C.J. No. 193\n",
      "GE Capital Canada Equipment Financing Inc. v. Bank of Montreal, [2003] B.C.J. No. 1791\n",
      "Ostrikoff v. Oliveira, [2014] B.C.J. No. 549\n",
      "White v. Stonestreet, [2006] B.C.J. No. 1150\n",
      "Jackson v. Backus, [2001] B.C.J. No. 2370\n",
      "Gohringer v. Hernandez-Lazo, [2009] B.C.J. No. 615\n",
      "Mennonite Church British Columbia v. Sur-Del Roofing Ltd., [2010] B.C.J. No. 297\n",
      "Paskall v. Scheithauer, [2012] B.C.J. No. 2601\n",
      "Salgado v. Toth, [2009] B.C.J. No. 2230\n",
      "Gray v. Fraser Health Authority (c.o.b. Ridge Meadows Hospital), [2009] B.C.J. No. 372\n",
      "Kirkham v. Richardson, [2014] B.C.J. No. 1194\n",
      "Neff v. Patry, [2008] B.C.J. No. 209\n",
      "Hardychuk v. Johnstone, [2012] B.C.J. No. 1909\n",
      "Najdychor v. Swartz, [2009] B.C.J. No. 1202\n",
      "Delgiglio v. Becker, [2012] B.C.J. No. 650\n",
      "Rycroft v. Rego, [2017] B.C.J. No. 447\n",
      "Roger Garside Construction Ltd. v. Stirling, [2013] B.C.J. No. 1777\n",
      "Zhang v. Law, [2009] B.C.J. No. 1468\n",
      "Chamberlain v. Pro Star Mechanical Technologies Ltd., [2014] B.C.J. No. 2669\n",
      "Andrusko v. Alexander, [2013] B.C.J. No. 1161\n",
      "Anderson v. Minhas, [2011] B.C.J. No. 259\n",
      "Berenjian v. Primus, [2013] B.C.J. No. 194\n",
      "Kumar v. Picco, [2007] B.C.J. No. 2463\n",
      "Gillespie v. Yellow Cab Co., [2014] B.C.J. No. 2332\n",
      "Gregory v. Penner, [2010] B.C.J. No. 32\n",
      "Kuras v. Repo, [2014] B.C.J. No. 2204\n",
      "Okanagan-Similkameen (Regional District) v. Associated Engineering (B.C.) Ltd., [2015] B.C.J. No. 1105\n",
      "Kerr (Litigation Guardian of) v. Creighton, [2007] B.C.J. No. 309\n",
      "Rosso v. Balubal, [2014] B.C.J. No. 2385\n",
      "Akbari v. Insurance Corp. of British Columbia, [2012] B.C.J. No. 2451\n",
      "Lumanlan v. Sadler, [2008] B.C.J. No. 2184\n",
      "Gregorowicz (Litigation guardian of) v. Lee, [2010] B.C.J. No. 636\n",
      "Dzumhur v. Davoody, [2015] B.C.J. No. 2730\n",
      "Evans v. Keill, [2018] B.C.J. No. 3274\n",
      "Eaton v. Regan, [2005] B.C.J. No. 240\n",
      "Hawkenson v. Rogers, [2005] B.C.J. No. 456\n",
      "Leweke v. Saanich School District No. 63, [2004] B.C.J. No. 1985\n",
      "Kilian v. Valentin, [2012] B.C.J. No. 2009\n",
      "Cox v. Bounthavilay, [2007] B.C.J. No. 1776\n",
      "Blackburn v. British Columbia, [2001] B.C.J. No. 1647\n",
      "First Majestic Silver Corp. v. Santos, [2013] B.C.J. No. 834\n",
      "Koshman v. Brodis, [2013] B.C.J. No. 755\n",
      "Gill v. A&P Fruit Growers Ltd., [2009] B.C.J. No. 593\n",
      "Kahl v. Jakobsson, [2006] B.C.J. No. 1722\n",
      "Mainardi v. Shannon, [2005] B.C.J. No. 1033\n",
      "0813054 B.C. Ltd. v. Overland West Freight Lines Ltd., [2013] B.C.J. No. 2829\n",
      "Rhodes v. Biggar, [2010] B.C.J. No. 1022\n",
      "Mawani v. Pitcairn, [2012] B.C.J. No. 1819\n",
      "Ediger (Guardian ad litem of) v. Johnston, [2009] B.C.J. No. 564\n",
      "Furness v. Guest, [2010] B.C.J. No. 1388\n",
      "Fabian v. Song, [2018] B.C.J. No. 896\n",
      "Aiken (Guardian ad litem of) v. Van Dyk, [2001] B.C.J. No. 1751\n",
      "Millard v. Singleton, [2015] B.C.J. No. 1234\n",
      "Dr. Andrew Hokhold Inc. v. Wells (c.o.b. Spall Machine & Welding), [2005] B.C.J. No. 255\n",
      "Cowie v. Draper, [2010] B.C.J. No. 910\n",
      "Jackson v. Fisheries and Oceans Canada, [2006] B.C.J. No. 2654\n",
      "Kappell v. Brown, [2012] B.C.J. No. 139\n",
      "Bjornson v. Field, [2007] B.C.J. No. 2734\n",
      "Austin v. Joaquin, [2007] B.C.J. No. 1894\n",
      "Los Angeles Salad Co. v. Canadian Food Inspection Agency, [2009] B.C.J. No. 161\n",
      "Fichtner v. Johnston Meier Insurance Services Ltd., [2001] B.C.J. No. 1666\n",
      "Strata Plan NW 3341 v. Canlan Ice Sports Corp., [2001] B.C.J. No. 1723\n",
      "Brooks-Martin v. Martin, [2011] B.C.J. No. 243\n",
      "Mclaren v. Rice, [2009] B.C.J. No. 2108\n",
      "Neidermayer v. Gillies, [2012] B.C.J. No. 183\n",
      "Gibson v. Matthies, [2017] B.C.J. No. 965\n",
      "Brito (Guardian ad litem of) v. Woolley, [2001] B.C.J. No. 1692\n",
      "Ruchelski v. Moore, [2013] B.C.J. No. 561\n",
      "Abbott v. Gerges, [2014] B.C.J. No. 1848\n",
      "N&C Transportation Ltd. v. Navistar International Corp., [2016] B.C.J. No. 2369\n",
      "Harder v. Poettcker, [2015] B.C.J. No. 2579\n",
      "Paniccia v. Eckert, [2012] B.C.J. No. 1997\n",
      "Intrawest Corp. v. Hart, [2002] B.C.J. No. 301\n",
      "Jacobs v. Basil, [2017] B.C.J. No. 1517\n",
      "Johal v. Conron, [2013] B.C.J. No. 2318\n",
      "Van den Hemel v. Kugathasan, [2010] B.C.J. No. 1767\n",
      "Ho v. Lau, [2012] B.C.J. No. 2561\n",
      "Thiessen v. Mutual Life Assurance Co. of Canada, [2001] B.C.J. No. 1849\n",
      "Rackstraw (Litigation guardian of) v. Robertson, [2011] B.C.J. No. 1354\n",
      "Ahlwat v. Green, [2014] B.C.J. No. 2452\n",
      "Mardones v. Toyota Credit Canada Inc., [2008] B.C.J. No. 1217\n",
      "Ellis (Litigation guardian of) v. Duong, [2017] B.C.J. No. 546\n",
      "McGavin v. Talbot, [2017] B.C.J. No. 2439\n",
      "Aberdeen v. Langley (Township), [2007] B.C.J. No. 1515\n",
      "C.H. v. British Columbia, [2003] B.C.J. No. 1706\n",
      "Logeman v. Rossa, [2006] B.C.J. No. 963\n"
     ]
    }
   ],
   "source": [
    "case_info = defaultdict(dict)\n",
    "case_tags = []\n",
    "tag_distribution = Counter()\n",
    "\n",
    "# define context length for use in features\n",
    "context_length = 8\n",
    "features_per_case = []\n",
    "path = \"../data/\"\n",
    "list_of_files = os.listdir(path)\n",
    "\n",
    "#iterate over new annotations\n",
    "for file in list_of_files:\n",
    "    if file == \"ilanas_annotations.txt\" or file == \"ravi_annotations.txt\" or file == 'new_annotations.txt':\n",
    "        with open(path+file) as f:\n",
    "            cases = f.read()\n",
    "        cases = cases.split('End of Document\\n')\n",
    "        for i in range(len(cases)):\n",
    "            case = cases[i]\n",
    "            if len(case) == 0: # Skip empty lines\n",
    "                continue\n",
    "            lines = case.split('\\n')\n",
    "            case_title = lines[0]\n",
    "            case_type = lines[1]\n",
    "\n",
    "            # skip irrelevant cases\n",
    "            if 'R. v.' in case_title or '(Re)' in case_title: # Skip crown cases, Skip (Re) cases\n",
    "                    continue\n",
    "\n",
    "            # Skip client/solicitor cases (not same as plaintiff/defendant)\n",
    "            regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?|[L|l]awyer[s]?))', case)\n",
    "            if regex_client_solicitor:\n",
    "                continue\n",
    "            regex_solicitor_client = re.search(r'(Between.*([L|l]awyer[s]?|[S|s]olicitor[s]?).*([C|c]lient[s]?))', case)\n",
    "            if regex_solicitor_client:\n",
    "                continue\n",
    "\n",
    "            key_words = ['appellant', 'respondent', 'claimant', 'petitioner', 'plaintiff', 'defendant',\n",
    "                'appellants', 'respondents', 'claimants', 'petitioners', 'plaintiffs', 'defendants']\n",
    "            regex_in_matter_of = re.search(r'IN THE MATTER OF .*\\n\\([0-9]+ paras.\\)', case)\n",
    "            if regex_in_matter_of:\n",
    "                remove = True\n",
    "                for key in key_words:\n",
    "                    if key in regex_in_matter_of.group(0).lower().strip():\n",
    "                        remove = False\n",
    "                if remove:\n",
    "                    continue\n",
    "                    \n",
    "            # Make sure we're dealing with a B.C.J. case   \n",
    "            if  case_type: #'British Columbia Judgments' in\n",
    "                summary = summary_tokenize(case)\n",
    "                last_paragraphs = paragraph_tokenize(case)\n",
    "                if last_paragraphs:\n",
    "                    last_paragraphs = \" \".join(last_paragraphs[-10:]).lower()\n",
    "                #remove stopwords, punctuation, and lower case\n",
    "                # only consider summary and last 7 paragraphs\n",
    "                if not summary:\n",
    "                    case = last_paragraphs\n",
    "                else:\n",
    "                    case = summary + last_paragraphs\n",
    "                case = ' '.join([word for word in case.lower().split() if word not in stop_words])\n",
    "                #get tagged values from annotations\n",
    "                values, tags = case2tags(case)\n",
    "#                 case_tags.append(tags)\n",
    "\n",
    "                # save tags, values per case in dictionary for reference\n",
    "                case_info[case_title]['values'] = values\n",
    "                case_info[case_title]['tags'] = tags\n",
    "                tag_distribution.update(tags)\n",
    "                \n",
    "                # get context of tagged values in case\n",
    "                print(case_title)\n",
    "                visited_indices = set()\n",
    "                case_feats = []\n",
    "                for value, tag in zip(values, tags):\n",
    "                    # save features in dictionary\n",
    "                    features = dict()\n",
    "                    \n",
    "                    # temp value is equal to value without $ symbol - useful for regex escape issues\n",
    "                    temp = value.strip()\n",
    "                    if '$' in value:\n",
    "                        temp = value.replace('$', '').strip()\n",
    "                    pattern = re.compile('''<damage type ?= ?['\"](.*?)['\"]>( ?\\$?.*?)<\\/damage>''')#%(tag, temp)\n",
    "                    matches = pattern.finditer(case)\n",
    "                    if len(re.findall('''<damage type ?= ?['\"](.*?)['\"]>(?: ?\\$?.*?)<\\/damage>''', case))<1:#<damage type\\s?=\\s?['|\"]\\s?%s\\s?['|\"]>\\s?\\$?\\s?%s\\s?<\\/damage>\n",
    "                        print(value, temp, tag)\n",
    "                        print(' ' in value)\n",
    "                        continue\n",
    "                    case_tags.append(tag)\n",
    "                    \n",
    "                    for match in matches:\n",
    "                        if match.start() in visited_indices: # dont want to add the same context twice\n",
    "                            continue\n",
    "                        if summary:\n",
    "                            in_summary = match.group(0) in summary.lower()\n",
    "                        else:\n",
    "                            in_summary = False\n",
    "                        if last_paragraphs:\n",
    "                            in_last_paragraphs = match.group(0) in last_paragraphs\n",
    "                        else:\n",
    "                            in_last_paragraphs = False\n",
    "                        start_idx = match.start()\n",
    "                        end_idx = match.end()\n",
    "                        \n",
    "                        # first use nltk sent_tokenize to get sentence on either side of value only\n",
    "                        start_tokenized = sent_tokenize(case[:start_idx])[-1]\n",
    "                        end_tokenized = sent_tokenize(case[end_idx:])[0]\n",
    "                        # remove any other dmg in the other contexts!\n",
    "                        start_matches = pattern.finditer(start_tokenized)\n",
    "                        for s in start_matches:\n",
    "                            start_tokenized = start_tokenized.replace(s.group(0), s.group(2))\n",
    "                        end_matches = pattern.finditer(end_tokenized)\n",
    "                        for e in end_matches:\n",
    "                            end_tokenized = end_tokenized.replace(e.group(0), e.group(2))\n",
    "                        tokens = start_tokenized + \" \" + value + \" \" + end_tokenized #sentence before and after value\n",
    "                        loc = len(start_tokenized.split())\n",
    "                        tokens = tokens.split()\n",
    "                        # there should always be (at least) one match - ideally one...\n",
    "#                         if len(loc) < 1:\n",
    "#                             print(\"WHYY\")\n",
    "#                             print('value in tokens?', any(value == token for token in tokens))\n",
    "#                             print(match.start(), match.end(), len(case))\n",
    "#                             print(case[start_idx-100:end_idx+100])\n",
    "#                             print(tokens)\n",
    "#                             print(value)\n",
    "#                             print('search boundary', search_boundary)\n",
    "#                             continue\n",
    "#                         if len(loc) > 1:\n",
    "#                             print(loc, len(tokens))\n",
    "#                             print(value)\n",
    "#                             print(case[start_idx-100:end_idx+100])\n",
    "# #                             print(tokens[loc[0]-5:loc[0]], tokens[loc[-1]-5:loc[-1]])\n",
    "#                             loc = loc[-1] #if more than one match, choose last one\n",
    "#                         else:\n",
    "#                             loc = loc[0] \n",
    "\n",
    "\n",
    "                        # context before and after of length: context-length    \n",
    "                        if loc - context_length >= 0 and loc + context_length < len(tokens):\n",
    "                            context_before = \" \".join(tokens[loc - context_length:loc+1])\n",
    "                            context_after = \" \".join(tokens[loc+1:loc + context_length + 1])\n",
    "                            context = \" \".join(tokens[loc - context_length:loc + context_length + 1])\n",
    "                        elif loc - context_length < 0 and loc + context_length < len(tokens):\n",
    "                            beg = abs(loc -context_length)\n",
    "                            context_before = \" \".join(tokens[loc-context_length + beg:loc+1])\n",
    "                            context_after =  \" \".join(tokens[loc+1:loc + context_length + 1])\n",
    "                            context = \" \".join(tokens[loc-context_length + beg:loc + context_length + 1])\n",
    "                        elif loc - context_length > 0 and loc + context_length > len(tokens): \n",
    "                            context_before = \" \".join(tokens[loc - context_length:loc+1])\n",
    "                            context_after = \" \".join(tokens[loc +1 : len(tokens)])\n",
    "                            context = \" \".join(tokens[loc - context_length : len(tokens)])\n",
    "\n",
    "        #                 assert any(val in tokens[loc] for val in value)\n",
    "                        count+= 1\n",
    "\n",
    "                        visited_indices.add(match.start())\n",
    "                        # features to engineer...\n",
    "                        features['value'] = value\n",
    "#                         features['context'] = case[start_idx-100:end_idx+100]\n",
    "                        if loc-2 > 0:\n",
    "                            features['prev bigram'] = tokens[loc-2]\n",
    "                        else:\n",
    "                            features['prev bigram'] = ''\n",
    "                        if loc+2 < len(tokens):\n",
    "                            features['next bigram'] = tokens[loc+2]\n",
    "                        else:\n",
    "                            features['next bigram'] = ''\n",
    "                        features['context_before'] = context_before\n",
    "                        features['context_after'] = context_after\n",
    "                        features['context'] = context\n",
    "                        features['float'] = clean_money_amount([temp.replace(' ', '')])\n",
    "                        features['in summary and last paragraphs'] = in_last_paragraphs and in_summary\n",
    "#                         features['start_idx_ratio'] = match.start()/len(case)\n",
    "#                         features['max_value'] = all(features['float'] > clean_money_amount([val.replace('$', \"\")]) for val in values)\n",
    "#                         features['in summary'] = in_summary\n",
    "#                         features['in last paragraphs'] = in_last_paragraphs\n",
    "                        break #only add one match at a time - features need to have same order as values list\n",
    "                    case_feats.append(features)\n",
    "                features_per_case.append(case_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non-pecuniary': ['non', 'pecuniary', 'pain', 'suffering'],\n",
       " 'special': ['special damages',\n",
       "  'housekeeping',\n",
       "  'in trust',\n",
       "  'bill',\n",
       "  'receipt',\n",
       "  'costs'],\n",
       " 'future care': ['future care',\n",
       "  'care',\n",
       "  'massage',\n",
       "  'pysio',\n",
       "  'therapy',\n",
       "  'medical',\n",
       "  'costs'],\n",
       " 'past wage loss': ['past',\n",
       "  'wage',\n",
       "  'loss',\n",
       "  'income',\n",
       "  'work',\n",
       "  'employment',\n",
       "  'previous',\n",
       "  'inability',\n",
       "  'earning capacity',\n",
       "  'earning'],\n",
       " 'total': ['total', 'damages are', 'assessed', 'sum'],\n",
       " 'future wage loss': ['future income',\n",
       "  'future wage',\n",
       "  'loss',\n",
       "  'work',\n",
       "  'employment',\n",
       "  'earning capacity',\n",
       "  'earning'],\n",
       " 'aggravated': ['aggravated'],\n",
       " 'punitive': ['punitive']}"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damages_keywords = dict()\n",
    "damages_keywords['non-pecuniary'] = ['non', 'pecuniary', 'pain', 'suffering']\n",
    "damages_keywords['special'] = ['special damages', 'housekeeping', 'in trust', 'bill', 'receipt', 'costs']\n",
    "damages_keywords['future care'] = ['future care', 'care', 'massage', 'pysio', 'therapy', 'medical', 'costs']\n",
    "damages_keywords['past wage loss'] = ['past', 'wage', 'loss', 'income', 'work', 'employment', 'previous', 'inability', 'earning capacity', 'earning']\n",
    "damages_keywords['total'] = ['total', 'damages are', 'assessed', 'sum']\n",
    "damages_keywords['future wage loss'] = ['future income', 'future wage', 'loss', 'work', 'employment', 'earning capacity', 'earning']\n",
    "damages_keywords['aggravated'] = ['aggravated']\n",
    "damages_keywords['punitive'] = ['punitive']\n",
    "damages_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_info['Gray v. Ellis, [2007] I.L.R. para. M-2118']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "===\n",
      "84\n",
      "84\n",
      "===\n",
      "4\n",
      "4\n",
      "===\n",
      "9\n",
      "9\n",
      "===\n",
      "32\n",
      "32\n",
      "===\n",
      "20\n",
      "20\n",
      "===\n",
      "19\n",
      "19\n",
      "===\n",
      "123\n",
      "123\n",
      "===\n",
      "42\n",
      "42\n",
      "===\n",
      "16\n",
      "16\n",
      "===\n",
      "57\n",
      "57\n",
      "===\n",
      "199\n",
      "199\n",
      "===\n",
      "20\n",
      "20\n",
      "===\n",
      "13\n",
      "13\n",
      "===\n",
      "56\n",
      "56\n",
      "===\n",
      "39\n",
      "39\n",
      "===\n",
      "10\n",
      "10\n",
      "===\n",
      "50\n",
      "50\n",
      "===\n",
      "48\n",
      "48\n",
      "===\n",
      "45\n",
      "45\n",
      "===\n",
      "49\n",
      "49\n",
      "===\n",
      "83\n",
      "83\n",
      "===\n",
      "39\n",
      "39\n",
      "===\n",
      "11\n",
      "11\n",
      "===\n",
      "12\n",
      "12\n",
      "===\n",
      "14\n",
      "14\n",
      "===\n",
      "37\n",
      "37\n",
      "===\n",
      "17\n",
      "17\n",
      "===\n",
      "47\n",
      "47\n",
      "===\n",
      "53\n",
      "53\n",
      "===\n",
      "82\n",
      "82\n",
      "===\n",
      "31\n",
      "31\n",
      "===\n",
      "39\n",
      "39\n",
      "===\n",
      "45\n",
      "45\n",
      "===\n",
      "8\n",
      "8\n",
      "===\n",
      "5\n",
      "5\n",
      "===\n",
      "123\n",
      "123\n",
      "===\n",
      "16\n",
      "16\n",
      "===\n",
      "3\n",
      "3\n",
      "===\n",
      "47\n",
      "47\n",
      "===\n",
      "12\n",
      "12\n",
      "===\n",
      "23\n",
      "23\n",
      "===\n",
      "47\n",
      "47\n",
      "===\n",
      "40\n",
      "40\n",
      "===\n",
      "76\n",
      "76\n",
      "===\n",
      "14\n",
      "14\n",
      "===\n",
      "63\n",
      "63\n",
      "===\n",
      "65\n",
      "65\n",
      "===\n",
      "45\n",
      "45\n",
      "===\n",
      "30\n",
      "30\n",
      "===\n",
      "2\n",
      "2\n",
      "===\n",
      "27\n",
      "27\n",
      "===\n",
      "33\n",
      "33\n",
      "===\n",
      "51\n",
      "51\n",
      "===\n",
      "22\n",
      "22\n",
      "===\n",
      "26\n",
      "26\n",
      "===\n",
      "29\n",
      "29\n",
      "===\n",
      "49\n",
      "49\n",
      "===\n",
      "43\n",
      "43\n",
      "===\n",
      "35\n",
      "35\n",
      "===\n",
      "92\n",
      "92\n",
      "===\n",
      "29\n",
      "29\n",
      "===\n",
      "17\n",
      "17\n",
      "===\n",
      "46\n",
      "46\n",
      "===\n",
      "4\n",
      "4\n",
      "===\n",
      "87\n",
      "87\n",
      "===\n",
      "72\n",
      "72\n",
      "===\n",
      "40\n",
      "40\n",
      "===\n",
      "77\n",
      "77\n",
      "===\n",
      "52\n",
      "52\n",
      "===\n",
      "64\n",
      "64\n",
      "===\n",
      "12\n",
      "12\n",
      "===\n",
      "14\n",
      "14\n",
      "===\n",
      "65\n",
      "65\n",
      "===\n",
      "50\n",
      "50\n",
      "===\n",
      "1\n",
      "1\n",
      "===\n",
      "52\n",
      "52\n",
      "===\n",
      "2\n",
      "2\n",
      "===\n",
      "12\n",
      "12\n",
      "===\n",
      "37\n",
      "37\n",
      "===\n",
      "90\n",
      "90\n",
      "===\n",
      "23\n",
      "23\n",
      "===\n",
      "60\n",
      "60\n",
      "===\n",
      "14\n",
      "14\n",
      "===\n",
      "27\n",
      "27\n",
      "===\n",
      "38\n",
      "38\n",
      "===\n",
      "4\n",
      "4\n",
      "===\n",
      "2\n",
      "2\n",
      "===\n",
      "17\n",
      "17\n",
      "===\n",
      "8\n",
      "8\n",
      "===\n",
      "38\n",
      "38\n",
      "===\n",
      "153\n",
      "153\n",
      "===\n",
      "63\n",
      "63\n",
      "===\n",
      "54\n",
      "54\n",
      "===\n",
      "2\n",
      "2\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "21\n",
      "21\n",
      "===\n",
      "31\n",
      "31\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "41\n",
      "41\n",
      "===\n",
      "45\n",
      "45\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "18\n",
      "18\n",
      "===\n",
      "7\n",
      "7\n",
      "===\n",
      "54\n",
      "54\n",
      "===\n",
      "55\n",
      "55\n",
      "===\n",
      "84\n",
      "84\n",
      "===\n",
      "38\n",
      "38\n",
      "===\n",
      "6\n",
      "6\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(features_per_case)):\n",
    "    d = features_per_case[i]\n",
    "    d2 = case_tags[i]\n",
    "    if d ==[]:\n",
    "        continue\n",
    "    print(len(d))\n",
    "    print(len(d2))\n",
    "    print('===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': '$75,000', 'prev bigram': 'non-pecuniary', 'next bigram': '$4,800', 'context_before': 'work en diminished — plaintiff awarded non-pecuniary damages $75,000', 'context_after': ', $4,800 past income loss, $60,000 loss future', 'context': 'work en diminished — plaintiff awarded non-pecuniary damages $75,000 , $4,800 past income loss, $60,000 loss future', 'float': 75000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$4,800', 'prev bigram': 'damages', 'next bigram': 'income', 'context_before': 'en diminished — plaintiff awarded non-pecuniary damages $75,000, $4,800', 'context_after': 'past income loss, $60,000 loss future earning capacity,', 'context': 'en diminished — plaintiff awarded non-pecuniary damages $75,000, $4,800 past income loss, $60,000 loss future earning capacity,', 'float': 4800.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$60,000', 'prev bigram': 'income', 'next bigram': 'future', 'context_before': 'awarded non-pecuniary damages $75,000, $4,800 past income loss, $60,000', 'context_after': 'loss future earning capacity, $16,274 cost future care', 'context': 'awarded non-pecuniary damages $75,000, $4,800 past income loss, $60,000 loss future earning capacity, $16,274 cost future care', 'float': 60000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$16,274', 'prev bigram': 'earning', 'next bigram': 'future', 'context_before': 'past income loss, $60,000 loss future earning capacity, $16,274', 'context_after': 'cost future care $746 special damages.', 'context': 'past income loss, $60,000 loss future earning capacity, $16,274 cost future care $746 special damages.', 'float': 16274.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$746', 'prev bigram': 'future', 'next bigram': 'damages.', 'context_before': 'loss future earning capacity, $16,274 cost future care $746', 'context_after': 'special damages.', 'context': 'loss future earning capacity, $16,274 cost future care $746 special damages.', 'float': 746.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$75,000', 'prev bigram': 'non-pecuniary', 'next bigram': '$4,800', 'context_before': 'ability work diminished — plaintiff awarded non-pecuniary damages $75,000', 'context_after': ', $4,800 past income loss, $60,000 loss future', 'context': 'ability work diminished — plaintiff awarded non-pecuniary damages $75,000 , $4,800 past income loss, $60,000 loss future', 'float': 75000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$4,800', 'prev bigram': 'damages', 'next bigram': 'income', 'context_before': 'work diminished — plaintiff awarded non-pecuniary damages $75,000, $4,800', 'context_after': 'past income loss, $60,000 loss future earning capacity,', 'context': 'work diminished — plaintiff awarded non-pecuniary damages $75,000, $4,800 past income loss, $60,000 loss future earning capacity,', 'float': 4800.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$60,000', 'prev bigram': 'income', 'next bigram': 'future', 'context_before': 'awarded non-pecuniary damages $75,000, $4,800 past income loss, $60,000', 'context_after': 'loss future earning capacity, $16,274 cost future care', 'context': 'awarded non-pecuniary damages $75,000, $4,800 past income loss, $60,000 loss future earning capacity, $16,274 cost future care', 'float': 60000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$16,274', 'prev bigram': 'earning', 'next bigram': 'future', 'context_before': 'past income loss, $60,000 loss future earning capacity, $16,274', 'context_after': 'cost future care $746 special damages.', 'context': 'past income loss, $60,000 loss future earning capacity, $16,274 cost future care $746 special damages.', 'float': 16274.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$746', 'prev bigram': 'future', 'next bigram': 'damages.', 'context_before': 'loss future earning capacity, $16,274 cost future care $746', 'context_after': 'special damages.', 'context': 'loss future earning capacity, $16,274 cost future care $746 special damages.', 'float': 746.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$45,000', 'prev bigram': 'claimed', 'next bigram': 'pain', 'context_before': 'plaintiff claimed damages $45,000', 'context_after': '$85,000 pain suffering, $44,760 past income loss, $150,000', 'context': 'plaintiff claimed damages $45,000 $85,000 pain suffering, $44,760 past income loss, $150,000', 'float': 45000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$85,000', 'prev bigram': 'damages', 'next bigram': 'suffering,', 'context_before': 'plaintiff claimed damages $45,000 $85,000', 'context_after': 'pain suffering, $44,760 past income loss, $150,000 $300,000', 'context': 'plaintiff claimed damages $45,000 $85,000 pain suffering, $44,760 past income loss, $150,000 $300,000', 'float': 85000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$44,760', 'prev bigram': 'pain', 'next bigram': 'income', 'context_before': 'plaintiff claimed damages $45,000 $85,000 pain suffering, $44,760', 'context_after': 'past income loss, $150,000 $300,000 loss future earning', 'context': 'plaintiff claimed damages $45,000 $85,000 pain suffering, $44,760 past income loss, $150,000 $300,000 loss future earning', 'float': 44760.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$150,000', 'prev bigram': 'income', 'next bigram': 'loss', 'context_before': '$45,000 $85,000 pain suffering, $44,760 past income loss, $150,000', 'context_after': '$300,000 loss future earning capacity, $23,304 cost future', 'context': '$45,000 $85,000 pain suffering, $44,760 past income loss, $150,000 $300,000 loss future earning capacity, $23,304 cost future', 'float': 150000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$300,000', 'prev bigram': 'loss,', 'next bigram': 'future', 'context_before': '$85,000 pain suffering, $44,760 past income loss, $150,000 $300,000', 'context_after': 'loss future earning capacity, $23,304 cost future care', 'context': '$85,000 pain suffering, $44,760 past income loss, $150,000 $300,000 loss future earning capacity, $23,304 cost future care', 'float': 300000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$23,304', 'prev bigram': 'earning', 'next bigram': 'future', 'context_before': 'income loss, $150,000 $300,000 loss future earning capacity, $23,304', 'context_after': 'cost future care special damages $746 past cost', 'context': 'income loss, $150,000 $300,000 loss future earning capacity, $23,304 cost future care special damages $746 past cost', 'float': 23304.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$746', 'prev bigram': 'special', 'next bigram': 'cost', 'context_before': 'earning capacity, $23,304 cost future care special damages $746', 'context_after': 'past cost care.', 'context': 'earning capacity, $23,304 cost future care special damages $746 past cost care.', 'float': 746.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$156,820 ', 'prev bigram': 'total', 'next bigram': '', 'context_before': 'earning capacity, $23,304 cost future care special damages $746', 'context_after': 'past cost care.', 'context': 'earning capacity, $23,304 cost future care special damages $746 past cost care.', 'float': 156820.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$75,000', 'prev bigram': 'non-pecuniary', 'next bigram': '$4,800', 'context_before': 'plaintiff awarded non-pecuniary damages $75,000', 'context_after': '; $4,800 damages past income loss, $60,000 loss', 'context': 'plaintiff awarded non-pecuniary damages $75,000 ; $4,800 damages past income loss, $60,000 loss', 'float': 75000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$4,800', 'prev bigram': 'damages', 'next bigram': 'past', 'context_before': 'plaintiff awarded non-pecuniary damages $75,000; $4,800', 'context_after': 'damages past income loss, $60,000 loss future earning', 'context': 'plaintiff awarded non-pecuniary damages $75,000; $4,800 damages past income loss, $60,000 loss future earning', 'float': 4800.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$60,000', 'prev bigram': 'income', 'next bigram': 'future', 'context_before': 'non-pecuniary damages $75,000; $4,800 damages past income loss, $60,000', 'context_after': 'loss future earning capacity, $16,274 cost future care', 'context': 'non-pecuniary damages $75,000; $4,800 damages past income loss, $60,000 loss future earning capacity, $16,274 cost future care', 'float': 60000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$16,274', 'prev bigram': 'earning', 'next bigram': 'future', 'context_before': 'past income loss, $60,000 loss future earning capacity, $16,274', 'context_after': 'cost future care $746 special damages.', 'context': 'past income loss, $60,000 loss future earning capacity, $16,274 cost future care $746 special damages.', 'float': 16274.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$746', 'prev bigram': 'future', 'next bigram': 'damages.', 'context_before': 'loss future earning capacity, $16,274 cost future care $746', 'context_after': 'special damages.', 'context': 'loss future earning capacity, $16,274 cost future care $746 special damages.', 'float': 746.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$47,381', 'prev bigram': 'installation', 'next bigram': '', 'context_before': 'reported gross business income hardwood floor installation business $47,381', 'context_after': '.', 'context': 'reported gross business income hardwood floor installation business $47,381 .', 'float': 47381.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$36,390', 'prev bigram': 'business', 'next bigram': '', 'context_before': 'reported gross business income hardwood floor installation business $47,381', 'context_after': '.', 'context': 'reported gross business income hardwood floor installation business $47,381 .', 'float': 36390.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$39,522', 'prev bigram': 'business', 'next bigram': '', 'context_before': 'reported gross business income hardwood floor installation business $47,381', 'context_after': '.', 'context': 'reported gross business income hardwood floor installation business $47,381 .', 'float': 39522.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$40,959', 'prev bigram': 'business', 'next bigram': '', 'context_before': 'reported gross business income hardwood floor installation business $47,381', 'context_after': '.', 'context': 'reported gross business income hardwood floor installation business $47,381 .', 'float': 40959.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$2,500', 'prev bigram': 'vehicle', 'next bigram': 'reduced', 'context_before': 'although repairs vehicle approximately $2,500', 'context_after': ', reduced amount due fact second-hand parts used', 'context': 'although repairs vehicle approximately $2,500 , reduced amount due fact second-hand parts used', 'float': 2500.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$39,955', 'prev bigram': 'business', 'next bigram': 'flooring', 'context_before': '2005 tax return discloses reported gross business income $39,955', 'context_after': 'hardwood flooring installation business.', 'context': '2005 tax return discloses reported gross business income $39,955 hardwood flooring installation business.', 'float': 39955.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$44,432', 'prev bigram': 'business', 'next bigram': '', 'context_before': '2005 tax return discloses reported gross business income $39,955', 'context_after': 'hardwood flooring installation business.', 'context': '2005 tax return discloses reported gross business income $39,955 hardwood flooring installation business.', 'float': 44432.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$50,054.61', 'prev bigram': 'business', 'next bigram': '', 'context_before': '2007 tax return discloses reported gross business income $50,054.61', 'context_after': '.', 'context': '2007 tax return discloses reported gross business income $50,054.61 .', 'float': 50054.61, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$45,000', 'prev bigram': 'damages', 'next bigram': 'past', 'context_before': 'mr. szymanski seeks following damages: non-pecuniary damages = $45,000', 'context_after': '$85,000 past loss income = $44,760 loss future', 'context': 'mr. szymanski seeks following damages: non-pecuniary damages = $45,000 $85,000 past loss income = $44,760 loss future', 'float': 45000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$85,000', 'prev bigram': '=', 'next bigram': 'loss', 'context_before': 'szymanski seeks following damages: non-pecuniary damages = $45,000 $85,000', 'context_after': 'past loss income = $44,760 loss future earning', 'context': 'szymanski seeks following damages: non-pecuniary damages = $45,000 $85,000 past loss income = $44,760 loss future earning', 'float': 85000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$44,760', 'prev bigram': 'income', 'next bigram': 'future', 'context_before': 'damages = $45,000 $85,000 past loss income = $44,760', 'context_after': 'loss future earning capacity = $150,000 $300,000 cost', 'context': 'damages = $45,000 $85,000 past loss income = $44,760 loss future earning capacity = $150,000 $300,000 cost', 'float': 44760.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$150,000', 'prev bigram': 'capacity', 'next bigram': 'cost', 'context_before': 'income = $44,760 loss future earning capacity = $150,000', 'context_after': '$300,000 cost future care = $23,304 special damages', 'context': 'income = $44,760 loss future earning capacity = $150,000 $300,000 cost future care = $23,304 special damages', 'float': 150000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$300,000', 'prev bigram': '=', 'next bigram': 'future', 'context_before': '= $44,760 loss future earning capacity = $150,000 $300,000', 'context_after': 'cost future care = $23,304 special damages =', 'context': '= $44,760 loss future earning capacity = $150,000 $300,000 cost future care = $23,304 special damages =', 'float': 300000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$23,304', 'prev bigram': 'care', 'next bigram': 'damages', 'context_before': 'capacity = $150,000 $300,000 cost future care = $23,304', 'context_after': 'special damages = $746 1. non-pecuniary damages 136', 'context': 'capacity = $150,000 $300,000 cost future care = $23,304 special damages = $746 1. non-pecuniary damages 136', 'float': 23304.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$746', 'prev bigram': 'damages', 'next bigram': 'non-pecuniary', 'context_before': 'cost future care = $23,304 special damages = $746', 'context_after': '1. non-pecuniary damages 136 non-pecuniary damages awarded compensate', 'context': 'cost future care = $23,304 special damages = $746 1. non-pecuniary damages 136 non-pecuniary damages awarded compensate', 'float': 746.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$45,000', 'prev bigram': 'seeks', 'next bigram': 'non-pecuniary', 'context_before': 'cost future care = $23,304 special damages = $746', 'context_after': '1. non-pecuniary damages 136 non-pecuniary damages awarded compensate', 'context': 'cost future care = $23,304 special damages = $746 1. non-pecuniary damages 136 non-pecuniary damages awarded compensate', 'float': 45000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$85,000', 'prev bigram': 'award', 'next bigram': 'damages.', 'context_before': 'cost future care = $23,304 special damages = $746', 'context_after': '1. non-pecuniary damages 136 non-pecuniary damages awarded compensate', 'context': 'cost future care = $23,304 special damages = $746 1. non-pecuniary damages 136 non-pecuniary damages awarded compensate', 'float': 85000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$65,000', 'prev bigram': 'kuskis', 'next bigram': 'heppner', 'context_before': 'relies upon following cases support position: kuskis ( $65,000', 'context_after': '); heppner v. zia, ($75,000); klein v. dowhy,', 'context': 'relies upon following cases support position: kuskis ( $65,000 ); heppner v. zia, ($75,000); klein v. dowhy,', 'float': 65000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$75,000', 'prev bigram': 'zia,', 'next bigram': 'klein', 'context_before': 'support position: kuskis ($65,000); heppner v. zia, ( $75,000', 'context_after': '); klein v. dowhy, ($75,000) [klein]; stone v.', 'context': 'support position: kuskis ($65,000); heppner v. zia, ( $75,000 ); klein v. dowhy, ($75,000) [klein]; stone v.', 'float': 75000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$75,000', 'prev bigram': 'dowhy,', 'next bigram': '[klein];', 'context_before': 'heppner v. zia, ($75,000); klein v. dowhy, ( $75,000', 'context_after': ') [klein]; stone v. ellerman, ($100,000); love v.', 'context': 'heppner v. zia, ($75,000); klein v. dowhy, ( $75,000 ) [klein]; stone v. ellerman, ($100,000); love v.', 'float': 75000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$100,000', 'prev bigram': 'ellerman,', 'next bigram': 'love', 'context_before': 'v. dowhy, ($75,000) [klein]; stone v. ellerman, ( $100,000', 'context_after': '); love v. pai, ($100,000).', 'context': 'v. dowhy, ($75,000) [klein]; stone v. ellerman, ( $100,000 ); love v. pai, ($100,000).', 'float': 100000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$100,000', 'prev bigram': 'pai,', 'next bigram': '', 'context_before': 'stone v. ellerman, ($100,000); love v. pai, ( $100,000', 'context_after': ').', 'context': 'stone v. ellerman, ($100,000); love v. pai, ( $100,000 ).', 'float': 100000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$10,000', 'prev bigram': 'damages', 'next bigram': '', 'context_before': 'june 2007 therefore appropriate award non-pecuniary damages between $10,000', 'context_after': '$15,000.', 'context': 'june 2007 therefore appropriate award non-pecuniary damages between $10,000 $15,000.', 'float': 10000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$15,000', 'prev bigram': 'damages', 'next bigram': '', 'context_before': 'june 2007 therefore appropriate award non-pecuniary damages between$10,000 $15,000', 'context_after': '.', 'context': 'june 2007 therefore appropriate award non-pecuniary damages between$10,000 $15,000 .', 'float': 15000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$15,000', 'prev bigram': 'johal,', 'next bigram': 'hall', 'context_before': 'rely following cases support: brar v. johal, ( $15,000', 'context_after': '); hall v. day, ($10,000); kovacevic v. leischner,', 'context': 'rely following cases support: brar v. johal, ( $15,000 ); hall v. day, ($10,000); kovacevic v. leischner,', 'float': 15000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$10,000', 'prev bigram': 'day,', 'next bigram': 'kovacevic', 'context_before': 'brar v. johal, ($15,000); hall v. day, ( $10,000', 'context_after': '); kovacevic v. leischner, ($12,000); machala v. roodenburg,', 'context': 'brar v. johal, ($15,000); hall v. day, ( $10,000 ); kovacevic v. leischner, ($12,000); machala v. roodenburg,', 'float': 10000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$12,000', 'prev bigram': 'leischner,', 'next bigram': 'machala', 'context_before': 'hall v. day, ($10,000); kovacevic v. leischner, ( $12,000', 'context_after': '); machala v. roodenburg, ($11,000); mann v. klassen', 'context': 'hall v. day, ($10,000); kovacevic v. leischner, ( $12,000 ); machala v. roodenburg, ($11,000); mann v. klassen', 'float': 12000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$11,000', 'prev bigram': 'roodenburg,', 'next bigram': 'mann', 'context_before': 'kovacevic v. leischner, ($12,000); machala v. roodenburg, ( $11,000', 'context_after': '); mann v. klassen et al., ($10,000); liao', 'context': 'kovacevic v. leischner, ($12,000); machala v. roodenburg, ( $11,000 ); mann v. klassen et al., ($10,000); liao', 'float': 11000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$10,000', 'prev bigram': 'al.,', 'next bigram': 'liao', 'context_before': 'roodenburg, ($11,000); mann v. klassen et al., ( $10,000', 'context_after': '); liao v. doe et al., ($10,000); al-mandlawi', 'context': 'roodenburg, ($11,000); mann v. klassen et al., ( $10,000 ); liao v. doe et al., ($10,000); al-mandlawi', 'float': 10000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$10,000', 'prev bigram': 'al.,', 'next bigram': 'al-mandlawi', 'context_before': 'al., ($10,000); liao v. doe et al., ( $10,000', 'context_after': '); al-mandlawi v. gara et al., ($7,500); pennykid', 'context': 'al., ($10,000); liao v. doe et al., ( $10,000 ); al-mandlawi v. gara et al., ($7,500); pennykid', 'float': 10000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$7,500', 'prev bigram': 'al.,', 'next bigram': 'pennykid', 'context_before': 'al., ($10,000); al-mandlawi v. gara et al., ( $7,500', 'context_after': '); pennykid v. escribano, ($14,000); darji v. regimbald,', 'context': 'al., ($10,000); al-mandlawi v. gara et al., ( $7,500 ); pennykid v. escribano, ($14,000); darji v. regimbald,', 'float': 7500.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$14,000', 'prev bigram': 'escribano,', 'next bigram': 'darji', 'context_before': 'gara et al., ($7,500); pennykid v. escribano, ( $14,000', 'context_after': '); darji v. regimbald, ($12,000); read v. marques,', 'context': 'gara et al., ($7,500); pennykid v. escribano, ( $14,000 ); darji v. regimbald, ($12,000); read v. marques,', 'float': 14000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$12,000', 'prev bigram': 'regimbald,', 'next bigram': 'read', 'context_before': 'pennykid v. escribano, ($14,000); darji v. regimbald, ( $12,000', 'context_after': '); read v. marques, ($13,500); kumar v. canada', 'context': 'pennykid v. escribano, ($14,000); darji v. regimbald, ( $12,000 ); read v. marques, ($13,500); kumar v. canada', 'float': 12000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$13,500', 'prev bigram': 'marques,', 'next bigram': 'kumar', 'context_before': 'darji v. regimbald, ($12,000); read v. marques, ( $13,500', 'context_after': '); kumar v. canada post corporation nixon, ($12,000).', 'context': 'darji v. regimbald, ($12,000); read v. marques, ( $13,500 ); kumar v. canada post corporation nixon, ($12,000).', 'float': 13500.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$12,000', 'prev bigram': 'nixon,', 'next bigram': '', 'context_before': '($13,500); kumar v. canada post corporation nixon, ( $12,000', 'context_after': ').', 'context': '($13,500); kumar v. canada post corporation nixon, ( $12,000 ).', 'float': 12000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$75,000', 'prev bigram': 'damages', 'next bigram': '', 'context_before': 'fact suffered diminishment lifestyle, assess non-pecuniary damages amount $75,000', 'context_after': '.', 'context': 'fact suffered diminishment lifestyle, assess non-pecuniary damages amount $75,000 .', 'float': 75000.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$15,000', 'prev bigram': 'capacity\"', 'next bigram': 'acknowledged', 'context_before': 'argued also receive damages \"loss housekeeping capacity\" amount $15,000', 'context_after': '$35,000 acknowledged could also reflected amount general damages', 'context': 'argued also receive damages \"loss housekeeping capacity\" amount $15,000 $35,000 acknowledged could also reflected amount general damages', 'float': 15000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$35,000', 'prev bigram': 'amount', 'next bigram': 'could', 'context_before': 'also receive damages \"loss housekeeping capacity\" amount $15,000 $35,000', 'context_after': 'acknowledged could also reflected amount general damages awarded', 'context': 'also receive damages \"loss housekeeping capacity\" amount $15,000 $35,000 acknowledged could also reflected amount general damages awarded', 'float': 35000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$30', 'prev bigram': 'wage', 'next bigram': 'counsel', 'context_before': 'based hourly wage rate $30', 'context_after': ', counsel argues mr. szymanski lost $1,200 month', 'context': 'based hourly wage rate $30 , counsel argues mr. szymanski lost $1,200 month', 'float': 30.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$1,200', 'prev bigram': 'szymanski', 'next bigram': 'gross', 'context_before': 'wage rate $30, counsel argues mr. szymanski lost $1,200', 'context_after': 'month gross salary due injuries accident total approximately', 'context': 'wage rate $30, counsel argues mr. szymanski lost $1,200 month gross salary due injuries accident total approximately', 'float': 1200.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$44,760', 'prev bigram': 'total', 'next bigram': '', 'context_before': 'month gross salary due injuries accident total approximately $44,760', 'context_after': '.', 'context': 'month gross salary due injuries accident total approximately $44,760 .', 'float': 44760.0, 'in summary and last paragraphs': True}\n",
      "===\n",
      "{'value': '$24,468', 'prev bigram': 'net', 'next bigram': '2000', 'context_before': 'tax returns 1999 2007. year gross net 1999 $24,468', 'context_after': '$5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002', 'context': 'tax returns 1999 2007. year gross net 1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002', 'float': 24468.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$5,900', 'prev bigram': '1999', 'next bigram': '$29,557', 'context_before': 'returns 1999 2007. year gross net 1999 $24,468 $5,900', 'context_after': '2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522', 'context': 'returns 1999 2007. year gross net 1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522', 'float': 5900.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$29,557', 'prev bigram': '$5,900', 'next bigram': '2001', 'context_before': '2007. year gross net 1999 $24,468 $5,900 2000 $29,557', 'context_after': '$5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003', 'context': '2007. year gross net 1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003', 'float': 29557.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$5,000', 'prev bigram': '2000', 'next bigram': '$36,390', 'context_before': 'year gross net 1999 $24,468 $5,900 2000 $29,557 $5,000', 'context_after': '2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959', 'context': 'year gross net 1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959', 'float': 5000.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$36,390', 'prev bigram': '$5,000', 'next bigram': '2002', 'context_before': 'net 1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390', 'context_after': '$7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004', 'context': 'net 1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004', 'float': 36390.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$7,381', 'prev bigram': '2001', 'next bigram': '$39,522', 'context_before': '1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390 $7,381', 'context_after': '2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381', 'context': '1999 $24,468 $5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381', 'float': 7381.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$39,522', 'prev bigram': '$7,381', 'next bigram': '2003', 'context_before': '$5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522', 'context_after': '$9,726 2003 $40,959 $9,665 2004 $47,381 $12,872 (accident', 'context': '$5,900 2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381 $12,872 (accident', 'float': 39522.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$9,726', 'prev bigram': '2002', 'next bigram': '$40,959', 'context_before': '2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522 $9,726', 'context_after': '2003 $40,959 $9,665 2004 $47,381 $12,872 (accident december', 'context': '2000 $29,557 $5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381 $12,872 (accident december', 'float': 9726.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$40,959', 'prev bigram': '$9,726', 'next bigram': '2004', 'context_before': '$5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959', 'context_after': '$9,665 2004 $47,381 $12,872 (accident december 7, 2004)', 'context': '$5,000 2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381 $12,872 (accident december 7, 2004)', 'float': 40959.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$9,665', 'prev bigram': '2003', 'next bigram': '$47,381', 'context_before': '2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959 $9,665', 'context_after': '2004 $47,381 $12,872 (accident december 7, 2004) 2005', 'context': '2001 $36,390 $7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381 $12,872 (accident december 7, 2004) 2005', 'float': 9665.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$47,381', 'prev bigram': '$9,665', 'next bigram': '(accident', 'context_before': '$7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381', 'context_after': '$12,872 (accident december 7, 2004) 2005 $39,955 $12,496', 'context': '$7,381 2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381 $12,872 (accident december 7, 2004) 2005 $39,955 $12,496', 'float': 47381.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$12,872', 'prev bigram': '2004', 'next bigram': 'december', 'context_before': '2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381 $12,872', 'context_after': '(accident december 7, 2004) 2005 $39,955 $12,496 2006', 'context': '2002 $39,522 $9,726 2003 $40,959 $9,665 2004 $47,381 $12,872 (accident december 7, 2004) 2005 $39,955 $12,496 2006', 'float': 12872.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$39,955', 'prev bigram': '2004)', 'next bigram': '2006', 'context_before': '2004 $47,381 $12,872 (accident december 7, 2004) 2005 $39,955', 'context_after': '$12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151', 'context': '2004 $47,381 $12,872 (accident december 7, 2004) 2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151', 'float': 39955.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$12,496', 'prev bigram': '2005', 'next bigram': '$44,432', 'context_before': '$47,381 $12,872 (accident december 7, 2004) 2005 $39,955 $12,496', 'context_after': '2006 $44,432 $19,288 2007 $47,223 $21,364 151 mr.', 'context': '$47,381 $12,872 (accident december 7, 2004) 2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151 mr.', 'float': 12496.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$44,432', 'prev bigram': '$12,496', 'next bigram': '2007', 'context_before': '(accident december 7, 2004) 2005 $39,955 $12,496 2006 $44,432', 'context_after': '$19,288 2007 $47,223 $21,364 151 mr. szymanski simply', 'context': '(accident december 7, 2004) 2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151 mr. szymanski simply', 'float': 44432.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$19,288', 'prev bigram': '2006', 'next bigram': '$47,223', 'context_before': 'december 7, 2004) 2005 $39,955 $12,496 2006 $44,432 $19,288', 'context_after': '2007 $47,223 $21,364 151 mr. szymanski simply provided', 'context': 'december 7, 2004) 2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151 mr. szymanski simply provided', 'float': 19288.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$47,223', 'prev bigram': '$19,288', 'next bigram': '151', 'context_before': '2004) 2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223', 'context_after': '$21,364 151 mr. szymanski simply provided income tax', 'context': '2004) 2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151 mr. szymanski simply provided income tax', 'float': 47223.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$21,364', 'prev bigram': '2007', 'next bigram': 'mr.', 'context_before': '2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364', 'context_after': '151 mr. szymanski simply provided income tax information', 'context': '2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151 mr. szymanski simply provided income tax information', 'float': 21364.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$376', 'prev bigram': '2005', 'next bigram': '', 'context_before': '2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364', 'context_after': '151 mr. szymanski simply provided income tax information', 'context': '2005 $39,955 $12,496 2006 $44,432 $19,288 2007 $47,223 $21,364 151 mr. szymanski simply provided income tax information', 'float': 376.0, 'in summary and last paragraphs': False}\n",
      "===\n",
      "{'value': '$6,416', 'prev bigram': 'income', 'next bigram': 'pre-accident', 'context_before': \"154 2006, mr. szymanski's net income increased $6,416\", 'context_after': \"2004 pre-accident income damage type = 'other'>$6,792</damage> 2005\", 'context': \"154 2006, mr. szymanski's net income increased $6,416 2004 pre-accident income damage type = 'other'>$6,792</damage> 2005\", 'float': 6416.0, 'in summary and last paragraphs': False}\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(features_per_case[1])):\n",
    "    d = features_per_case[1][i]\n",
    "#     v = case_tags[i]\n",
    "    print(d)\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# turn list of lists into one-dimenion\n",
    "# tags = list(chain.from_iterable(case_tags))\n",
    "feats = list(chain.from_iterable(features_per_case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "vectorizer = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(30)\n",
    "from sklearn.model_selection import train_test_split #cross validation instead\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     feats, case_tags, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.11077074289616495, micro: 0.42712110224800587 f-scores\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('macro: %s, micro: %s f-scores' %(f1_score(y_test, y_pred, average = 'macro'), f1_score(y_test, y_pred, average= 'micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4271211022480058"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilana/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aggravated': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 2},\n",
       " 'future care': {'precision': 0.19745222929936307,\n",
       "  'recall': 0.5961538461538461,\n",
       "  'f1-score': 0.2966507177033493,\n",
       "  'support': 52},\n",
       " 'future wage loss': {'precision': 0.09090909090909091,\n",
       "  'recall': 0.07407407407407407,\n",
       "  'f1-score': 0.08163265306122448,\n",
       "  'support': 54},\n",
       " 'general': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 14},\n",
       " 'non pecuniary': {'precision': 0.09433962264150944,\n",
       "  'recall': 0.12048192771084337,\n",
       "  'f1-score': 0.10582010582010581,\n",
       "  'support': 83},\n",
       " 'other': {'precision': 0.7522935779816514,\n",
       "  'recall': 0.6172043010752688,\n",
       "  'f1-score': 0.6780862374483166,\n",
       "  'support': 930},\n",
       " 'past wage loss': {'precision': 0.28125,\n",
       "  'recall': 0.20930232558139536,\n",
       "  'f1-score': 0.24,\n",
       "  'support': 43},\n",
       " 'punitive': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 6},\n",
       " 'reduction': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 9},\n",
       " 'special': {'precision': 0.07291666666666667,\n",
       "  'recall': 0.1044776119402985,\n",
       "  'f1-score': 0.08588957055214724,\n",
       "  'support': 67},\n",
       " 'sub-future care': {'precision': 0.5,\n",
       "  'recall': 0.1111111111111111,\n",
       "  'f1-score': 0.1818181818181818,\n",
       "  'support': 18},\n",
       " 'sub-future wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 9},\n",
       " 'sub-general': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 4},\n",
       " 'sub-non pecuniary': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 4},\n",
       " 'sub-past wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 5},\n",
       " 'sub-special': {'precision': 0.06896551724137931,\n",
       "  'recall': 0.15384615384615385,\n",
       "  'f1-score': 0.09523809523809525,\n",
       "  'support': 26},\n",
       " 'sub-total': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5},\n",
       " 'total': {'precision': 0.208955223880597,\n",
       "  'recall': 0.3333333333333333,\n",
       "  'f1-score': 0.2568807339449541,\n",
       "  'support': 42},\n",
       " 'total after': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 6},\n",
       " 'accuracy': 0.47715736040609136,\n",
       " 'macro avg': {'precision': 0.1719516804536978,\n",
       "  'recall': 0.14842024656980654,\n",
       "  'f1-score': 0.14150962959226532,\n",
       "  'support': 1379},\n",
       " 'weighted avg': {'precision': 0.5548863035190474,\n",
       "  'recall': 0.47715736040609136,\n",
       "  'f1-score': 0.5046046265391466,\n",
       "  'support': 1379}}"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilana/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aggravated': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 1},\n",
       " 'future care': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 43},\n",
       " 'future wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 53},\n",
       " 'general': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 11},\n",
       " 'non pecuniary': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 80},\n",
       " 'other': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 927},\n",
       " 'past wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 44},\n",
       " 'punitive': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5},\n",
       " 'reduction': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 11},\n",
       " 'special': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70},\n",
       " 'sub-future care': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 14},\n",
       " 'sub-future wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 12},\n",
       " 'sub-general': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 2},\n",
       " 'sub-non pecuniary': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 4},\n",
       " 'sub-past wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 7},\n",
       " 'sub-special': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 38},\n",
       " 'sub-total': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 6},\n",
       " 'total': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 44},\n",
       " 'total after': {'precision': 0.004357298474945534,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.008676789587852495,\n",
       "  'support': 6},\n",
       " 'accuracy': 0.0043541364296081275,\n",
       " 'macro avg': {'precision': 0.0002293314986813439,\n",
       "  'recall': 0.05263157894736842,\n",
       "  'f1-score': 0.0004566731362027629,\n",
       "  'support': 1378},\n",
       " 'weighted avg': {'precision': 1.897227202443629e-05,\n",
       "  'recall': 0.0043541364296081275,\n",
       "  'f1-score': 3.777992563651304e-05,\n",
       "  'support': 1378}}"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr = LogisticRegressionCV(cv = 5, random_state=30, max_iter=500, solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "classification_report(y_test, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6727140783744557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilana/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aggravated': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 1},\n",
       " 'future care': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 43},\n",
       " 'future wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 53},\n",
       " 'general': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 11},\n",
       " 'non pecuniary': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 80},\n",
       " 'other': {'precision': 0.6727140783744557,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.8043383947939262,\n",
       "  'support': 927},\n",
       " 'past wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 44},\n",
       " 'punitive': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5},\n",
       " 'reduction': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 11},\n",
       " 'special': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70},\n",
       " 'sub-future care': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 14},\n",
       " 'sub-future wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 12},\n",
       " 'sub-general': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 2},\n",
       " 'sub-non pecuniary': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 4},\n",
       " 'sub-past wage loss': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 7},\n",
       " 'sub-special': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 38},\n",
       " 'sub-total': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 6},\n",
       " 'total': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 44},\n",
       " 'total after': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 6},\n",
       " 'accuracy': 0.6727140783744557,\n",
       " 'macro avg': {'precision': 0.03540600412497136,\n",
       "  'recall': 0.05263157894736842,\n",
       "  'f1-score': 0.04233359972599612,\n",
       "  'support': 1378},\n",
       " 'weighted avg': {'precision': 0.4525442312431934,\n",
       "  'recall': 0.6727140783744557,\n",
       "  'f1-score': 0.5410897619549853,\n",
       "  'support': 1378}}"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print(svc.score(X_test, y_test))\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "classification_report(y_test, y_pred_svc, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<damage type=\"non pecuniary\">$1,000,000</damage>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['$1,000,000'], ['non pecuniary'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case2tags(\"I asses non-pecuniary damages of <damage type='non pecuniary'>$1,000,000</damage>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<damage type=\"non pecuniary\">$1,000,000</damage>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup('<xml>'+\"I asses non-pecuniary damages of <damage type='non pecuniary'>$1,000,000</damage>\"+'</xml>', \"xml\")\n",
    "soup.find_all('damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
