{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_parse_BCJ(path):\n",
    "    '''Given file path (text file) of negligence cases, finds static \n",
    "    information within the case (information that can be pattern matched)\n",
    "    Expects a B.C.J. case format (British Columbia Judgments)\n",
    "    \n",
    "    The following fields are currently implemented:\n",
    "    - Case Title\n",
    "    - Judge Name\n",
    "    - Registry\n",
    "    - Year\n",
    "    - Decision Length (in paragraphs)\n",
    "    - Damages\n",
    "    - Multiple Defendants\n",
    "    - Plaintiff Wins\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: case_parsed_data (list) of case_dict (Dictionary): List of Dictionaries with rule based parsable fields filled in\n",
    "    '''\n",
    "    with open(path, encoding='utf-8') as document:\n",
    "        document_data = document.read()\n",
    "        \n",
    "    document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "    case_parsed_data = []\n",
    "    for i in range(len(document_data)):\n",
    "        case_dict = dict() \n",
    "        case = document_data[i]\n",
    "        case = case.strip() # Make sure to strip!\n",
    "        if len(case) == 0: # Skip empty lines\n",
    "            continue\n",
    "        \n",
    "        lines = case.split('\\n')\n",
    "        if len(lines) < 2:\n",
    "            print(case)\n",
    "        case_title = lines[0]\n",
    "        case_type = lines[1]\n",
    "\n",
    "        if filter_unwanted_cases(case, case_title, case_type):\n",
    "            # Fields that can be found via pattern matching\n",
    "            if re.search('contributory negligence', case, re.IGNORECASE):\n",
    "                contributory_negligence_raised = True\n",
    "            else:\n",
    "                contributory_negligence_raised = False\n",
    "            case_number = re.search(r'\\/P([0-9]+)\\.txt', path).group(1)\n",
    "            decision_len = re.search(r'\\(([0-9]+) paras\\.?\\)', case) # e.g.) (100 paras.)\n",
    "            registry = re.search(r'(Registry|Registries): ?([A-Za-z0-9 ]+)', case) # e.g.) Registry: Vancouver\n",
    "            written_decision = True if int(decision_len.group(1)) > 1 else False\n",
    "            if registry:\n",
    "                registry = registry.group(2).strip()\n",
    "            else:\n",
    "                registry = re.search(r'([A-Za-z ]+) Registry No.', case) # Alt form e.g.) Vancouver Registory No. XXX\n",
    "                if registry:\n",
    "                    registry = registry.group(1).strip()\n",
    "                else:\n",
    "                    registry = re.search(r'([A-Za-z ]+) No. S[0-9]*', case)\n",
    "                    if registry:\n",
    "                        registry = registry.group(1).strip()\n",
    "                    else:\n",
    "                        print('WARNING: Registry could not be found (This shouldn\\'t occur!)')\n",
    "            # Fields that are always in the same place\n",
    "            judge_name = lines[4].strip()\n",
    "            case_title = lines[0].strip()\n",
    "            # Extract year from case_title (in case we want to make visualizations, etc.)\n",
    "            year = re.search(r'20[0-2][0-9]', case_title) # Limit regex to be from 2000 to 2029\n",
    "            if year:\n",
    "                year = year.group(0)\n",
    "            else:\n",
    "                # Rare case: Sometimes the title is too long. Rely on Heard date.\n",
    "                year = re.search(r'Heard:.* ([2][0][0-2][0-9])', case)\n",
    "                if year:\n",
    "                    year = year.group(1)\n",
    "                else:\n",
    "                    print('WARNING: Year not found')\n",
    "            case_dict['case_number'] = '%s of %s'%(i+1+((int(case_number)-1)*50), case_number)\n",
    "            case_dict['case_title'] = case_title\n",
    "            case_dict['year'] = year\n",
    "            case_dict['registry'] = registry\n",
    "            case_dict['judge'] = judge_name\n",
    "            case_dict['decision_length'] = decision_len.group(1)\n",
    "            case_dict['multiple_defendants'] = rule_based_multiple_defendants_parse(case)\n",
    "            case_dict['contributory_negligence_raised'] = contributory_negligence_raised\n",
    "            case_dict['written_decision'] = written_decision\n",
    "            \n",
    "            # TODO: Improve plaintiff_wins to take one case at a time.\n",
    "            plaintiff_list = plaintiff_wins(path)\n",
    "            if case_title in plaintiff_list:\n",
    "                case_dict['plaintiff_wins'] = plaintiff_list[case_title]\n",
    "            else:\n",
    "                case_dict['plaintiff_wins'] = \"NA\"\n",
    "                \n",
    "            case_dict['damages'] = rule_based_damage_extraction(case)\n",
    "            percent_reduction, contributory_negligence_successful = get_percent_reduction_and_contributory_negligence_success(case_dict, case)\n",
    "            case_dict['percent_reduction'] = percent_reduction\n",
    "            case_dict['contributory_negligence_successful'] = contributory_negligence_successful\n",
    "             \n",
    "        \n",
    "        # don't add empty dictionaries (non BCJ cases) to list\n",
    "        if case_dict != dict(): \n",
    "            case_parsed_data.append(case_dict)\n",
    "    return case_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_multiple_defendants_parse(doc):\n",
    "    ''' Helper function for rule_based_parse_BCJ\n",
    "    \n",
    "    Given a case. Uses regex/pattern-matching to determine whether we have multiple defendants.\n",
    "    For the most part the logic relies on whether the langauge used implies plurality or not.\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    Returns: response (String, 'Y', 'N', or 'UNK')\n",
    "    '''\n",
    "\n",
    "    # Case 1)\n",
    "    # Traditional/most common. Of form \"Between A, B, C, Plaintiff(s), X, Y, Z Defendant(s)\"\n",
    "    # Will also allow \"IN THE MATTER OF ... Plaintiff .... Defendant...\"\n",
    "    # Can successfully cover ~98% of data\n",
    "    regex_between_plaintiff_claimant = re.search(r'([Between|IN THE MATTER OF].*([P|p]laintiff[s]?|[C|c]laimant[s]?|[A|a]ppellant[s]?|[P|p]etitioner[s]?|[R|r]espondent[s]?).*([D|d]efendant[s]?|[R|r]espondent[s]?|[A|a]pplicant[s]?).*\\n)', doc)\n",
    "    \n",
    "    # Match found\n",
    "    if regex_between_plaintiff_claimant:\n",
    "        text = regex_between_plaintiff_claimant.group(0).lower()\n",
    "        if 'defendants' in text or 'respondents' in text or 'applicants' in text: # Defendant/respondent same thing.\n",
    "            return 'Y'\n",
    "        elif 'defendant' in text or 'respondent' in text or 'applicant' in text:\n",
    "            return 'N'\n",
    "    \n",
    "    # If not found, try other less common cases\n",
    "    else:\n",
    "        # Case 2)\n",
    "        # Sometimes it does not mention the name of the second item. (Defendent/Respondent)\n",
    "        # We can estimate if there are multiple based on the number of \",\" in the line (Covers all cases in initial data)\n",
    "        regex_missing_defendent = re.search(r'(Between.*([P|p]laintiff[s]?|[C|c]laimant[s]?|[A|a]ppellant[s]?|[P|p]etitioner[s]?).*\\n)', doc)\n",
    "        if regex_missing_defendent:\n",
    "            text = regex_missing_defendent.group(0).lower()\n",
    "            if len(text.split(',')) > 5:\n",
    "                return 'Y'\n",
    "            else:\n",
    "                return 'N'\n",
    "            \n",
    "        else:\n",
    "            print('Multiple defendants: Unknown! Unable to regex match')\n",
    "            return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_damage_extraction(doc, min_score = 0.9, max_match_len_split = 10):\n",
    "    '''Helper function for rule_based_parse_BCJ\n",
    "    \n",
    "    Given a case, attempts to extract damages using regex patterns\n",
    "    \n",
    "    Arguments: doc (String): The case in text format following the form used in the DOCX to TXT notebook\n",
    "    min_score (float): The minimum paragraph score to consider having a valid $ number\n",
    "                       Paragraph has score 1 if its the last paragraph\n",
    "                       Paragraph has score 0 if its the first paragraph\n",
    "    max_match_len_split (int): The max amount of items that can appear in a regex match after splitting (no. words)\n",
    "    \n",
    "    Returns: damages (Dict): Contains any found damages\n",
    "    \n",
    "    '''\n",
    "    damages = defaultdict(float)\n",
    "    repetition_detection = defaultdict(set) # try to stem the repeated values\n",
    "    no_paras = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', doc).group(1) # Get number of paragraphs\n",
    "    pattern = r'([.]?)(?=\\n[0-9]{1,%s}[\\xa0|\\s| ]{2})'%len(no_paras) # Used to split into paras\n",
    "    paras_split = re.split(pattern, doc)\n",
    "    money_patt = r'\\$[0-9|,]+' # Used to get all paragraphs with a money amount\n",
    "    scored_paras = [] # Score paragraphs based on where they appear in the document\n",
    "                      # Score of 0.0 would be the first paragraph. Score of 1.0 would be the last paragraph\n",
    "        \n",
    "    for i, paragraph in enumerate(paras_split):\n",
    "        if re.search(money_patt, paragraph):\n",
    "            scored_paras.append((i / len(paras_split), paragraph)) # (score, paragraph). Score formula: i/no_paras\n",
    "            \n",
    "    scored_paras = sorted(scored_paras, key=lambda x:x[0])[::-1] # Store from last paragraph to first\n",
    "    if len(scored_paras) == 0:\n",
    "        return None\n",
    "    if scored_paras[0][0] < min_score: #If highest scored paragraph is less than minimum score.\n",
    "        return None\n",
    "    \n",
    "    # Rule based dmg extraction REGEX patterns\n",
    "    regex_damages = r'[\\w|-]* ?(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "    #regex_damages = r'(?:[\\w|-]* ?){0,3}(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "    #regex_in_trust = r'(?:in-?trust|award).*?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "    #regex_damages = r'(?![and])(?:[\\w|-]* ?){0,2} ?(?:damage|loss|capacity|cost).+?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "    regex_damages_2 = r'[^:] \\$? ?[0-9][0-9|,|.]+[0-9] (?:for|representing)?[ \\w\\-+]+damages?'\n",
    "    regex_damages_3 = r'[^:] \\$? ?[0-9][0-9|,|.]+[0-9] (?:for|representing)?[ \\w\\-+]+damages?(?:(?:for|representing)?.*?[;.\\n])'\n",
    "    regex_future_care_loss = r'(?:future|past|in[-| ]?trust|award).*?(?:loss|costs?|income|care)?.*?\\$? ?[0-9][0-9|,|.]+[0-9]'\n",
    "    regex_for_cost_of = r'\\$? ?[0-9][0-9|,|.]+[0-9][\\w ]*? cost .*?\\.'\n",
    "\n",
    "    # Keywords to look in match for categorization\n",
    "    general_damage_keywords = [('general',), ('future', 'income', 'loss'), ('future', 'income'), ('future', 'wage', 'loss'), ('future', 'earning'), ('!past', 'earning', 'capacity'), ('future', 'capacity'), ('future', 'earning'), ('!past', 'loss', 'opportunity'), ('!past', 'loss', 'housekeep'), ('ei', 'benefit')]\n",
    "    special_damage_keywords = [('special',), ('trust',), ('past', 'income', 'loss'), ('past', 'wage'), ('past', 'earning'), ('past', 'income'), ('earning', 'capacity')]\n",
    "    aggravated_damage_keywords = [('aggravated',)]\n",
    "    non_pecuniary_damage_keywords = [('non', 'pecuniary')]\n",
    "    punitive_damage_keywords = [('punitive',)]\n",
    "    future_care_damage_keywords = [('future', 'care'), ('future', 'cost')]\n",
    "    \n",
    "    patterns = [regex_damages, regex_damages_2, regex_damages_3, regex_future_care_loss, regex_for_cost_of]\n",
    "    banned_words = ['seek', 'claim', 'propose', 'range', ' v. '] # Skip paragraphs containing these\n",
    "    counter_words = ['summary', 'dismissed'] # Unless these are mentioned. \n",
    "                                             # example) \"Special damage is $5k. But claims for aggravated are 'dismissed'\" \n",
    "    \n",
    "    # Get money mounts from the text\n",
    "    total = None\n",
    "    matches = []\n",
    "    summary_matches = []\n",
    "    for i, scored_para in enumerate(scored_paras):\n",
    "        text = scored_para[1]\n",
    "        score = scored_para[0]\n",
    "        \n",
    "        if score > min_score:\n",
    "            if any(item.startswith('summary') for item in text.lower().split()[:4]) or any(item.startswith('conclusion') for item in text.lower().split()[:4]):\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    summary_matches.append((score, t_m))\n",
    "            elif i+1 < len(scored_paras) and (any(item.startswith('summary') for item in scored_paras[i+1][1].lower().split()[-4:]) or any(item.startswith('conclusion') for item in scored_paras[i+1][1].lower().split()[-4:])):\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    summary_matches.append((score, t_m))\n",
    "            else:\n",
    "                skip = False # Skip paras with banned words\n",
    "                for banned_word in banned_words: \n",
    "                    if banned_word in text:\n",
    "                        skip = True       \n",
    "                for counter_word in counter_words:\n",
    "                    if counter_word in text:\n",
    "                        skip = False\n",
    "                if skip:\n",
    "                    continue\n",
    "\n",
    "                text_matches = get_matching_text(patterns, text, max_match_len_split)\n",
    "                for t_m in text_matches:\n",
    "                    matches.append((score, t_m))\n",
    "        \n",
    "    # Only keep matches from the summary if a summary was found. If not keep all matches.\n",
    "    if len(summary_matches) > 0: \n",
    "        matches = summary_matches\n",
    "\n",
    "    # Extract $ value. Determine correct column\n",
    "    regex_number_extraction = r' ?[0-9][0-9|,|.]+[0-9]'\n",
    "    for score, match in matches:\n",
    "        skip = False # Banned words should not appear in final matches\n",
    "        for banned_word in banned_words: \n",
    "            if banned_word in match:    \n",
    "                skip = True\n",
    "                break\n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        amount = re.findall(regex_number_extraction, match, re.IGNORECASE)\n",
    "        extracted_value = clean_money_amount(amount)\n",
    "        if extracted_value is None: # Make sure we are able to extract a value\n",
    "            continue\n",
    "            \n",
    "        value_mapped = False # If we mapped the value into a damage category - stop trying to map into other categories\n",
    "        value_mapped = assign_damage_to_category(extracted_value, general_damage_keywords, match, score, matches, 'General', damages, repetition_detection, repetition_key = ('general',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, special_damage_keywords, match, score, matches, 'Special', damages, repetition_detection, repetition_key = ('special',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, non_pecuniary_damage_keywords, match, score, matches, 'Non-pecuniary', damages, repetition_detection, repetition_key = ('non','pecuniary'))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, aggravated_damage_keywords, match, score, matches, 'Aggravated', damages, repetition_detection, repetition_key = ('aggravated',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, punitive_damage_keywords, match, score, matches, 'Punitive', damages, repetition_detection, repetition_key = ('punitive',))\n",
    "        if not value_mapped:\n",
    "            value_mapped = assign_damage_to_category(extracted_value, future_care_damage_keywords, match, score, matches, 'Future Care', damages, repetition_detection) \n",
    "        if not value_mapped: # Last attempt: Only use \"total amounts\" if nothing else was found\n",
    "            total_keywords = [('total',), ('sum',), ('award',)]\n",
    "            for keywords in total_keywords:\n",
    "                if match_contains_words(match.lower(), keywords):\n",
    "                    if is_best_score(score, matches, keywords):\n",
    "                        if extracted_value not in repetition_detection[('total',)]:\n",
    "                            damages['Pecuniary Total'] = damages['Special'] + damages['General'] + damages['Punitive'] + damages['Aggravated'] + damages['Future Care']\n",
    "                            damages['Total'] = damages['Pecuniary Total'] + damages['Non-pecuniary']\n",
    "                            if damages['Total'] == 0:\n",
    "                                total = extracted_value\n",
    "                                repetition_detection[('total',)].add(extracted_value)\n",
    "                        \n",
    "    damages['Pecuniary Total'] = damages['Special'] + damages['General'] + damages['Punitive'] + damages['Aggravated'] + damages['Future Care']\n",
    "    damages['Total'] = damages['Pecuniary Total'] + damages['Non-pecuniary']\n",
    "    \n",
    "    if damages['Total'] == 0 and total is not None: # Only use the \"total\" if we couldnt find anything else!\n",
    "        damages['Total'] = total\n",
    "        damages['General'] = total\n",
    "        \n",
    "    columns = ['Total', 'Pecuniary Total', 'Non-pecuniary', 'Special', 'General', 'Punitive', 'Aggravated', 'Future Care']\n",
    "    for c in columns:\n",
    "        damages[c] = None if damages[c] == 0 else damages[c]\n",
    "    \n",
    "    return damages\n",
    "\n",
    "def assign_damage_to_category(damage, damage_keywords, match, match_score, matches, damage_type, damage_dict, repetition_dict, repetition_key = None):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Adds damage to dictionary based on given parameters so long as it is the\n",
    "    highest scoring match & doesn't appear in the repetition dictionary\n",
    "    \n",
    "    Argumets:\n",
    "    damage (float) - The damage amount in the match\n",
    "    damage_keywords (list) - Keywords that may appear in match\n",
    "    match (string) - The match string itself\n",
    "    matches (list) - All matches. Used to determine if we found the best match\n",
    "    damage_dict (dict) - Dictionary storing all damages\n",
    "                       - Will be modified in place\n",
    "    repetition_dict (dict) - Dictionary storing repeated values\n",
    "                           - Will be modified in place\n",
    "    (Optional) repetition_key (Tuple) - If not none, will use this key to store repetitions. Else will use matching keyword\n",
    "    \n",
    "    Returns:\n",
    "    value_belongs (Boolean) - True if the value belongs in the given keyword category. False otherwise\n",
    "    '''\n",
    "    match = match.lower()\n",
    "    value_belongs = False\n",
    "    \n",
    "    for keywords in damage_keywords:\n",
    "        if match_contains_words(match, keywords):\n",
    "            value_belongs = True\n",
    "            if is_best_score(match_score, matches, keywords):\n",
    "                if damage not in repetition_dict[repetition_key if repetition_key else keywords]:\n",
    "                    damage_dict[damage_type] += damage\n",
    "                    repetition_dict[repetition_key if repetition_key else keywords].add(damage)\n",
    "            break\n",
    "    \n",
    "    return value_belongs\n",
    "\n",
    "def clean_money_amount(money_regex_match):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Arguments:\n",
    "    money_regex_match (Regex.findall object) - Match of $ amount\n",
    "    \n",
    "    Returns:\n",
    "    None if a bad match\n",
    "    extracted_value (float) - The money amount in float form\n",
    "    '''\n",
    "    # If our regex contains more than 1 or 0 money values. We cannot use the match.\n",
    "    if len(money_regex_match) > 1:\n",
    "        return None\n",
    "    if len(money_regex_match) == 0:\n",
    "        print('Error: No Money in match!', match)\n",
    "        return None\n",
    "\n",
    "    extracted_value = None\n",
    "    amount = money_regex_match[0].replace(',' , '')\n",
    "    amount = amount.replace(' ' , '')\n",
    "    # Deals with money at end of sentence. example) ... for '5,000.00.' -> '5000.00'\n",
    "    if amount[-1] == '.': \n",
    "        amount = amount[:-1]\n",
    "    # Deals with quantities such as $2.5 million\n",
    "    if 'million' in amount or amount[-1] == 'm':\n",
    "        amount = str(float(re.findall('[0-9|\\.]+', amount)[0])*10e6)\n",
    "    # Deals with a rare typo in some cases. example) 50.000.00 -> 50000.00\n",
    "    if amount.count('.') > 1: \n",
    "        dot_count = amount.count('.')\n",
    "        changes_made = 0\n",
    "        new_amount = ''\n",
    "        for letter in amount:\n",
    "            if letter == '.' and changes_made != dot_count-1:\n",
    "                changes_made += 1\n",
    "            else:\n",
    "                new_amount += letter\n",
    "        amount = new_amount\n",
    "    extracted_value = float(amount)\n",
    "    return extracted_value\n",
    "\n",
    "def get_matching_text(patterns, text, max_match_len_split):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given a set of regex; pulls out all matching text\n",
    "    \n",
    "    Arguments:\n",
    "    patterns (list) - List of regex patterns in string format\n",
    "    text (string) - Text to search for matches in\n",
    "    \n",
    "    Returns:\n",
    "    matches (list) - List containing all matches in text format\n",
    "    '''\n",
    "\n",
    "    matches = []\n",
    "    for pattern in patterns:\n",
    "        for match in re.findall(pattern, text, re.IGNORECASE):\n",
    "            if 'and' not in match:\n",
    "                if len(match.split()) <= max_match_len_split:\n",
    "                    matches.append(match)\n",
    "                    \n",
    "    return matches\n",
    "\n",
    "def is_best_score(score, matches, keywords):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given a set of regex matches, determine if the score is the highest score out of all matches for the given keywords\n",
    "    Score is from 0 - 1; describes where in the paragraph the match was found\n",
    "    Score is 1 if the match came from the final paragraph\n",
    "    Score is 0 if the match came from the first paragraph\n",
    "    \n",
    "    Arguments:\n",
    "    score (float) - The score of the item you're inspecting\n",
    "    matches (list) - List of matches where each element is of form (score, match text)\n",
    "    keywords (tuple) - All words that should appear in the match\n",
    "    \n",
    "    Returns: True or False\n",
    "    \n",
    "    '''\n",
    "    best_score = score\n",
    "    \n",
    "    for score, match in matches:\n",
    "        if all(word in match.lower() for word in keywords):\n",
    "            if score > best_score:\n",
    "                return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def match_contains_words(match, words):\n",
    "    '''Helper function for rule based damage extraction.\n",
    "    \n",
    "    Given some text. Find if the words are all present in the text.\n",
    "    If word begins with '!' the word cannot appear in the text, acts as a negation. \n",
    "    Can handle mix/matching of both types.\n",
    "    \n",
    "    Example: ('!good', 'day') would match any string with the word \"day\" present and \"good\" NOT present.\n",
    "    \n",
    "    Arguments:\n",
    "    match (String) - The text to look for words in\n",
    "    words (list) - List of words to check for. If word begins with ! (i.e. '!past'), then the word cannot appear in it\n",
    "    \n",
    "    Returns:\n",
    "    True if all words are present (or not present if using !)\n",
    "    False otherwise\n",
    "    \n",
    "    '''\n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    "    for word in words:\n",
    "        if word.startswith('!'):\n",
    "            neg_words.append(word[1:])\n",
    "        else:\n",
    "            pos_words.append(word)\n",
    "            \n",
    "    if all(word in match for word in pos_words):\n",
    "        if all(word not in match for word in neg_words):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def filter_unwanted_cases(case, case_title, case_type):\n",
    "    '''Given a case, its title & type, determines whether the case\n",
    "    is relevant or not for our analysis\n",
    "    \n",
    "    Removes crown cases 'R.v.'\n",
    "    Removes '(Re)' cases\n",
    "    Removes client-solicitor cases\n",
    "    Removes IN THE MATTER OF cases where plaintiff/defendant is not mentioned\n",
    "    Removes non 'British Columbia Judgments' cases\n",
    "    \n",
    "    Arguments:\n",
    "    case (string) - Case data in string form\n",
    "    case_title (string) - Case title (line 1 of case)\n",
    "    case_type (string) - Case type (line 2 of case)\n",
    "    \n",
    "    Returns:\n",
    "    boolean - True if case should be analyzed. False if it should be skipped.\n",
    "    '''\n",
    "    \n",
    "    if 'R. v.' in case_title or '(Re)' in case_title: # Skip crown cases, Skip (Re) cases\n",
    "        return False\n",
    "\n",
    "    # Skip client/solicitor cases (not same as plaintiff/defendant)\n",
    "    regex_client_solicitor = re.search(r'(Between.*([C|c]lient[s]?).*([S|s]olicitor[s]?|[L|l]awyer[s]?))', case)\n",
    "    if regex_client_solicitor:\n",
    "        return False\n",
    "\n",
    "    regex_solicitor_client = re.search(r'(Between.*([L|l]awyer[s]?|[S|s]olicitor[s]?).*([C|c]lient[s]?))', case)\n",
    "    if regex_solicitor_client:\n",
    "        return False\n",
    "\n",
    "    # In some rare cases we have 'IN THE MATTER OF ..' (rather than 'Between ...') .. but it is following by the normal\n",
    "    # plaintiff/defendant dynamic. Only skip cases if there is no mention of the following terms\n",
    "    # (Can be cleaned up in future)\n",
    "    key_words = ['appellant', 'respondent', 'claimant', 'petitioner', 'plaintiff', 'defendant',\n",
    "    'appellants', 'respondents', 'claimants', 'petitioners', 'plaintiffs', 'defendants']\n",
    "    regex_in_matter_of = re.search(r'IN THE MATTER OF .*\\n\\([0-9]+ paras.\\)', case)\n",
    "    if regex_in_matter_of:\n",
    "        remove = True\n",
    "        for key in key_words:\n",
    "            if key in regex_in_matter_of.group(0).lower().strip():\n",
    "                remove = False\n",
    "\n",
    "        if remove:\n",
    "            return False\n",
    "\n",
    "    if 'British Columbia Judgments' in case_type:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_tokenize(case):\n",
    "    ''' Takes string input the of wntire document (case) and returns list of lists of paragraphs in the document.\n",
    "    ---------\n",
    "    Input: case (str) - string of single legal case\n",
    "    Return: case_data(list) - list of of numbrered paragraphs in the document where the first item is the case_title'''\n",
    "    \n",
    "    case_data = []\n",
    "    lines = case.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    case_data.append(lines[0])\n",
    "    decision_length = re.search(r'\\(([0-9|,]+) paras?\\.?\\)', case).group(1)\n",
    "\n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    pattern = r'.?(?=\\n[0-9]{1,%s}[\\xa0]{2})'%len(decision_length)\n",
    "    paras_split = re.split(pattern, case)\n",
    "\n",
    "    paras = []\n",
    "    for para in paras_split:   \n",
    "        # make sure the paragraph starts with the correct characters\n",
    "        para_start = re.match(r'^\\n([0-9]{1,%s})[\\xa0]{2}'%len(decision_length), para)\n",
    "        if para_start:\n",
    "            paras.append(para)\n",
    "    case_data.extend(paras)\n",
    "    return case_data\n",
    "\n",
    "def summary_tokenize(case):\n",
    "    ''' String of Entire Document and returns the document summary and HELD section.\n",
    "    ---------\n",
    "    Input: case (str) - string of single legal case\n",
    "    Return: summary - summary and HELD section of case (str)'''\n",
    "    \n",
    "    lines = case.split('\\n')\n",
    "    if not 'British Columbia Judgments' in lines[1]:\n",
    "        return\n",
    "    \n",
    "    # split paragraphs on newline, paragraph number, two spaces\n",
    "    summary = re.search(r'\\([0-9]{1,3} paras\\.\\)\\ncase summary\\n((.*\\n+?)+)(?=HELD|(Statutes, Regulations and Rules Cited:)|(Counsel\\n))', case, re.IGNORECASE)\n",
    "    if summary:\n",
    "        summary = summary.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return summary\n",
    "\n",
    "def get_context_and_float(value, text, context_length = 8, plaintiff_name = 'Plaintiff', defendant_name = 'Defendant'):\n",
    "    '''Given a string value found in a body of text, \n",
    "    return its context, and its float equivalent.\n",
    "    -----------------\n",
    "    Arguments:\n",
    "    value - percent match found in text\n",
    "    text - string value where matches were extracted from, eg paragraph or summary (str)\n",
    "    context_length - the length of context around each quantity to return\n",
    "    Rerturn:\n",
    "    value_context - string of context around value (str)\n",
    "    extracted_value - string quantity value extracted to its float equivalent'''\n",
    "    \n",
    "    \n",
    "    # get context for monetary/percent values \n",
    "    context = ''\n",
    "    amount = re.findall(r'[0-9]+[0-9|,]*(?:\\.[0-9]+)?', value)\n",
    "    extracted_value = clean_money_amount(amount) #use helper function to get float of dollar/percent value\n",
    "    if not extracted_value:\n",
    "        print('ERROR: cant convert string, %s'%value)\n",
    "        return context, None\n",
    "    # get indices of last instance of value in text - tokenize like this for values of type 'per cent and percent'\n",
    "    start_idx = text.rfind(value)\n",
    "    if start_idx == -1:\n",
    "        print('ERROR: value not in text')\n",
    "    end_idx = start_idx + len(value)\n",
    "    tokens = text[:start_idx].split() + [value] + text[end_idx:].split()\n",
    "    \n",
    "    # get indices of quantity value in text\n",
    "    loc = [i for i, token in enumerate(tokens) if value in token] \n",
    "    \n",
    "    # if the quantity is in the text, choose context of last mention of value\n",
    "    if len(loc) > 0:\n",
    "        loc = loc[-1] \n",
    "        if loc - context_length >= 0 and loc + context_length < len(tokens):\n",
    "            context = \" \".join(tokens[loc - context_length:loc + context_length + 1])\n",
    "        elif loc - context_length < 0 and loc + context_length < len(tokens):\n",
    "            beg = abs(loc -context_length)\n",
    "            context = \" \".join(tokens[loc-context_length + beg:loc + context_length + 1])\n",
    "        elif loc - context_length > 0 and loc + context_length > len(tokens): \n",
    "            context = \" \".join(tokens[loc - context_length:len(tokens)])\n",
    "\n",
    "    return context.lower(), extracted_value\n",
    "\n",
    "def conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities):\n",
    "    ''' Given the context surrounding an extracted value (percent), keywords relevant to contributory negligence (ie liability, approtion, fault, etc), \n",
    "    a list of the Plaintiffs names (ie John Doe), a list of the defendants names, and a combined list of entities(ie plaintiff, john, doe, defendant):\n",
    "    Return: the modifed extracted value (float)\n",
    "    ------------\n",
    "    Arugments:\n",
    "    context: (str)\n",
    "    extracted_value: (float) found in context\n",
    "    keywords, plaintiff_split, defendant_split, entities: (list) of strings\n",
    "    ------------\n",
    "    Example:\n",
    "    context = 'the defendant is responsible for 30% of damages'\n",
    "    extracted_value = 30.0\n",
    "    keywords = ['fault', 'liable', 'liability', 'apportion', 'contributor', 'recover', 'responsible']\n",
    "    plaintiff_split = ['john', 'doe']\n",
    "    defendant_split = ['jane', 'smith']\n",
    "    entities = ['plaintiff', 'defendant', 'john', 'jane', 'doe', 'smith']\n",
    "    conditions_for_extracted_value(context, extracted_value, \n",
    "                        keywords, plaintiff_split, defendant_split) = 70.0\n",
    "    '''\n",
    "    # conditions for keeping extracted_value and updating extracted_value\n",
    "    # skip extracted_values with contexts lacking keywords/entities\n",
    "    if extracted_value == 100 or extracted_value == 0 or extracted_value < 10:\n",
    "        return\n",
    "    if not any(token in context for token in keywords + entities) or context == '' or any('costs' == token for token in context.split()) or ('interest' in context and 'rate' in context.split()):\n",
    "        return \n",
    "    if 'recover' in context and any(word in context for word in plaintiff_split + ['plaintiff']):\n",
    "        extracted_value = 100 - extracted_value\n",
    "    if any(word1 in context and word2 in context for word1 in defendant_split + ['defendant'] for word2 in ['liable', 'responsible', 'fault', 'against']):\n",
    "        extracted_value = 100 - extracted_value\n",
    "    return extracted_value\n",
    "\n",
    "def contributory_negligence_successful_fun(context, keywords):\n",
    "    '''Given text containing percent reduction and a list of keywords to check for,\n",
    "    confirm presence of keywords and return whether or not contributory negligence was successful\n",
    "    --------------\n",
    "    Arguments:\n",
    "    context (str)\n",
    "    keywords(list)\n",
    "    Returns: True or None (bool)'''\n",
    "    if any(word in context for word in keywords):\n",
    "        if 'plaintiff' or 'damages' or 'defendant' in context:\n",
    "            contributory_negligence_successful = True\n",
    "            return contributory_negligence_successful\n",
    "    return\n",
    "\n",
    "def get_percent_reduction_and_contributory_negligence_success(case_dict, case, min_score = 0.9):\n",
    "    paragraphs = paragraph_tokenize(case)\n",
    "    case_title = case_dict['case_title']\n",
    "    assert paragraphs[0] == case_title\n",
    "    \n",
    "    # default value for contributory negligence success is FALSE\n",
    "    contributory_negligence_successful = False\n",
    "    percent_pattern = r'([0-9][0-9|\\.]*(?:%|\\sper\\s?cent))'\n",
    "    \n",
    "    # entities and keywords used to filter percent values\n",
    "    keywords = ['against', 'reduce', 'liability', 'liable', 'contributor', 'fault', 'apportion', 'recover', 'responsible']\n",
    "    # extract plaintiff and defendant name for use in %reduction conditions\n",
    "    plaintiff_defendant_pattern = r'([A-Za-z|-|\\.]+(:? \\(.*\\))?)+ v\\. ([A-Za-z|-]+)+' # group 1 is plaintiff group 2 is defendant\n",
    "    if re.search(plaintiff_defendant_pattern, case_title):\n",
    "        plaitiff_defendant = re.search(plaintiff_defendant_pattern, case_title).groups() # tuple (plaintiff, defendant)\n",
    "    else:\n",
    "        plaitiff_defendant = ('Plaintiff', 'Defendant')\n",
    "    plaintiff_split = [word.lower() for word in plaitiff_defendant[0].split()]\n",
    "    defendant_split = [word.lower() for word in plaitiff_defendant[-1].split()]\n",
    "    entities = ['defendant', 'plaintiff'] + plaintiff_split + defendant_split \n",
    "\n",
    "    if case_dict['contributory_negligence_raised'] and case_dict['plaintiff_wins']:\n",
    "        percent_reduction = None\n",
    "        best_percent = None\n",
    "        best_score = 0\n",
    "        for j, paragraph in enumerate(paragraphs[1:]):\n",
    "            score = float((j+1)/int(case_dict['decision_length']))\n",
    "            paragraph = paragraph.lower()\n",
    "            if not score >= min_score: ## min score not existant in bcj parser\n",
    "                continue\n",
    "\n",
    "            percent_mentioned = re.findall(percent_pattern, paragraph, re.IGNORECASE)\n",
    "            extracted_value_tie_breaker = Counter()\n",
    "            if len(percent_mentioned) > 0:\n",
    "                for percent in percent_mentioned:\n",
    "                    context, extracted_value = get_context_and_float(percent, paragraph)\n",
    "                    # conditions for keeping extracted_value and updating extracted_value\n",
    "                    # skip extracted_values with contexts lacking keywords/entities\n",
    "                    if context == '':\n",
    "                        continue\n",
    "                    extracted_value = conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities)\n",
    "                    if not extracted_value:\n",
    "                        continue\n",
    "                        \n",
    "                    extracted_value_tie_breaker.update([extracted_value])\n",
    "                \n",
    "                    # conditions for contributory negligence successful\n",
    "                    if not contributory_negligence_successful and extracted_value:\n",
    "                        contributory_negligence_successful = contributory_negligence_successful_fun(context, keywords)\n",
    "\n",
    "                    # matches patter \"PERCENT against plaintiff\"\n",
    "                    if ('against' in context or 'fault' in context) and any(plaintiff_word in context for plaintiff_word in plaintiff_split+['plaintiff']):\n",
    "                        best_percent = extracted_value\n",
    "                        best_score = score\n",
    "                        break                    \n",
    "                    \n",
    "                    # choose most common percent mentioned in highest scoring paragraph\n",
    "                    if extracted_value_tie_breaker != Counter():\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_percent = extracted_value_tie_breaker.most_common(1)[0][0]\n",
    "\n",
    "             # if no percent found, check for equal apportionment\n",
    "            else:\n",
    "                equal_apportionment = re.findall(r'.{20} (?:liability|fault) [a-zA-Z]{1,3} apportione?d? equally .{20}', paragraph)\n",
    "                if len(equal_apportionment) > 0:\n",
    "                    if contributory_negligence_successful_fun(equal_apportionment[0], keywords):\n",
    "                        best_percent = 50.0\n",
    "                        contributory_negligence_successful = True\n",
    "        \n",
    "        if best_score == 0 or not best_percent or not contributory_negligence_successful:\n",
    "            # no percents found in paragraphs - time to check summary - same process\n",
    "            summary = summary_tokenize(case)\n",
    "            if summary:\n",
    "                summary = summary.lower()\n",
    "                percent_mentioned = re.findall(percent_pattern, summary, re.IGNORECASE)\n",
    "                extracted_value_tie_breaker = Counter()\n",
    "                if len(percent_mentioned) > 0:\n",
    "                    for percent in percent_mentioned:\n",
    "                        context, extracted_value = get_context_and_float(percent, summary)\n",
    "                        # conditions for keeping extracted_value and updating extracted_value\n",
    "                        # skip extracted_values with contexts lacking keywords/entities\n",
    "                        extracted_value = conditions_for_extracted_value(context, extracted_value, keywords, plaintiff_split, defendant_split, entities)\n",
    "                        if not extracted_value:\n",
    "                            continue\n",
    "                        extracted_value_tie_breaker.update([extracted_value])\n",
    "                                                   \n",
    "                        # conditions for contributory negligence successful\n",
    "                        if not contributory_negligence_successful and extracted_value:\n",
    "                            contributory_negligence_successful = contributory_negligence_successful_fun(context, keywords) \n",
    "                            \n",
    "                        # matches patter \"PERCENT against plaintiff\"\n",
    "                        if ('against' in context or 'fault' in context) and any(plaintiff_word in context for plaintiff_word in plaintiff_split+['plaintiff']):\n",
    "                            best_percent = extracted_value\n",
    "                            best_score = score\n",
    "                            break \n",
    "                        # choose most common percent mentioned in summary\n",
    "                        if extracted_value_tie_breaker != Counter():\n",
    "                            best_percent = extracted_value_tie_breaker.most_common(1)[0][0]\n",
    "\n",
    "               # if no percent found, check for equal apportionment\n",
    "                else:\n",
    "                    equal_apportionment = re.findall(r'.{20} (?:liability|fault) [a-zA-Z]{1,3} apportione?d? equally .{20}', summary)\n",
    "                    if len(equal_apportionment) > 0:\n",
    "                        if contributory_negligence_successful_fun(equal_apportionment[0], keywords):\n",
    "                            best_percent = 50.0\n",
    "                            contributory_negligence_successful = True\n",
    "        if contributory_negligence_successful:\n",
    "            percent_reduction = best_percent\n",
    "    else:\n",
    "        percent_reduction = None\n",
    " \n",
    "    return percent_reduction, contributory_negligence_successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(path, clf = MultinomialNB()):\n",
    "    '''Trains a classifier based on the given training data path\n",
    "    \n",
    "    Arguments:\n",
    "    path (String) - Path to .txt containing training data\n",
    "    clf - untrained sklearn classifier, ie MultinomialNB()\n",
    "    \n",
    "    Returns:\n",
    "    model (sklearn model) - Trained model\n",
    "    '''\n",
    "    tag_extractor = re.compile('''<damage type ?= ?['\"](.*?)['\"]> ?(\\$?.*?) ?<\\/damage>''')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    with open(path, encoding='utf-8') as document:\n",
    "        document_data = document.read()\n",
    "        \n",
    "    document_data = document_data.split('End of Document\\n') # Always split on 'End of Document\\n'\n",
    "    \n",
    "    examples_per_case = [] # Each element contains all examples in a case\n",
    "    answers_per_case = [] # Each element contains all answers in a case \n",
    "    num_cases = len(document_data)\n",
    "    for i in range(len(document_data)):\n",
    "        print('Reading training data and extracting features...', i / num_cases * 100, '%', end='\\r')\n",
    "        case = document_data[i]\n",
    "        case = case.strip() # Make sure to strip!\n",
    "        if len(case) == 0: # Skip empty lines\n",
    "            continue\n",
    "        \n",
    "        lines = case.split('\\n')\n",
    "        case_title = lines[0]\n",
    "        case_type = lines[1]\n",
    "        \n",
    "        # lower case and remove stopwords\n",
    "        case = ' '.join([word for word in case.lower().split() if word not in stop_words])\n",
    "        \n",
    "        case_examples = []\n",
    "        case_answers = []\n",
    "        if filter_unwanted_cases(case, case_title, case_type):\n",
    "            matches = tag_extractor.finditer(case) # Extract all <damage ...>$x</damage> tags used for training\n",
    "            for match in matches:\n",
    "                features, answer = extract_training_features(match, case, tag_extractor)\n",
    "                case_examples.append(features)\n",
    "                case_answers.append(answer)\n",
    "                \n",
    "        if len(case_examples) > 0 and len(case_answers) > 0:\n",
    "            examples_per_case.append(case_examples)\n",
    "            answers_per_case.append(case_answers)\n",
    "        else:\n",
    "            print('Didnt find any tags in', case_title)\n",
    "                    \n",
    "    print('\\nVectorizing...')    \n",
    "    vectorizer = DictVectorizer()\n",
    "    feats = list(chain.from_iterable(examples_per_case)) # Puts it into one big list\n",
    "    X = vectorizer.fit_transform(feats)\n",
    "    y = list(chain.from_iterable(answers_per_case))\n",
    "    \n",
    "    print('Tag Distribution')\n",
    "    dist = Counter(y)\n",
    "    print(dist)\n",
    "    \n",
    "    print('Cross validation evaluation...')\n",
    "    print('Scores (F1-MACRO):', np.mean(cross_val_score(clf, X, y, cv = 5, scoring = 'f1_macro')))\n",
    "    print('Scores (F1-MICRO):', np.mean(cross_val_score(clf, X, y, cv = 5, scoring = 'f1_micro')))\n",
    "    print('Scores (F1-WEIGHTED):', np.mean(cross_val_score(clf, X, y, cv = 5, scoring = 'f1_weighted')))\n",
    "    \n",
    "    print('Training final model...')\n",
    "    clf.fit(X, y)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=5)\n",
    "    print(classification_report(y, y_pred))\n",
    "    return clf\n",
    "\n",
    "def extract_training_features(match, case, pattern, context_length = 5):\n",
    "    '''Given a match will return the features associated with the specific example\n",
    "    Extracts the examples by finding the damage annotation tags\n",
    "    in the form <damage type = \"TYPE\">$5000</damage>\n",
    "    \n",
    "    Arguments:\n",
    "    match (Match Object) - Match object with the type as group 1 and value as group 2\n",
    "    case (String) - The case data in string format\n",
    "    pattern (String, regex pattern) - The regex pattern being used to find damages.\n",
    "                                      Used to remove the tags in features using context around value.\n",
    "    [Optional] context_length (int) - The number of words to use around the value for context\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    features (dict) - Dictionary containing each feature for the current match\n",
    "    damage_type (str) - The type of damage associated with  the value\n",
    "    '''\n",
    "    features = dict()\n",
    "    damage_type = match.group(1)\n",
    "    damage_value = match.group(2)\n",
    "    \n",
    "    start_idx = match.start()\n",
    "    end_idx = match.end()\n",
    "    damages_keywords = {}\n",
    "    damages_keywords['non-pecuniary'] = ['non', 'pecuniary', 'pain', 'suffering']\n",
    "    damages_keywords['sub-non-pecuniary'] = ['non', 'pecuniary', 'pain', 'suffering']\n",
    "    \n",
    "    damages_keywords['special'] = ['special damages','renovations','housekeeping', 'home','in trust', 'bill', 'receipt', 'costs','therapies','equipment']\n",
    "    damages_keywords['sub-special'] = ['special damages', 'housekeeping', 'in trust','home','bill', 'receipt', 'costs','accommodations']\n",
    "    \n",
    "    damages_keywords['future care'] = ['future care', 'care', 'massage', 'pysio', 'therapy', 'medical', 'costs', 'future']\n",
    "    damages_keywords['sub-future care'] = ['future care', 'care', 'massage', 'pysio', 'therapy', 'medical', 'costs']\n",
    "    \n",
    "    damages_keywords['wage loss'] = ['wage', 'loss', 'income', 'work', 'employment', 'inability', 'earning capacity', 'earning', 'ability']\n",
    "    damages_keywords['total'] = ['total', 'damages are', 'assessed', 'sum','awarded']\n",
    "    damages_keywords['aggravated'] = ['aggravated']\n",
    "    damages_keywords['punitive'] = ['punitive']\n",
    "    \n",
    "    damages_keywords['general'] = ['general damages']\n",
    "    damages_keywords['sub-general'] = ['general damages', 'injuries',]\n",
    "    \n",
    "    damages_keywords['reduction'] = ['reduc', 'less', 'discounted', 'recover', '%']\n",
    "    # Use NLTK to get sentence on either side\n",
    "    # NOTE: This is extremely slow and needs to be improved\n",
    "    # We want to sent_tokenize ONCE not for every match!\n",
    "    start_tokenized = sent_tokenize(case[:start_idx])[-1]\n",
    "    end_tokenized = sent_tokenize(case[end_idx:])[0]\n",
    "\n",
    "    # Remove damage tags in context around match\n",
    "    start_matches = pattern.finditer(start_tokenized)\n",
    "    for s in start_matches:\n",
    "        start_tokenized = start_tokenized.replace(s.group(0), s.group(2))\n",
    "    end_matches = pattern.finditer(end_tokenized)\n",
    "    for e in end_matches:\n",
    "        end_tokenized = end_tokenized.replace(e.group(0), e.group(2))\n",
    "\n",
    "    # Reconstruct sentence\n",
    "    tokens = start_tokenized + \" \" + damage_value + \" \" + end_tokenized \n",
    "    value_start_idx = len(start_tokenized.split()) # Location of value in relation to sentence (token level)\n",
    "    if len(damage_value.split()) > 1: # Deals with problems like '2 million' (where value is multiple tokens)\n",
    "        value_end_idx = value_start_idx + len(damage_value.split()) - 1\n",
    "    else:\n",
    "        value_end_idx = value_start_idx\n",
    "    tokens = tokens.split()\n",
    "    \n",
    "    # Features: Context_before, Context_after, Context\n",
    "    start_boundary = value_start_idx - context_length if value_start_idx - context_length >= 0 else 0\n",
    "    end_boundary = value_end_idx + context_length + 1 if value_end_idx + context_length + 1 < len(tokens) else len(tokens)\n",
    "    context_before=  \" \".join(tokens[start_boundary : value_start_idx])\n",
    "    context_after = \" \".join(tokens[value_end_idx + 1 : end_boundary])\n",
    "    context =  \" \".join(tokens[start_boundary : end_boundary])\n",
    "    features['context_before'] = context_before\n",
    "    features['context_after'] = context_after\n",
    "    features['context'] = context\n",
    "    features['value'] = damage_value\n",
    "    features['float'] = clean_money_amount([damage_value.strip('$')])\n",
    "    features['start_idx_ratio'] = match.start()/len(case)\n",
    "    features['greater_than_1000'] = features['float'] > 1000\n",
    "    features['punitive'] = any(word in features['context'] for word in damages_keywords['punitive'])\n",
    "    features['non-pecuniary'] = any(word in features['context'] for word in damages_keywords['non-pecuniary'])\n",
    "    features['sub-non-pecuniary'] = any(word in features['context'] for word in damages_keywords['sub-non-pecuniary'])\n",
    "    \n",
    "    features['special'] = any(word in features['context'] for word in damages_keywords['special'])\n",
    "    features['sub-special'] = any(word in features['context'] for word in damages_keywords['sub-special'])\n",
    "    \n",
    "    features['general'] = any(word in features['context'] for word in damages_keywords['general'])\n",
    "    features['sub-general'] = any(word in features['context'] for word in damages_keywords['sub-general'])\n",
    "    \n",
    "    features['aggravated'] = any(word in features['context'] for word in damages_keywords['aggravated'])\n",
    "    features['future wage loss'] = 'future' or 'loss' in features['context'] and any(word in features['context'] for word in damages_keywords['wage loss'])\n",
    "    features['past wage loss'] = 'past' or 'previous' in features['context'] and any(word in features['context'] for word in damages_keywords['wage loss'])\n",
    "    features['total'] = any(word in features['context'] for word in damages_keywords['total'])\n",
    "    features['reduction'] = any(word in features['context'] for word in damages_keywords['reduction'])\n",
    "    \n",
    "    \n",
    "    return features, damage_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didnt find any tags in Mawani v. Pitcairn, [2012] B.C.J. No. 1819\n",
      "Didnt find any tags in Aiken (Guardian ad litem of) v. Van Dyk, [2001] B.C.J. No. 1751\n",
      "Didnt find any tags in Millard v. Singleton, [2015] B.C.J. No. 1234\n",
      "Didnt find any tags in Cowie v. Draper, [2010] B.C.J. No. 910000005 %\n",
      "Didnt find any tags in Bajwa v. Deol, [2018] I.L.R. para. G-2792\n",
      "Didnt find any tags in Jackson v. Fisheries and Oceans Canada, [2006] B.C.J. No. 2654\n",
      "Didnt find any tags in Los Angeles Salad Co. v. Canadian Food Inspection Agency, [2009] B.C.J. No. 161\n",
      "Didnt find any tags in Brooks-Martin v. Martin, [2011] B.C.J. No. 243\n",
      "Didnt find any tags in Gibson v. Matthies, [2017] B.C.J. No. 965999 %\n",
      "Didnt find any tags in Gray v. Ellis, [2007] I.L.R. para. M-2118\n",
      "Didnt find any tags in Morrow v. Outerbridge, 2009 CHFL para. 15,554%\n",
      "Didnt find any tags in Rackstraw (Litigation guardian of) v. Robertson, [2011] B.C.J. No. 1354\n",
      "Didnt find any tags in Ahlwat v. Green, [2014] B.C.J. No. 245200004 %\n",
      "Didnt find any tags in Quade v. Schwartz, [2008] B.C.J. No. 1032116 %\n",
      "Didnt find any tags in McKee (Guardian ad litem of) v. McCoy, [2001] B.C.J. No. 2675\n",
      "Didnt find any tags in Kerr (Litigation Guardian of) v. Creighton, [2007] B.C.J. No. 309\n",
      "Reading training data and extracting features... 99.2 %99999999999 %\n",
      "Vectorizing...\n",
      "Tag Distribution\n",
      "Counter({'other': 2774, 'non pecuniary': 244, 'special': 189, 'future care': 175, 'past wage loss': 172, 'future wage loss': 161, 'total': 149, 'sub-special': 107, 'general': 62, 'sub-future care': 60, 'sub-future wage loss': 37, 'sub-past wage loss': 27, 'reduction to': 26, 'total after': 20, 'sub-general': 18, 'sub-total': 15, 'sub-non pecuniary': 15, 'punitive': 14, 'reduction by': 13, 'aggravated': 8, 'in trust': 7, 'sub-in trust': 4, 'sub-reduction': 2})\n",
      "Cross validation evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores (F1-MACRO): 0.24494681137694668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores (F1-MICRO): 0.4542913609659691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores (F1-WEIGHTED): 0.5226388161204291\n",
      "Training final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          aggravated       0.35      0.75      0.48         8\n",
      "         future care       0.22      0.21      0.22       175\n",
      "    future wage loss       0.14      0.25      0.18       161\n",
      "             general       0.16      0.23      0.19        62\n",
      "            in trust       0.14      0.57      0.23         7\n",
      "       non pecuniary       0.51      0.68      0.58       244\n",
      "               other       0.92      0.50      0.65      2774\n",
      "      past wage loss       0.07      0.09      0.08       172\n",
      "            punitive       0.46      0.86      0.60        14\n",
      "        reduction by       0.15      0.62      0.24        13\n",
      "        reduction to       0.09      0.50      0.16        26\n",
      "             special       0.48      0.59      0.53       189\n",
      "     sub-future care       0.14      0.38      0.20        60\n",
      "sub-future wage loss       0.01      0.03      0.01        37\n",
      "         sub-general       0.03      0.17      0.05        18\n",
      "        sub-in trust       0.00      0.00      0.00         4\n",
      "   sub-non pecuniary       0.11      0.53      0.18        15\n",
      "  sub-past wage loss       0.02      0.07      0.03        27\n",
      "       sub-reduction       0.08      0.50      0.14         2\n",
      "         sub-special       0.06      0.11      0.08       107\n",
      "           sub-total       0.03      0.13      0.05        15\n",
      "               total       0.42      0.52      0.47       149\n",
      "         total after       0.17      0.50      0.25        20\n",
      "\n",
      "            accuracy                           0.45      4299\n",
      "           macro avg       0.21      0.38      0.24      4299\n",
      "        weighted avg       0.68      0.45      0.52      4299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_classifier('../data/all_annotations.txt',  LGBMClassifier(learning_rate=0.01, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_convert_cases_to_DF(cases):\n",
    "    '''Given a list of parsed cases returns a dataframe'''\n",
    "\n",
    "    lists = defaultdict(list)    \n",
    "    for case in cases:\n",
    "        lists['Case Number'].append(case['case_number'])\n",
    "        lists['Case Name'].append(case['case_title'])\n",
    "        lists['Year'].append(case['year'])\n",
    "        lists['Total Damage'].append(case['damages']['Total'] if case['damages'] != None else None)\n",
    "        lists['Total Pecuniary'].append(case['damages']['Pecuniary Total'] if case['damages'] != None else None)\n",
    "        lists['Non Pecuniary'].append(case['damages']['Non-pecuniary'] if case['damages'] != None else None)\n",
    "        lists['General'].append(case['damages']['General'] if case['damages'] != None else None)\n",
    "        lists['Special'].append(case['damages']['Special'] if case['damages'] != None else None)\n",
    "        lists['Punitive'].append(case['damages']['Punitive'] if case['damages'] != None else None)\n",
    "        lists['Aggravated'].append(case['damages']['Aggravated'] if case['damages'] != None else None)\n",
    "        lists['Future Care'].append(case['damages']['Future Care'] if case['damages'] != None else None)\n",
    "        lists['Judge Name'].append(case['judge'])\n",
    "        lists['Decision Length'].append(case['decision_length'])\n",
    "        lists['Multiple defendants?'].append(case['multiple_defendants'])\n",
    "        lists['Plaintiff Wins?'].append(case['plaintiff_wins'])\n",
    "        lists['Contributory Negligence Raised'].append(case['contributory_negligence_raised'])\n",
    "        lists['Written Decision?'].append(case['written_decision'])\n",
    "        lists['Registry'].append(case['registry'])\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    for key in lists.keys():\n",
    "        df[key] = lists[key]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def evaluate(dev_data, gold_data, subset=None):\n",
    "    '''Evaluates the results against a gold standard set\n",
    "    \n",
    "    Arguments:\n",
    "    dev_data (dataframe) - Dataframe containing results from rule based parse BCJ\n",
    "    gold_data (dataframe) - Dataframe containing manually annotated data\n",
    "    (Optional) subset (list/string) - Specific columns to evaluate\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print('#### Evaluation ####')\n",
    "    \n",
    "    # Use case name as 'primary key'\n",
    "    dev_case_names = list(dev_data['Case Name'])\n",
    "    gold_case_names = list(gold_data['Case Name'])\n",
    "    \n",
    "    # Filter data to only use overlapping items\n",
    "    gold_data = gold_data[gold_data['Case Name'].isin(dev_case_names)]\n",
    "    dev_data = dev_data[dev_data['Case Name'].isin(gold_case_names)]\n",
    "    \n",
    "    # Mapping from our variable names to Lachlan's column names\n",
    "    column_mapping = {'Decision Length': 'Decision Length: paragraphs)',\n",
    "                      'Total Damage': '$ Damages total before contributory negligence',\n",
    "                      'Non Pecuniary': '$ Non-Pecuniary Damages', \n",
    "                      'Total Pecuniary': '$ Pecuniary Damages Total',\n",
    "                      'Special': '$ Special damages Pecuniary (ie. any expenses already incurred)',\n",
    "                      'Future Care': 'Future Care Costs (General Damages)',\n",
    "                      'General': '$ General Damages',\n",
    "                      'Punitive': '$ Punitive Damages',\n",
    "                      'Aggravated': '$Aggravated Damages'}\n",
    "    dev_data.rename(columns = column_mapping, inplace = True)\n",
    "     \n",
    "    if subset is None: # Use all columns if no subset specified\n",
    "        subset = dev_data.columns\n",
    "        \n",
    "    for column in dev_data.columns:\n",
    "        if column in gold_data.columns:\n",
    "            if column in subset:\n",
    "                empty_correct = 0\n",
    "                non_empty_correct = 0\n",
    "                total_empty = 0\n",
    "                total_non_empty = 0\n",
    "                for case_name in list(dev_data['Case Name']):\n",
    "                    dev_value = list(dev_data[dev_data['Case Name'] == case_name][column])[0]\n",
    "                    gold_value = list(gold_data[gold_data['Case Name'] == case_name][column])[0]\n",
    "\n",
    "                    # Convert string to float if possible\n",
    "                    try:\n",
    "                        gold_value = float(gold_value)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        dev_value = float(dev_value)\n",
    "                    except:\n",
    "                        pass\n",
    "                    # Set values to 'None' if they're a NaN float value\n",
    "                    dev_value = None if isinstance(dev_value, float) and math.isnan(dev_value) else dev_value\n",
    "                    gold_value = None if isinstance(gold_value, float) and math.isnan(gold_value) else gold_value\n",
    "                    # Lowercase values if they're a string\n",
    "                    dev_value = dev_value.lower().strip() if isinstance(dev_value, str) else dev_value\n",
    "                    gold_value = gold_value.lower().strip() if isinstance(gold_value, str) else gold_value\n",
    "\n",
    "                    if gold_value is None:\n",
    "                        total_empty += 1\n",
    "                        if dev_value is None:\n",
    "                            empty_correct += 1\n",
    "                    else:\n",
    "                        total_non_empty += 1\n",
    "                        if isinstance(dev_value, float) and isinstance(gold_value, float):\n",
    "                            if math.isclose(dev_value, gold_value, abs_tol=1): # Tolerance within 1\n",
    "                                non_empty_correct += 1\n",
    "                        elif dev_value == gold_value:\n",
    "                            non_empty_correct += 1\n",
    "                        \n",
    "                print('-------')\n",
    "                print('COLUMN:', column)\n",
    "                if total_empty != 0:\n",
    "                    print('Empty field accuracy:', empty_correct / total_empty * 100, '%', empty_correct, '/', total_empty)\n",
    "                if total_non_empty != 0:\n",
    "                    print('Filled field accuracy:', non_empty_correct / total_non_empty * 100, '%', non_empty_correct, '/', total_non_empty)\n",
    "                print('Overall accuracy:', (empty_correct+non_empty_correct) / (total_non_empty+total_empty) * 100, '%', (empty_correct+non_empty_correct), '/', (total_non_empty+total_empty))\n",
    "    \n",
    "    # for testing:\n",
    "    #return dev_data, gold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = rule_based_parse_BCJ('../data/Lexis Cases txt/P1.txt')\n",
    "cases[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file_number in file_identifiers:\n",
    "    #print('## Processing ' + path_to_data + file_prefix + str(file_number) + file_suffix + ' ##\\n')\n",
    "    data.extend(rule_based_parse_BCJ(path_to_data + file_prefix + str(file_number) + file_suffix))\n",
    "    \n",
    "data[-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../data/Lexis Cases txt/'\n",
    "file_prefix = 'P'\n",
    "file_suffix = '.txt'\n",
    "file_identifiers = range(1, 86) # Range from 1 to 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_type_counts = defaultdict(int)\n",
    "for file_number in file_identifiers:\n",
    "\n",
    "    print('Processing ' + path_to_data + file_prefix + str(file_number) + file_suffix)\n",
    "\n",
    "    with open(path_to_data + file_prefix + str(file_number) + file_suffix, 'r') as document:\n",
    "        document_data = document.read()\n",
    "        \n",
    "    document_data = document_data.split('End of Document\\n') # Must have \\n as the phrase appears in one of the cases\n",
    "    for case in document_data:\n",
    "        case = case.strip()\n",
    "        # Just in-case we have an empty case in the list\n",
    "        if len(case) > 0:\n",
    "            case_title = case.split('\\n')[0]\n",
    "            case_type = case.split('\\n')[1]\n",
    "            if 'R. v.' in case_title:\n",
    "                case_type_counts['Crown Cases (R. v. ___)'] += 1\n",
    "            elif 'Canadian Health Facilities Law Guide' in case_type: # CHFL\n",
    "                case_type_counts['CHFL'] += 1\n",
    "            elif 'British Columbia Judgments' in case_type: # B.C.J.\n",
    "                case_type_counts['BCJ'] += 1\n",
    "            elif 'Canadian Insurance Law Reporter Cases' in case_type: # I.L.R.\n",
    "                case_type_counts['ILR'] += 1\n",
    "            elif 'Canadian Commercial Law Guide' in case_type: # CCLG\n",
    "                case_type_counts['CCLG'] += 1\n",
    "            elif 'Ontario Corporations Law Guide' in case_type: # OCLG\n",
    "                case_type_counts['OCLG'] += 1\n",
    "            elif 'Canadian Corporate Secretary\\'s Guide' in case_type: # CCSG\n",
    "                case_type_counts['CCSG'] += 1\n",
    "            elif 'Canadian Employment Benefits & Pension Guide' in case_type: # CBPG\n",
    "                case_type_counts['CBPG'] += 1\n",
    "            elif 'Alberta Corporations Law Guide' in case_type: # ACLG\n",
    "                case_type_counts['ACLG'] += 1\n",
    "            elif 'British Columbia Real Estate Law Guide' in case_type: # BREG\n",
    "                case_type_counts['BREG'] += 1\n",
    "            elif 'Canadian Native Law Reporter' in case_type: # C.N.L.R\n",
    "                case_type_counts['CNLR'] += 1\n",
    "            elif 'Dominion Tax Cases' in case_type: # DTC\n",
    "                case_type_counts['DTC'] += 1\n",
    "            elif 'Canadian Labour Law Reporter' in case_type: # CLLC\n",
    "                case_type_counts['CLLC'] += 1\n",
    "            elif 'British Columbia Corporations Law Guide' in case_type: # BCLG\n",
    "                case_type_counts['BCLG'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plaintiff_wins(path):\n",
    "    '''This function will search the cases and returns a dictionary\n",
    "    with case names as keys and Y or N for value, Y if the plaintiff\n",
    "    wins the case and N if plaintiff looses'''\n",
    "    \n",
    "    plaintiff_dict = {}\n",
    "    with open(path,'r') as f:\n",
    "        contents = f.read() \n",
    "        cases = contents.split(\"End of Document\\n\")\n",
    "        for line in cases:\n",
    "            lines = line.strip().split(\"\\n\")\n",
    "            name = lines[0]        \n",
    "            #check if it's a British columbia case    \n",
    "            if \"B.C.J\" in name:\n",
    "                #check if it's not a crown case    \n",
    "                if 'R. v.' in name or '(Re)' in name:\n",
    "                    continue\n",
    "                \n",
    "                # regex search for keyword HELD in cases, which determines if case was allowed or dismissed\n",
    "                HELD = re.search(r'HELD(.+)?', line)\n",
    "                \n",
    "                if HELD:\n",
    "                    matched = HELD.group(0)  \n",
    "                    # regex searching for words such as liablity, liable, negligance, negligant, convicted, convict in matched\n",
    "                    liable = re.search(r'(l|L)iab(.+)?.+|(neglige(.+)?)|(convict(.+)?)', matched)\n",
    "                    # regex searching fot dissmiss/dissmissed/adjourned, negative in matched\n",
    "                    dismiss = re.search(r'(dismiss(.+)?.+)|(adjourned.+?)|(negative(.+)?)', matched)\n",
    "                    # regex searching for damage/Damage/fault/faulty\n",
    "                    damage = re.search(r'(D|d)amage(.+)?.+|(fault(.+)?)', matched)\n",
    "                    if \"allowed\" in matched or \"favour\" in matched or \"awarded\" in matched or \"granted\" in matched or \"accepted\" in matched or \"entitled\" in matched or \"guilty\" in matched or liable or damage:\n",
    "                        plaintiff_dict[name] = \"Y\"\n",
    "                    \n",
    "                    elif dismiss:\n",
    "                        plaintiff_dict[name] = \"N\"\n",
    "\n",
    "                else:\n",
    "                    if line and name not in plaintiff_dict :\n",
    "                        \n",
    "                        last_paras = lines[-5]+\" \"+lines[-4]+\" \"+lines[-3]+\" \"+lines[-2]\n",
    "                        #regex searches for pattern of award ... plaintiff ...\n",
    "                        awarded =  re.search(r'award(.+)?.+?(plaintiff(.+)?)?', last_paras)\n",
    "                        #regex searches for pattern of plaintiff/defendant/applicant....entitled/have...costs\n",
    "                        entiteled = re.search(r'(plaintiff|defendant.?|applicant)(.+)?(entitle(.)?(.+)?|have).+?cost(.+)?', last_paras)\n",
    "                        #regex searches for pattern of successful...(case)\n",
    "                        successful = re.search(r'successful(.+)?.+?', last_paras)\n",
    "                        #regex searches for dismiss....\n",
    "                        dismiss = re.search(r'(dismiss(.+)?.+)|(adjourned.+?)|(negative(.+)?)', last_paras)\n",
    "                        costs = re.search(r'costs.+?(award(.+)?|cause).+?', last_paras)\n",
    "                        damage = re.search(r'(D|d)amage(.+)?.+|(fault(.+)?)', last_paras)\n",
    "\n",
    "                        if dismiss and \"not dismissed\" not in last_paras:\n",
    "                            plaintiff_dict[name] = \"N\"\n",
    "                        elif damage:\n",
    "                            plaintiff_dict[name] = \"Y\"\n",
    "                        elif awarded:\n",
    "                            plaintiff_dict[name] = \"Y\"\n",
    "                        elif entiteled:\n",
    "                            plaintiff_dict[name] = \"Y\"\n",
    "                        elif successful:\n",
    "                            plaintiff_dict[name] = \"Y\"\n",
    "                        elif costs:\n",
    "                            plaintiff_dict[name] = \"Y\"\n",
    "                        else:\n",
    "                            plaintiff_dict[name] = \"OpenCase\"\n",
    "\n",
    "\n",
    "    return plaintiff_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
